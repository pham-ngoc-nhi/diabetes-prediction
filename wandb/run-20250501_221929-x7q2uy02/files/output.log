01-05-2025 22:19:32 Downloading and reading train artifact
[34m[1mwandb[0m:   1 of 1 files downloaded.
01-05-2025 22:19:35 Spliting data into train/val
01-05-2025 22:19:35 x train: (375, 28)
01-05-2025 22:19:35 y train: (375,)
01-05-2025 22:19:35 x val: (162, 28)
01-05-2025 22:19:35 y val: (162,)
01-05-2025 22:19:35 Outlier Removal
01-05-2025 22:19:36 x_train shape [original]: (375, 28)
01-05-2025 22:19:36 x_train shape [outlier removal]: (357, 28)
Requirement already satisfied: wandb in c:\users\x-hp\anaconda3\lib\site-packages (0.19.8)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.1.43)
Requirement already satisfied: platformdirs in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (4.25.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (5.9.0)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.8.2)
Requirement already satisfied: pyyaml in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (6.0.1)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.18.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (75.1.0)
Requirement already satisfied: colorama in c:\users\x-hp\anaconda3\lib\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)
Requirement already satisfied: typing-extensions>=4.6.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)
Requirement already satisfied: smmap<5,>=3.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.login() after wandb.init() has no effect.
01-05-2025 22:21:11 Downloading and reading train artifact
[34m[1mwandb[0m:   1 of 1 files downloaded.
01-05-2025 22:21:13 Spliting data into train/val
01-05-2025 22:21:13 x train: (375, 28)
01-05-2025 22:21:13 y train: (375,)
01-05-2025 22:21:13 x val: (162, 28)
01-05-2025 22:21:13 y val: (162,)
01-05-2025 22:21:13 Outlier Removal
01-05-2025 22:21:14 x_train shape [original]: (375, 28)
01-05-2025 22:21:14 x_train shape [outlier removal]: (357, 28)
01-05-2025 22:21:14 Data after SMOTE: (476, 28)
Requirement already satisfied: wandb in c:\users\x-hp\anaconda3\lib\site-packages (0.19.8)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.1.43)
Requirement already satisfied: platformdirs in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (4.25.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (5.9.0)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.8.2)
Requirement already satisfied: pyyaml in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (6.0.1)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.18.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (75.1.0)
Requirement already satisfied: colorama in c:\users\x-hp\anaconda3\lib\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)
Requirement already satisfied: typing-extensions>=4.6.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)
Requirement already satisfied: smmap<5,>=3.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.login() after wandb.init() has no effect.
01-05-2025 22:21:47 Downloading and reading train artifact
[34m[1mwandb[0m:   1 of 1 files downloaded.
01-05-2025 22:21:49 Spliting data into train/val
01-05-2025 22:21:49 x train: (375, 28)
01-05-2025 22:21:49 y train: (375,)
01-05-2025 22:21:49 x val: (162, 28)
01-05-2025 22:21:49 y val: (162,)
01-05-2025 22:21:49 Outlier Removal
01-05-2025 22:21:49 x_train shape [original]: (375, 28)
01-05-2025 22:21:49 x_train shape [outlier removal]: (357, 28)
01-05-2025 22:21:49 Data after SMOTE: (476, 28)
01-05-2025 22:21:50 [Logistic] Accuracy: 0.7469135802469136
01-05-2025 22:21:50 [Random Forest] Accuracy: 0.7222222222222222
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:21:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
01-05-2025 22:21:50 [XGBoost] Accuracy: 0.7222222222222222
                 Model  Accuracy
0  Logistic Regression  0.746914
1        Random Forest  0.722222
2              XGBoost  0.722222
Requirement already satisfied: optuna-integration[wandb] in c:\users\x-hp\anaconda3\lib\site-packages (4.3.0)
Requirement already satisfied: optuna in c:\users\x-hp\anaconda3\lib\site-packages (from optuna-integration[wandb]) (4.2.1)
Requirement already satisfied: wandb in c:\users\x-hp\anaconda3\lib\site-packages (from optuna-integration[wandb]) (0.19.8)
Requirement already satisfied: alembic>=1.5.0 in c:\users\x-hp\anaconda3\lib\site-packages (from optuna->optuna-integration[wandb]) (1.13.3)
Requirement already satisfied: colorlog in c:\users\x-hp\anaconda3\lib\site-packages (from optuna->optuna-integration[wandb]) (6.9.0)
Requirement already satisfied: numpy in c:\users\x-hp\anaconda3\lib\site-packages (from optuna->optuna-integration[wandb]) (2.0.2)
Requirement already satisfied: packaging>=20.0 in c:\users\x-hp\anaconda3\lib\site-packages (from optuna->optuna-integration[wandb]) (24.2)
Requirement already satisfied: sqlalchemy>=1.4.2 in c:\users\x-hp\anaconda3\lib\site-packages (from optuna->optuna-integration[wandb]) (2.0.34)
Requirement already satisfied: tqdm in c:\users\x-hp\anaconda3\lib\site-packages (from optuna->optuna-integration[wandb]) (4.66.5)
Requirement already satisfied: PyYAML in c:\users\x-hp\anaconda3\lib\site-packages (from optuna->optuna-integration[wandb]) (6.0.1)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (3.1.43)
Requirement already satisfied: platformdirs in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (4.25.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (5.9.0)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (2.8.2)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (2.18.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\anaconda3\lib\site-packages (from wandb->optuna-integration[wandb]) (75.1.0)
Requirement already satisfied: Mako in c:\users\x-hp\anaconda3\lib\site-packages (from alembic>=1.5.0->optuna->optuna-integration[wandb]) (1.2.3)
Requirement already satisfied: typing-extensions>=4 in c:\users\x-hp\anaconda3\lib\site-packages (from alembic>=1.5.0->optuna->optuna-integration[wandb]) (4.11.0)
Requirement already satisfied: colorama in c:\users\x-hp\anaconda3\lib\site-packages (from click!=8.0.0,>=7.1->wandb->optuna-integration[wandb]) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from docker-pycreds>=0.4.0->wandb->optuna-integration[wandb]) (1.17.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->optuna-integration[wandb]) (4.0.7)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb->optuna-integration[wandb]) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb->optuna-integration[wandb]) (2.20.1)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (2025.1.31)
Requirement already satisfied: greenlet!=0.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[wandb]) (3.0.1)
Requirement already satisfied: smmap<5,>=3.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->optuna-integration[wandb]) (4.0.0)
Requirement already satisfied: MarkupSafe>=0.9.2 in c:\users\x-hp\anaconda3\lib\site-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration[wandb]) (2.1.3)
Collecting optuna-integration[wandb]Note: you may need to restart the kernel to use updated packages.

[notice] A new release of pip is available: 24.2 -> 25.1

  Using cached optuna_integration-4.3.0-py3-none-any.whl.metadata (12 kB)
Requirement already satisfied: optuna in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from optuna-integration[wandb]) (4.2.1)
[notice] To update, run: python.exe -m pip install --upgrade pip
Requirement already satisfied: wandb in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from optuna-integration[wandb]) (0.19.8)
Requirement already satisfied: alembic>=1.5.0 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from optuna->optuna-integration[wandb]) (1.14.0)
Requirement already satisfied: colorlog in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from optuna->optuna-integration[wandb]) (6.9.0)
Requirement already satisfied: numpy in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from optuna->optuna-integration[wandb]) (1.26.0)
Requirement already satisfied: packaging>=20.0 in c:\users\x-hp\appdata\roaming\python\python311\site-packages (from optuna->optuna-integration[wandb]) (23.1)
Requirement already satisfied: sqlalchemy>=1.4.2 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from optuna->optuna-integration[wandb]) (2.0.37)
Requirement already satisfied: tqdm in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from optuna->optuna-integration[wandb]) (4.67.1)
Requirement already satisfied: PyYAML in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from optuna->optuna-integration[wandb]) (6.0.2)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (3.1.40)
Requirement already satisfied: platformdirs in c:\users\x-hp\appdata\roaming\python\python311\site-packages (from wandb->optuna-integration[wandb]) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (5.29.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\appdata\roaming\python\python311\site-packages (from wandb->optuna-integration[wandb]) (5.9.5)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (2.10.4)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (2.24.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (65.5.0)
Requirement already satisfied: typing-extensions<5,>=4.4 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from wandb->optuna-integration[wandb]) (4.12.2)
Requirement already satisfied: Mako in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from alembic>=1.5.0->optuna->optuna-integration[wandb]) (1.3.8)
Requirement already satisfied: colorama in c:\users\x-hp\appdata\roaming\python\python311\site-packages (from click!=8.0.0,>=7.1->wandb->optuna-integration[wandb]) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\appdata\roaming\python\python311\site-packages (from docker-pycreds>=0.4.0->wandb->optuna-integration[wandb]) (1.16.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->optuna-integration[wandb]) (4.0.11)
Requirement already satisfied: annotated-types>=0.6.0 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from pydantic<3,>=2.6->wandb->optuna-integration[wandb]) (0.7.0)
Requirement already satisfied: pydantic-core==2.27.2 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from pydantic<3,>=2.6->wandb->optuna-integration[wandb]) (2.27.2)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (3.4.1)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (3.10)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (2.3.0)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from requests<3,>=2.0.0->wandb->optuna-integration[wandb]) (2024.12.14)
Requirement already satisfied: greenlet!=0.4.17 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration[wandb]) (3.1.1)
Requirement already satisfied: smmap<6,>=3.0.1 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->optuna-integration[wandb]) (5.0.1)
Requirement already satisfied: MarkupSafe>=0.9.2 in c:\users\x-hp\appdata\local\programs\python\python311\lib\site-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration[wandb]) (3.0.2)
Using cached optuna_integration-4.3.0-py3-none-any.whl (98 kB)
Installing collected packages: optuna-integration
Successfully installed optuna-integration-4.3.0
C:\Users\x-hp\AppData\Local\Temp\ipykernel_14124\3551299460.py:22: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.
  wandb_callback = WeightsAndBiasesCallback(metric_name="accuracy", wandb_kwargs={"project": "diabetes"})
[I 2025-05-01 22:22:50,705] A new study created in memory with name: no-name-d8985519-6a59-4f51-8b8c-72cbd4838320
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:22:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:22:51,811] Trial 0 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 176, 'max_depth': 8, 'learning_rate': 0.14192391775969737, 'subsample': 0.6409776478052065, 'colsample_bytree': 0.7185262310188173}. Best is trial 0 with value: 0.6851851851851852.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:22:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:22:52,490] Trial 1 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 278, 'max_depth': 9, 'learning_rate': 0.22968091230388868, 'subsample': 0.606964067310534, 'colsample_bytree': 0.7288190875624996}. Best is trial 1 with value: 0.7098765432098766.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:22:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:22:52,642] Trial 2 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 162, 'max_depth': 3, 'learning_rate': 0.11225906439078312, 'subsample': 0.7244433588587978, 'colsample_bytree': 0.723147458974333}. Best is trial 1 with value: 0.7098765432098766.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:22:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 0 that is less than the current step 7. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 1 that is less than the current step 7. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 2 that is less than the current step 7. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[I 2025-05-01 22:22:53,941] Trial 3 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 253, 'max_depth': 3, 'learning_rate': 0.2025528393742984, 'subsample': 0.6838799882212594, 'colsample_bytree': 0.7782115452594391}. Best is trial 3 with value: 0.7222222222222222.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:22:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:22:55,157] Trial 4 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 166, 'max_depth': 3, 'learning_rate': 0.09393005215374337, 'subsample': 0.6702460561209486, 'colsample_bytree': 0.6556152642291578}. Best is trial 3 with value: 0.7222222222222222.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:22:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:22:58,878] Trial 5 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 291, 'max_depth': 4, 'learning_rate': 0.0939631904833631, 'subsample': 0.6091227167181786, 'colsample_bytree': 0.7511169166117629}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:22:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 3 that is less than the current step 7. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 4 that is less than the current step 7. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 5 that is less than the current step 7. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[I 2025-05-01 22:23:06,219] Trial 6 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 217, 'max_depth': 3, 'learning_rate': 0.15768692998563907, 'subsample': 0.7288374722084823, 'colsample_bytree': 0.857778658649782}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:10,459] Trial 7 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 249, 'max_depth': 5, 'learning_rate': 0.0806811770216957, 'subsample': 0.6421897656745219, 'colsample_bytree': 0.9274148787271578}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:12,861] Trial 8 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 108, 'max_depth': 5, 'learning_rate': 0.0452085723544168, 'subsample': 0.8327531379974671, 'colsample_bytree': 0.8556749841123392}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 6 that is less than the current step 7. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[I 2025-05-01 22:23:17,392] Trial 9 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 160, 'max_depth': 6, 'learning_rate': 0.249346516052363, 'subsample': 0.9652077569264503, 'colsample_bytree': 0.9392630802838288}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:19,455] Trial 10 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 291, 'max_depth': 7, 'learning_rate': 0.014641496683650335, 'subsample': 0.8352695920415049, 'colsample_bytree': 0.6010822340312701}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:20,110] Trial 11 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 217, 'max_depth': 4, 'learning_rate': 0.1746343237902611, 'subsample': 0.7482301024573387, 'colsample_bytree': 0.8569100461553544}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:20,582] Trial 12 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 220, 'max_depth': 5, 'learning_rate': 0.14911121317236664, 'subsample': 0.7498512007314404, 'colsample_bytree': 0.8315799377531798}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:21,293] Trial 13 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 256, 'max_depth': 10, 'learning_rate': 0.284948397172423, 'subsample': 0.9224983265875986, 'colsample_bytree': 0.9844989458750965}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:21,930] Trial 14 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 106, 'max_depth': 4, 'learning_rate': 0.12442947003905415, 'subsample': 0.7980265017340916, 'colsample_bytree': 0.7790410358656424}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:22,712] Trial 15 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 299, 'max_depth': 4, 'learning_rate': 0.06267579954341329, 'subsample': 0.704570679023378, 'colsample_bytree': 0.8947323485133793}. Best is trial 5 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:23,356] Trial 16 finished with value: 0.7407407407407407 and parameters: {'n_estimators': 199, 'max_depth': 6, 'learning_rate': 0.18146693782516765, 'subsample': 0.6076510419631096, 'colsample_bytree': 0.8108119629798756}. Best is trial 16 with value: 0.7407407407407407.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:24,072] Trial 17 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 187, 'max_depth': 7, 'learning_rate': 0.19426168508621713, 'subsample': 0.6006609848975496, 'colsample_bytree': 0.7941888756484843}. Best is trial 16 with value: 0.7407407407407407.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:24,643] Trial 18 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 138, 'max_depth': 6, 'learning_rate': 0.02849755980153937, 'subsample': 0.6457414409949012, 'colsample_bytree': 0.6833943487765417}. Best is trial 16 with value: 0.7407407407407407.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:23:24,931] Trial 19 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 198, 'max_depth': 8, 'learning_rate': 0.2947631780847094, 'subsample': 0.9028110050008213, 'colsample_bytree': 0.7559740410408184}. Best is trial 16 with value: 0.7407407407407407.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:23:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
C:\Users\x-hp\AppData\Local\Temp\ipykernel_14124\2652961358.py:22: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.
  wandb_callback = WeightsAndBiasesCallback(metric_name="accuracy", wandb_kwargs={"project": "diabetes"})
[I 2025-05-01 22:24:08,681] A new study created in memory with name: no-name-a2f11c88-ace8-4ffd-a089-09ef0ecbed8e
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:09,326] Trial 0 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 112, 'max_depth': 9, 'learning_rate': 0.2914388455300805, 'subsample': 0.7510049134399402, 'colsample_bytree': 0.7090212853384275}. Best is trial 0 with value: 0.6975308641975309.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:09,644] Trial 1 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 205, 'max_depth': 6, 'learning_rate': 0.23663356563434484, 'subsample': 0.7981587394456615, 'colsample_bytree': 0.854287039682077}. Best is trial 1 with value: 0.7037037037037037.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:12,342] Trial 2 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 186, 'max_depth': 5, 'learning_rate': 0.010465602598928123, 'subsample': 0.6139330861862117, 'colsample_bytree': 0.6468828113919968}. Best is trial 2 with value: 0.7160493827160493.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 0 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 1 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 2 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[I 2025-05-01 22:24:14,409] Trial 3 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 288, 'max_depth': 7, 'learning_rate': 0.23628896264894095, 'subsample': 0.9937201999810554, 'colsample_bytree': 0.7958120465056814}. Best is trial 2 with value: 0.7160493827160493.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:15,545] Trial 4 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 243, 'max_depth': 4, 'learning_rate': 0.10762913398154941, 'subsample': 0.8218845930441614, 'colsample_bytree': 0.8101833498207986}. Best is trial 2 with value: 0.7160493827160493.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:16,336] Trial 5 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 118, 'max_depth': 5, 'learning_rate': 0.2681033992367235, 'subsample': 0.9447590453029424, 'colsample_bytree': 0.7641041410292895}. Best is trial 2 with value: 0.7160493827160493.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:17,223] Trial 6 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 193, 'max_depth': 5, 'learning_rate': 0.23191470823865007, 'subsample': 0.7391145376893022, 'colsample_bytree': 0.6653598817901755}. Best is trial 2 with value: 0.7160493827160493.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:17,373] Trial 7 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 102, 'max_depth': 3, 'learning_rate': 0.19974399930467468, 'subsample': 0.7405241839529044, 'colsample_bytree': 0.7704850376865453}. Best is trial 2 with value: 0.7160493827160493.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:17,635] Trial 8 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 224, 'max_depth': 3, 'learning_rate': 0.14381468765192162, 'subsample': 0.8556200253132973, 'colsample_bytree': 0.6972227993535005}. Best is trial 8 with value: 0.7283950617283951.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:19,045] Trial 9 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 212, 'max_depth': 5, 'learning_rate': 0.20580353946930077, 'subsample': 0.7830163682791824, 'colsample_bytree': 0.631781324073852}. Best is trial 8 with value: 0.7283950617283951.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:20,693] Trial 10 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 278, 'max_depth': 10, 'learning_rate': 0.12332506451635694, 'subsample': 0.8903631394019056, 'colsample_bytree': 0.961099977894704}. Best is trial 8 with value: 0.7283950617283951.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:21,322] Trial 11 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 165, 'max_depth': 3, 'learning_rate': 0.013714838318579736, 'subsample': 0.6005514523228174, 'colsample_bytree': 0.6035213988814441}. Best is trial 8 with value: 0.7283950617283951.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:21,984] Trial 12 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 164, 'max_depth': 3, 'learning_rate': 0.014510534459034302, 'subsample': 0.6073179710796403, 'colsample_bytree': 0.6980865287678629}. Best is trial 8 with value: 0.7283950617283951.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:22,880] Trial 13 finished with value: 0.691358024691358 and parameters: {'n_estimators': 236, 'max_depth': 7, 'learning_rate': 0.06816289425629837, 'subsample': 0.865766714981941, 'colsample_bytree': 0.707033278228092}. Best is trial 8 with value: 0.7283950617283951.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:23,168] Trial 14 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 159, 'max_depth': 4, 'learning_rate': 0.06415798296539638, 'subsample': 0.6724628804901824, 'colsample_bytree': 0.6424041442917354}. Best is trial 8 with value: 0.7283950617283951.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 3 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 4 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 5 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 6 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 7 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 8 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 9 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 10 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 11 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 12 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 13 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 14 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[I 2025-05-01 22:24:24,588] Trial 15 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 141, 'max_depth': 4, 'learning_rate': 0.1620548119597452, 'subsample': 0.6803820151623122, 'colsample_bytree': 0.907481041843879}. Best is trial 8 with value: 0.7283950617283951.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:25,287] Trial 16 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 245, 'max_depth': 4, 'learning_rate': 0.06479252039988194, 'subsample': 0.6678821819267372, 'colsample_bytree': 0.7348531410734555}. Best is trial 8 with value: 0.7283950617283951.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:25,613] Trial 17 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 146, 'max_depth': 8, 'learning_rate': 0.1468756755067478, 'subsample': 0.8875662261524461, 'colsample_bytree': 0.6011113845998436}. Best is trial 17 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:25,983] Trial 18 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.15689962134846638, 'subsample': 0.8965792085272607, 'colsample_bytree': 0.6137570346487639}. Best is trial 17 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 22:24:26,288] Trial 19 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 131, 'max_depth': 8, 'learning_rate': 0.12073945790376481, 'subsample': 0.8484531208732551, 'colsample_bytree': 0.6942775509658652}. Best is trial 17 with value: 0.7345679012345679.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:24:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
✅ Saved and logged best XGBoost model with Optuna tuning.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 15 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 16 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 17 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 18 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
[34m[1mwandb[0m: [33mWARNING[0m Tried to log to step 19 that is less than the current step 21. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Tuning RandomForest with Optuna...
[I 2025-05-01 22:36:52,820] A new study created in memory with name: no-name-a111a1de-d02d-4570-aaea-e67b4f63b37b
[W 2025-05-01 22:36:52,829] Trial 0 failed with parameters: {'n_estimators': 70, 'max_depth': 14, 'min_samples_split': 6} because of the following error: NameError("name 'y_resampled' is not defined").
Traceback (most recent call last):
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\optuna\study\_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "C:\Users\x-hp\AppData\Local\Temp\ipykernel_14124\2607794363.py", line 9, in objective
    model.fit(X_train_scaled, y_resampled)
                              ^^^^^^^^^^^
NameError: name 'y_resampled' is not defined
[W 2025-05-01 22:36:52,851] Trial 0 failed with value None.
Tuning RandomForest with Optuna...
[I 2025-05-01 22:37:37,527] A new study created in memory with name: no-name-2cbc9dcf-a9c4-46fa-99f8-3f57817a9b5a
[I 2025-05-01 22:37:38,009] Trial 0 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 122, 'max_depth': 10, 'min_samples_split': 2}. Best is trial 0 with value: 0.7160493827160493.
[I 2025-05-01 22:37:38,367] Trial 1 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 58, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:39,090] Trial 2 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 141, 'max_depth': 6, 'min_samples_split': 4}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:39,393] Trial 3 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 129, 'max_depth': 6, 'min_samples_split': 2}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:39,782] Trial 4 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 164, 'max_depth': 7, 'min_samples_split': 4}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:40,115] Trial 5 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 143, 'max_depth': 12, 'min_samples_split': 9}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:40,460] Trial 6 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 136, 'max_depth': 16, 'min_samples_split': 10}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:41,298] Trial 7 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 192, 'max_depth': 14, 'min_samples_split': 3}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:41,726] Trial 8 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 168, 'max_depth': 19, 'min_samples_split': 7}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:41,974] Trial 9 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 90, 'max_depth': 17, 'min_samples_split': 10}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:42,159] Trial 10 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 58, 'max_depth': 20, 'min_samples_split': 6}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:42,656] Trial 11 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 200, 'max_depth': 14, 'min_samples_split': 4}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:42,839] Trial 12 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 57, 'max_depth': 14, 'min_samples_split': 6}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:43,083] Trial 13 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 90, 'max_depth': 11, 'min_samples_split': 3}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:43,571] Trial 14 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 193, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:43,849] Trial 15 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 100, 'max_depth': 9, 'min_samples_split': 8}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:44,104] Trial 16 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 71, 'max_depth': 14, 'min_samples_split': 5}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:44,418] Trial 17 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 109, 'max_depth': 18, 'min_samples_split': 3}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:44,906] Trial 18 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 170, 'max_depth': 15, 'min_samples_split': 5}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:37:45,358] Trial 19 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 155, 'max_depth': 12, 'min_samples_split': 7}. Best is trial 1 with value: 0.7283950617283951.
Best params for RandomForest: {'n_estimators': 58, 'max_depth': 16, 'min_samples_split': 5}
Tuning RandomForest with Optuna...
[I 2025-05-01 22:38:21,472] A new study created in memory with name: no-name-b4f1bf15-7563-45b1-824f-a544dd50f749
[I 2025-05-01 22:38:22,632] Trial 0 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 175, 'max_depth': 19, 'min_samples_split': 9}. Best is trial 0 with value: 0.7037037037037037.
[I 2025-05-01 22:38:23,100] Trial 1 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 150, 'max_depth': 9, 'min_samples_split': 10}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:23,441] Trial 2 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 126, 'max_depth': 17, 'min_samples_split': 5}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:23,672] Trial 3 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 83, 'max_depth': 11, 'min_samples_split': 2}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:24,004] Trial 4 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 112, 'max_depth': 8, 'min_samples_split': 9}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:24,588] Trial 5 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 135, 'max_depth': 19, 'min_samples_split': 2}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:24,893] Trial 6 finished with value: 0.691358024691358 and parameters: {'n_estimators': 81, 'max_depth': 7, 'min_samples_split': 7}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:25,575] Trial 7 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 178, 'max_depth': 14, 'min_samples_split': 2}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:26,344] Trial 8 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 169, 'max_depth': 16, 'min_samples_split': 4}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:26,656] Trial 9 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 142, 'max_depth': 5, 'min_samples_split': 6}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:27,147] Trial 10 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 196, 'max_depth': 10, 'min_samples_split': 10}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:27,855] Trial 11 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 145, 'max_depth': 14, 'min_samples_split': 7}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:28,250] Trial 12 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 152, 'max_depth': 13, 'min_samples_split': 8}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:28,691] Trial 13 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 105, 'max_depth': 10, 'min_samples_split': 10}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:28,899] Trial 14 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 55, 'max_depth': 14, 'min_samples_split': 7}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:29,646] Trial 15 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 156, 'max_depth': 8, 'min_samples_split': 8}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:30,414] Trial 16 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 110, 'max_depth': 12, 'min_samples_split': 5}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:31,090] Trial 17 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 199, 'max_depth': 16, 'min_samples_split': 4}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:31,464] Trial 18 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 157, 'max_depth': 5, 'min_samples_split': 9}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:38:31,852] Trial 19 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 134, 'max_depth': 9, 'min_samples_split': 7}. Best is trial 1 with value: 0.7283950617283951.
Best params for RandomForest: {'n_estimators': 150, 'max_depth': 9, 'min_samples_split': 10}
Tuning XGBoost with Optuna...
[I 2025-05-01 22:38:32,363] A new study created in memory with name: no-name-7d2a0b7e-86a8-410b-85a6-30e6092ecb33
[I 2025-05-01 22:38:33,010] Trial 0 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 295, 'max_depth': 8, 'learning_rate': 0.255724997672593}. Best is trial 0 with value: 0.7037037037037037.
[I 2025-05-01 22:38:33,403] Trial 1 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 244, 'max_depth': 9, 'learning_rate': 0.13806115957815202}. Best is trial 1 with value: 0.7098765432098766.
[I 2025-05-01 22:38:33,673] Trial 2 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 208, 'max_depth': 9, 'learning_rate': 0.2908597956721467}. Best is trial 1 with value: 0.7098765432098766.
[I 2025-05-01 22:38:33,900] Trial 3 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 166, 'max_depth': 7, 'learning_rate': 0.24929276630481945}. Best is trial 1 with value: 0.7098765432098766.
[I 2025-05-01 22:38:34,197] Trial 4 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 277, 'max_depth': 7, 'learning_rate': 0.2941604613582456}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:34,431] Trial 5 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 126, 'max_depth': 7, 'learning_rate': 0.18570917198078746}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:34,734] Trial 6 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 139, 'max_depth': 6, 'learning_rate': 0.21423198961239176}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:34,979] Trial 7 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 155, 'max_depth': 5, 'learning_rate': 0.26401320244252696}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:35,464] Trial 8 finished with value: 0.691358024691358 and parameters: {'n_estimators': 240, 'max_depth': 7, 'learning_rate': 0.05251285152487319}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:35,908] Trial 9 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 279, 'max_depth': 9, 'learning_rate': 0.11333313156081848}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:36,199] Trial 10 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 251, 'max_depth': 3, 'learning_rate': 0.029999395916382343}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:36,502] Trial 11 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 257, 'max_depth': 3, 'learning_rate': 0.04118078655566396}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:36,766] Trial 12 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 210, 'max_depth': 4, 'learning_rate': 0.10157139576902678}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:37,128] Trial 13 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 274, 'max_depth': 5, 'learning_rate': 0.07667118628364022}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:37,732] Trial 14 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 225, 'max_depth': 3, 'learning_rate': 0.01764541370146755}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:38,431] Trial 15 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 180, 'max_depth': 5, 'learning_rate': 0.1845279883564858}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:38,952] Trial 16 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 296, 'max_depth': 10, 'learning_rate': 0.15258456757316813}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:39,471] Trial 17 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 264, 'max_depth': 6, 'learning_rate': 0.20585517895237887}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:39,848] Trial 18 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 105, 'max_depth': 4, 'learning_rate': 0.11654943406294607}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:38:40,259] Trial 19 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 237, 'max_depth': 8, 'learning_rate': 0.2922138403593238}. Best is trial 4 with value: 0.7345679012345679.
Best params for XGBoost: {'n_estimators': 277, 'max_depth': 7, 'learning_rate': 0.2941604613582456}
Best model: XGBoost
01-05-2025 22:38:40 Retraining best model on original full dataset...
Requirement already satisfied: wandb in c:\users\x-hp\anaconda3\lib\site-packages (0.19.8)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.1.43)
Requirement already satisfied: platformdirs in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (4.25.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (5.9.0)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.8.2)
Requirement already satisfied: pyyaml in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (6.0.1)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.18.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (75.1.0)
Requirement already satisfied: colorama in c:\users\x-hp\anaconda3\lib\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)
Requirement already satisfied: typing-extensions>=4.6.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)
Requirement already satisfied: smmap<5,>=3.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.login() after wandb.init() has no effect.
01-05-2025 22:41:39 Downloading and reading train artifact
[34m[1mwandb[0m:   1 of 1 files downloaded.
01-05-2025 22:41:42 Spliting data into train/val
01-05-2025 22:41:42 x train: (375, 28)
01-05-2025 22:41:42 y train: (375,)
01-05-2025 22:41:42 x val: (162, 28)
01-05-2025 22:41:42 y val: (162,)
01-05-2025 22:41:42 Outlier Removal
01-05-2025 22:41:42 x_train shape [original]: (375, 28)
01-05-2025 22:41:42 x_train shape [outlier removal]: (357, 28)
01-05-2025 22:41:43 Data after SMOTE: (476, 28)
01-05-2025 22:41:43 [Logistic] Accuracy: 0.7469135802469136
01-05-2025 22:41:44 [Random Forest] Accuracy: 0.7222222222222222
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:41:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
01-05-2025 22:41:44 [XGBoost] Accuracy: 0.7222222222222222
                 Model  Accuracy
0  Logistic Regression  0.746914
1        Random Forest  0.722222
2              XGBoost  0.722222
Tuning RandomForest with Optuna...
[I 2025-05-01 22:41:48,690] A new study created in memory with name: no-name-ec798fd7-f4bd-4db6-a9a0-cf0fe3c2ff7e
[I 2025-05-01 22:41:49,403] Trial 0 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 193, 'max_depth': 15, 'min_samples_split': 4}. Best is trial 0 with value: 0.7160493827160493.
[I 2025-05-01 22:41:49,871] Trial 1 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 69, 'max_depth': 13, 'min_samples_split': 3}. Best is trial 0 with value: 0.7160493827160493.
[I 2025-05-01 22:41:50,707] Trial 2 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 84, 'max_depth': 20, 'min_samples_split': 5}. Best is trial 0 with value: 0.7160493827160493.
[I 2025-05-01 22:41:51,901] Trial 3 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 180, 'max_depth': 12, 'min_samples_split': 9}. Best is trial 0 with value: 0.7160493827160493.
[I 2025-05-01 22:41:52,586] Trial 4 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 84, 'max_depth': 7, 'min_samples_split': 5}. Best is trial 0 with value: 0.7160493827160493.
[I 2025-05-01 22:41:53,318] Trial 5 finished with value: 0.6790123456790124 and parameters: {'n_estimators': 157, 'max_depth': 7, 'min_samples_split': 8}. Best is trial 0 with value: 0.7160493827160493.
[I 2025-05-01 22:41:53,641] Trial 6 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 90, 'max_depth': 18, 'min_samples_split': 4}. Best is trial 0 with value: 0.7160493827160493.
[I 2025-05-01 22:41:54,044] Trial 7 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 111, 'max_depth': 9, 'min_samples_split': 9}. Best is trial 0 with value: 0.7160493827160493.
[I 2025-05-01 22:41:54,684] Trial 8 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 172, 'max_depth': 14, 'min_samples_split': 2}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:55,195] Trial 9 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 175, 'max_depth': 10, 'min_samples_split': 5}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:55,661] Trial 10 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 144, 'max_depth': 16, 'min_samples_split': 2}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:56,213] Trial 11 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 192, 'max_depth': 15, 'min_samples_split': 2}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:56,756] Trial 12 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 199, 'max_depth': 15, 'min_samples_split': 7}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:57,183] Trial 13 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 154, 'max_depth': 13, 'min_samples_split': 7}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:57,728] Trial 14 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 200, 'max_depth': 17, 'min_samples_split': 7}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:58,119] Trial 15 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 133, 'max_depth': 19, 'min_samples_split': 10}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:58,847] Trial 16 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 164, 'max_depth': 11, 'min_samples_split': 7}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:59,309] Trial 17 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 122, 'max_depth': 14, 'min_samples_split': 6}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:41:59,868] Trial 18 finished with value: 0.691358024691358 and parameters: {'n_estimators': 174, 'max_depth': 5, 'min_samples_split': 6}. Best is trial 8 with value: 0.7283950617283951.
[I 2025-05-01 22:42:00,436] Trial 19 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 182, 'max_depth': 17, 'min_samples_split': 3}. Best is trial 8 with value: 0.7283950617283951.
Best params for RandomForest: {'n_estimators': 172, 'max_depth': 14, 'min_samples_split': 2}
Tuning XGBoost with Optuna...
[I 2025-05-01 22:42:01,067] A new study created in memory with name: no-name-bb942afe-a4eb-4f68-9980-d932f5221736
[I 2025-05-01 22:42:01,408] Trial 0 finished with value: 0.691358024691358 and parameters: {'n_estimators': 155, 'max_depth': 10, 'learning_rate': 0.11407727412170478}. Best is trial 0 with value: 0.691358024691358.
[I 2025-05-01 22:42:01,692] Trial 1 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 194, 'max_depth': 10, 'learning_rate': 0.27084710573533055}. Best is trial 1 with value: 0.7037037037037037.
[I 2025-05-01 22:42:01,935] Trial 2 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 234, 'max_depth': 3, 'learning_rate': 0.2652306339277749}. Best is trial 2 with value: 0.7283950617283951.
[I 2025-05-01 22:42:02,112] Trial 3 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 108, 'max_depth': 4, 'learning_rate': 0.16062962618601992}. Best is trial 2 with value: 0.7283950617283951.
[I 2025-05-01 22:42:02,474] Trial 4 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 297, 'max_depth': 9, 'learning_rate': 0.278021608979701}. Best is trial 2 with value: 0.7283950617283951.
[I 2025-05-01 22:42:02,818] Trial 5 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 297, 'max_depth': 5, 'learning_rate': 0.29253871616798366}. Best is trial 2 with value: 0.7283950617283951.
[I 2025-05-01 22:42:03,204] Trial 6 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 290, 'max_depth': 4, 'learning_rate': 0.1220454596118358}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:03,844] Trial 7 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 244, 'max_depth': 8, 'learning_rate': 0.014720313613836283}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:04,399] Trial 8 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 246, 'max_depth': 4, 'learning_rate': 0.11383519765282792}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:05,051] Trial 9 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 262, 'max_depth': 7, 'learning_rate': 0.055255486813114194}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:05,660] Trial 10 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 191, 'max_depth': 6, 'learning_rate': 0.1937996026207913}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:05,928] Trial 11 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 226, 'max_depth': 3, 'learning_rate': 0.21681637446059726}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:06,212] Trial 12 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 277, 'max_depth': 3, 'learning_rate': 0.11278637546014736}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:06,476] Trial 13 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 218, 'max_depth': 5, 'learning_rate': 0.23091100675302997}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:06,655] Trial 14 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 163, 'max_depth': 3, 'learning_rate': 0.16046788642338752}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:07,025] Trial 15 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 272, 'max_depth': 5, 'learning_rate': 0.07372671885201551}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:07,557] Trial 16 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 223, 'max_depth': 4, 'learning_rate': 0.19454717229927518}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:07,895] Trial 17 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 257, 'max_depth': 6, 'learning_rate': 0.25111416508042644}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:08,132] Trial 18 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 162, 'max_depth': 4, 'learning_rate': 0.14317083905089836}. Best is trial 6 with value: 0.7345679012345679.
[I 2025-05-01 22:42:08,619] Trial 19 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 283, 'max_depth': 7, 'learning_rate': 0.07298820437115572}. Best is trial 6 with value: 0.7345679012345679.
Best params for XGBoost: {'n_estimators': 290, 'max_depth': 4, 'learning_rate': 0.1220454596118358}
Best model: XGBoost
01-05-2025 22:42:09 Retraining best model on original full dataset...
01-05-2025 22:46:34 Retraining best model on original full dataset...
01-05-2025 22:46:47 Retraining best model on original full dataset...
01-05-2025 22:47:10 Retraining best model on original full dataset...
01-05-2025 22:47:31 Retraining best model on original full dataset...
01-05-2025 22:47:31 ✅ Exported and logged final model to W&B as artifact: final_model_xgboost.pkl
Requirement already satisfied: wandb in c:\users\x-hp\anaconda3\lib\site-packages (0.19.8)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.1.43)
Requirement already satisfied: platformdirs in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (4.25.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (5.9.0)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.8.2)
Requirement already satisfied: pyyaml in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (6.0.1)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.18.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (75.1.0)
Requirement already satisfied: colorama in c:\users\x-hp\anaconda3\lib\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)
Requirement already satisfied: typing-extensions>=4.6.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)
Requirement already satisfied: smmap<5,>=3.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.login() after wandb.init() has no effect.
01-05-2025 22:49:22 Downloading and reading train artifact
[34m[1mwandb[0m:   1 of 1 files downloaded.
01-05-2025 22:49:24 Spliting data into train/val
01-05-2025 22:49:24 x train: (375, 28)
01-05-2025 22:49:24 y train: (375,)
01-05-2025 22:49:24 x val: (162, 28)
01-05-2025 22:49:24 y val: (162,)
01-05-2025 22:49:24 Outlier Removal
01-05-2025 22:49:25 x_train shape [original]: (375, 28)
01-05-2025 22:49:25 x_train shape [outlier removal]: (357, 28)
01-05-2025 22:49:25 Data after SMOTE: (476, 28)
01-05-2025 22:49:25 [Logistic] Accuracy: 0.7469135802469136
01-05-2025 22:49:25 [Random Forest] Accuracy: 0.7222222222222222
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:49:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
01-05-2025 22:49:26 [XGBoost] Accuracy: 0.7222222222222222
                 Model  Accuracy
0  Logistic Regression  0.746914
1        Random Forest  0.722222
2              XGBoost  0.722222
Tuning RandomForest with Optuna...
[I 2025-05-01 22:49:28,225] A new study created in memory with name: no-name-a80d3701-b940-4592-b7da-e0dfb0e2b458
[I 2025-05-01 22:49:29,538] Trial 0 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 185, 'max_depth': 18, 'min_samples_split': 8}. Best is trial 0 with value: 0.7222222222222222.
[I 2025-05-01 22:49:29,941] Trial 1 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 110, 'max_depth': 7, 'min_samples_split': 6}. Best is trial 0 with value: 0.7222222222222222.
[I 2025-05-01 22:49:30,455] Trial 2 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 100, 'max_depth': 9, 'min_samples_split': 10}. Best is trial 0 with value: 0.7222222222222222.
[I 2025-05-01 22:49:30,899] Trial 3 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 133, 'max_depth': 11, 'min_samples_split': 3}. Best is trial 0 with value: 0.7222222222222222.
[I 2025-05-01 22:49:31,072] Trial 4 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 56, 'max_depth': 15, 'min_samples_split': 7}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:31,861] Trial 5 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 184, 'max_depth': 13, 'min_samples_split': 10}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:32,346] Trial 6 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 194, 'max_depth': 11, 'min_samples_split': 7}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:32,780] Trial 7 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 142, 'max_depth': 14, 'min_samples_split': 10}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:33,113] Trial 8 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 98, 'max_depth': 11, 'min_samples_split': 6}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:33,373] Trial 9 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 76, 'max_depth': 14, 'min_samples_split': 9}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:33,579] Trial 10 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 50, 'max_depth': 20, 'min_samples_split': 4}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:34,033] Trial 11 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 157, 'max_depth': 18, 'min_samples_split': 8}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:34,496] Trial 12 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 159, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:34,668] Trial 13 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 53, 'max_depth': 18, 'min_samples_split': 8}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:35,101] Trial 14 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 173, 'max_depth': 17, 'min_samples_split': 7}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:35,304] Trial 15 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 75, 'max_depth': 16, 'min_samples_split': 2}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:35,600] Trial 16 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 119, 'max_depth': 19, 'min_samples_split': 8}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:35,840] Trial 17 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 82, 'max_depth': 15, 'min_samples_split': 5}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:36,199] Trial 18 finished with value: 0.691358024691358 and parameters: {'n_estimators': 150, 'max_depth': 5, 'min_samples_split': 7}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:36,784] Trial 19 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 200, 'max_depth': 20, 'min_samples_split': 9}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:37,149] Trial 20 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 129, 'max_depth': 20, 'min_samples_split': 9}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:37,771] Trial 21 finished with value: 0.691358024691358 and parameters: {'n_estimators': 198, 'max_depth': 18, 'min_samples_split': 9}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:38,398] Trial 22 finished with value: 0.691358024691358 and parameters: {'n_estimators': 172, 'max_depth': 20, 'min_samples_split': 8}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:38,909] Trial 23 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 182, 'max_depth': 16, 'min_samples_split': 7}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:39,401] Trial 24 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 198, 'max_depth': 17, 'min_samples_split': 9}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:39,818] Trial 25 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 169, 'max_depth': 19, 'min_samples_split': 6}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:40,223] Trial 26 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 185, 'max_depth': 14, 'min_samples_split': 8}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:40,393] Trial 27 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 68, 'max_depth': 17, 'min_samples_split': 9}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:40,643] Trial 28 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 93, 'max_depth': 19, 'min_samples_split': 5}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:41,002] Trial 29 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 89, 'max_depth': 19, 'min_samples_split': 5}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:41,260] Trial 30 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 61, 'max_depth': 15, 'min_samples_split': 4}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:41,559] Trial 31 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 113, 'max_depth': 19, 'min_samples_split': 6}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:41,834] Trial 32 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 105, 'max_depth': 18, 'min_samples_split': 7}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:42,100] Trial 33 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 98, 'max_depth': 8, 'min_samples_split': 4}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:42,469] Trial 34 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 140, 'max_depth': 20, 'min_samples_split': 6}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:42,952] Trial 35 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 187, 'max_depth': 12, 'min_samples_split': 10}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:43,192] Trial 36 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 62, 'max_depth': 17, 'min_samples_split': 3}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:43,494] Trial 37 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 92, 'max_depth': 13, 'min_samples_split': 7}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:43,817] Trial 38 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 114, 'max_depth': 15, 'min_samples_split': 5}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:44,296] Trial 39 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 189, 'max_depth': 9, 'min_samples_split': 10}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:44,528] Trial 40 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 84, 'max_depth': 19, 'min_samples_split': 8}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:44,791] Trial 41 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 82, 'max_depth': 19, 'min_samples_split': 8}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:45,000] Trial 42 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 72, 'max_depth': 20, 'min_samples_split': 9}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:45,305] Trial 43 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 122, 'max_depth': 18, 'min_samples_split': 8}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:45,466] Trial 44 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 56, 'max_depth': 19, 'min_samples_split': 7}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:45,683] Trial 45 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 85, 'max_depth': 18, 'min_samples_split': 9}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:45,964] Trial 46 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 106, 'max_depth': 17, 'min_samples_split': 8}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:46,425] Trial 47 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 180, 'max_depth': 16, 'min_samples_split': 7}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:46,625] Trial 48 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 68, 'max_depth': 19, 'min_samples_split': 10}. Best is trial 4 with value: 0.7345679012345679.
[I 2025-05-01 22:49:47,019] Trial 49 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 159, 'max_depth': 20, 'min_samples_split': 5}. Best is trial 4 with value: 0.7345679012345679.
Best params for RandomForest: {'n_estimators': 56, 'max_depth': 15, 'min_samples_split': 7}
Tuning XGBoost with Optuna...
[I 2025-05-01 22:49:47,171] A new study created in memory with name: no-name-a3dba2bd-650f-4252-b065-f9ba2044a561
[I 2025-05-01 22:49:47,315] Trial 0 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 162, 'max_depth': 3, 'learning_rate': 0.020319717179109815}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:47,494] Trial 1 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 183, 'max_depth': 4, 'learning_rate': 0.21889460485902118}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:47,703] Trial 2 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 103, 'max_depth': 6, 'learning_rate': 0.017017448772590135}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:47,980] Trial 3 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 145, 'max_depth': 7, 'learning_rate': 0.04775113435810176}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:48,267] Trial 4 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 172, 'max_depth': 9, 'learning_rate': 0.29686928853439465}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:48,531] Trial 5 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 254, 'max_depth': 3, 'learning_rate': 0.17536557691650057}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:48,795] Trial 6 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 186, 'max_depth': 5, 'learning_rate': 0.11076200380724183}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:49,335] Trial 7 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 297, 'max_depth': 7, 'learning_rate': 0.284560181987242}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:49,567] Trial 8 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 240, 'max_depth': 4, 'learning_rate': 0.25014839355769575}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:49,765] Trial 9 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 186, 'max_depth': 4, 'learning_rate': 0.1457794498396206}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:50,061] Trial 10 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 121, 'max_depth': 10, 'learning_rate': 0.08881879797883485}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:50,249] Trial 11 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 225, 'max_depth': 3, 'learning_rate': 0.20417286939275958}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:50,420] Trial 12 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 151, 'max_depth': 5, 'learning_rate': 0.23544699114672082}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:50,616] Trial 13 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 216, 'max_depth': 3, 'learning_rate': 0.1375185987998885}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:50,822] Trial 14 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 154, 'max_depth': 5, 'learning_rate': 0.1977396793377238}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:51,069] Trial 15 finished with value: 0.691358024691358 and parameters: {'n_estimators': 210, 'max_depth': 4, 'learning_rate': 0.07902199293076409}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:51,293] Trial 16 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 135, 'max_depth': 8, 'learning_rate': 0.2359367610400286}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:51,640] Trial 17 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 171, 'max_depth': 6, 'learning_rate': 0.016388827462773148}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:51,866] Trial 18 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 190, 'max_depth': 4, 'learning_rate': 0.1828604654902077}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:52,134] Trial 19 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 280, 'max_depth': 3, 'learning_rate': 0.11743332021727543}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:52,410] Trial 20 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 167, 'max_depth': 5, 'learning_rate': 0.05377149553325847}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:52,732] Trial 21 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.2877810234229219}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:53,037] Trial 22 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 260, 'max_depth': 7, 'learning_rate': 0.2645648662546273}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:53,391] Trial 23 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 202, 'max_depth': 8, 'learning_rate': 0.21812631735980959}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:53,749] Trial 24 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 230, 'max_depth': 8, 'learning_rate': 0.2665851640748621}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:53,978] Trial 25 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 117, 'max_depth': 6, 'learning_rate': 0.16428331182482303}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:54,274] Trial 26 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 288, 'max_depth': 4, 'learning_rate': 0.2794249460471097}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:54,618] Trial 27 finished with value: 0.691358024691358 and parameters: {'n_estimators': 265, 'max_depth': 9, 'learning_rate': 0.22687271415798754}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:54,806] Trial 28 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 159, 'max_depth': 3, 'learning_rate': 0.2021701405597685}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:55,024] Trial 29 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 138, 'max_depth': 6, 'learning_rate': 0.2666202080792595}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:55,313] Trial 30 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 196, 'max_depth': 5, 'learning_rate': 0.06181329831921274}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:55,546] Trial 31 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 243, 'max_depth': 4, 'learning_rate': 0.24739680922608093}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:55,775] Trial 32 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 237, 'max_depth': 4, 'learning_rate': 0.2524797075704461}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:56,140] Trial 33 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 180, 'max_depth': 7, 'learning_rate': 0.02843799056298793}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:56,364] Trial 34 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.29580021452131167}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:56,687] Trial 35 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 274, 'max_depth': 6, 'learning_rate': 0.21453218980991307}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:57,159] Trial 36 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 205, 'max_depth': 4, 'learning_rate': 0.248531244661451}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:57,467] Trial 37 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2702819611272846}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:57,608] Trial 38 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.18962715979475708}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:57,901] Trial 39 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 180, 'max_depth': 9, 'learning_rate': 0.1638841670840967}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 22:49:58,122] Trial 40 finished with value: 0.7530864197530864 and parameters: {'n_estimators': 218, 'max_depth': 3, 'learning_rate': 0.29837475808045516}. Best is trial 40 with value: 0.7530864197530864.
[I 2025-05-01 22:49:58,340] Trial 41 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 224, 'max_depth': 3, 'learning_rate': 0.2949048311596915}. Best is trial 40 with value: 0.7530864197530864.
[I 2025-05-01 22:49:58,655] Trial 42 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 222, 'max_depth': 3, 'learning_rate': 0.2963795284546778}. Best is trial 40 with value: 0.7530864197530864.
[I 2025-05-01 22:49:58,906] Trial 43 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 215, 'max_depth': 3, 'learning_rate': 0.2847436649459752}. Best is trial 40 with value: 0.7530864197530864.
[I 2025-05-01 22:49:59,143] Trial 44 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 193, 'max_depth': 3, 'learning_rate': 0.12967025129537887}. Best is trial 40 with value: 0.7530864197530864.
[I 2025-05-01 22:49:59,357] Trial 45 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 166, 'max_depth': 4, 'learning_rate': 0.2993840191846296}. Best is trial 40 with value: 0.7530864197530864.
[I 2025-05-01 22:49:59,534] Trial 46 finished with value: 0.691358024691358 and parameters: {'n_estimators': 141, 'max_depth': 3, 'learning_rate': 0.27678370267654423}. Best is trial 40 with value: 0.7530864197530864.
[I 2025-05-01 22:49:59,756] Trial 47 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 178, 'max_depth': 4, 'learning_rate': 0.23450345332658007}. Best is trial 40 with value: 0.7530864197530864.
[I 2025-05-01 22:49:59,921] Trial 48 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 130, 'max_depth': 3, 'learning_rate': 0.09240781087195468}. Best is trial 40 with value: 0.7530864197530864.
[I 2025-05-01 22:50:00,240] Trial 49 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 232, 'max_depth': 10, 'learning_rate': 0.25367317489773733}. Best is trial 40 with value: 0.7530864197530864.
Best params for XGBoost: {'n_estimators': 218, 'max_depth': 3, 'learning_rate': 0.29837475808045516}
Best model: XGBoost
01-05-2025 22:50:00 Retraining best model on original full dataset...
01-05-2025 22:50:01 ✅ Exported and logged final model to W&B as artifact: final_model_xgboost.pkl
Requirement already satisfied: wandb in c:\users\x-hp\anaconda3\lib\site-packages (0.19.8)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.1.43)
Requirement already satisfied: platformdirs in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (4.25.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (5.9.0)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.8.2)
Requirement already satisfied: pyyaml in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (6.0.1)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.18.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (75.1.0)
Requirement already satisfied: colorama in c:\users\x-hp\anaconda3\lib\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)
Requirement already satisfied: typing-extensions>=4.6.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)
Requirement already satisfied: smmap<5,>=3.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.login() after wandb.init() has no effect.
01-05-2025 22:52:49 Downloading and reading train artifact
[34m[1mwandb[0m:   1 of 1 files downloaded.
01-05-2025 22:52:51 Spliting data into train/val
01-05-2025 22:52:51 x train: (375, 28)
01-05-2025 22:52:51 y train: (375,)
01-05-2025 22:52:51 x val: (162, 28)
01-05-2025 22:52:51 y val: (162,)
01-05-2025 22:52:52 Outlier Removal
01-05-2025 22:52:52 x_train shape [original]: (375, 28)
01-05-2025 22:52:52 x_train shape [outlier removal]: (357, 28)
01-05-2025 22:52:52 Data after SMOTE: (476, 28)
01-05-2025 22:52:52 [Logistic] Accuracy: 0.7469135802469136
01-05-2025 22:52:53 [Random Forest] Accuracy: 0.7222222222222222
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [22:52:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
01-05-2025 22:52:53 [XGBoost] Accuracy: 0.7222222222222222
                 Model  Accuracy
0  Logistic Regression  0.746914
1        Random Forest  0.722222
2              XGBoost  0.722222
Tuning RandomForest with Optuna...
[I 2025-05-01 22:52:55,110] A new study created in memory with name: no-name-33d3d127-bbfc-4a4a-ba81-264fd7954f02
[I 2025-05-01 22:52:55,780] Trial 0 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 133, 'max_depth': 13, 'min_samples_split': 10}. Best is trial 0 with value: 0.7037037037037037.
[I 2025-05-01 22:52:56,373] Trial 1 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 111, 'max_depth': 13, 'min_samples_split': 3}. Best is trial 1 with value: 0.7160493827160493.
[I 2025-05-01 22:52:56,805] Trial 2 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 182, 'max_depth': 17, 'min_samples_split': 5}. Best is trial 1 with value: 0.7160493827160493.
[I 2025-05-01 22:52:57,200] Trial 3 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 195, 'max_depth': 5, 'min_samples_split': 7}. Best is trial 1 with value: 0.7160493827160493.
[I 2025-05-01 22:52:57,395] Trial 4 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 78, 'max_depth': 15, 'min_samples_split': 8}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:52:57,768] Trial 5 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 161, 'max_depth': 12, 'min_samples_split': 4}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:52:58,262] Trial 6 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 120, 'max_depth': 10, 'min_samples_split': 2}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:52:58,467] Trial 7 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 93, 'max_depth': 10, 'min_samples_split': 8}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:52:58,724] Trial 8 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 106, 'max_depth': 20, 'min_samples_split': 7}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:52:59,083] Trial 9 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 165, 'max_depth': 12, 'min_samples_split': 9}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:52:59,235] Trial 10 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 56, 'max_depth': 17, 'min_samples_split': 6}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:52:59,406] Trial 11 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 57, 'max_depth': 16, 'min_samples_split': 4}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:52:59,762] Trial 12 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 159, 'max_depth': 5, 'min_samples_split': 5}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:52:59,973] Trial 13 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 80, 'max_depth': 15, 'min_samples_split': 3}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:53:00,311] Trial 14 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 143, 'max_depth': 8, 'min_samples_split': 8}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:53:00,507] Trial 15 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 76, 'max_depth': 20, 'min_samples_split': 10}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 22:53:00,893] Trial 16 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 150, 'max_depth': 14, 'min_samples_split': 5}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:01,238] Trial 17 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 142, 'max_depth': 15, 'min_samples_split': 6}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:01,476] Trial 18 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 93, 'max_depth': 18, 'min_samples_split': 8}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:01,687] Trial 19 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 77, 'max_depth': 14, 'min_samples_split': 7}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:02,118] Trial 20 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 177, 'max_depth': 10, 'min_samples_split': 5}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:02,483] Trial 21 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 155, 'max_depth': 12, 'min_samples_split': 4}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:02,798] Trial 22 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 131, 'max_depth': 11, 'min_samples_split': 4}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:03,205] Trial 23 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 174, 'max_depth': 8, 'min_samples_split': 6}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:03,579] Trial 24 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 149, 'max_depth': 14, 'min_samples_split': 2}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:04,021] Trial 25 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 188, 'max_depth': 18, 'min_samples_split': 3}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:04,401] Trial 26 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 170, 'max_depth': 15, 'min_samples_split': 9}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:04,701] Trial 27 finished with value: 0.691358024691358 and parameters: {'n_estimators': 121, 'max_depth': 14, 'min_samples_split': 5}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:05,008] Trial 28 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 140, 'max_depth': 7, 'min_samples_split': 4}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:05,488] Trial 29 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 134, 'max_depth': 13, 'min_samples_split': 9}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:05,838] Trial 30 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 158, 'max_depth': 11, 'min_samples_split': 7}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:05,986] Trial 31 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 55, 'max_depth': 16, 'min_samples_split': 4}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:06,151] Trial 32 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 64, 'max_depth': 16, 'min_samples_split': 3}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:06,342] Trial 33 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 68, 'max_depth': 13, 'min_samples_split': 3}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:06,609] Trial 34 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 106, 'max_depth': 16, 'min_samples_split': 2}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:06,863] Trial 35 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 96, 'max_depth': 18, 'min_samples_split': 2}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:07,117] Trial 36 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 107, 'max_depth': 19, 'min_samples_split': 2}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:07,351] Trial 37 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 90, 'max_depth': 17, 'min_samples_split': 2}. Best is trial 16 with value: 0.7283950617283951.
[I 2025-05-01 22:53:07,642] Trial 38 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 112, 'max_depth': 18, 'min_samples_split': 3}. Best is trial 38 with value: 0.7345679012345679.
[I 2025-05-01 22:53:07,932] Trial 39 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 114, 'max_depth': 16, 'min_samples_split': 3}. Best is trial 38 with value: 0.7345679012345679.
[I 2025-05-01 22:53:08,292] Trial 40 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 127, 'max_depth': 19, 'min_samples_split': 3}. Best is trial 38 with value: 0.7345679012345679.
[I 2025-05-01 22:53:08,589] Trial 41 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 100, 'max_depth': 18, 'min_samples_split': 2}. Best is trial 38 with value: 0.7345679012345679.
[I 2025-05-01 22:53:08,943] Trial 42 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 115, 'max_depth': 17, 'min_samples_split': 2}. Best is trial 38 with value: 0.7345679012345679.
[I 2025-05-01 22:53:09,217] Trial 43 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 99, 'max_depth': 19, 'min_samples_split': 3}. Best is trial 38 with value: 0.7345679012345679.
[I 2025-05-01 22:53:09,554] Trial 44 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 102, 'max_depth': 16, 'min_samples_split': 2}. Best is trial 38 with value: 0.7345679012345679.
[I 2025-05-01 22:53:09,746] Trial 45 finished with value: 0.7407407407407407 and parameters: {'n_estimators': 65, 'max_depth': 18, 'min_samples_split': 2}. Best is trial 45 with value: 0.7407407407407407.
[I 2025-05-01 22:53:09,909] Trial 46 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 50, 'max_depth': 18, 'min_samples_split': 3}. Best is trial 45 with value: 0.7407407407407407.
[I 2025-05-01 22:53:10,102] Trial 47 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 63, 'max_depth': 20, 'min_samples_split': 5}. Best is trial 45 with value: 0.7407407407407407.
[I 2025-05-01 22:53:10,333] Trial 48 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 87, 'max_depth': 17, 'min_samples_split': 3}. Best is trial 45 with value: 0.7407407407407407.
[I 2025-05-01 22:53:10,571] Trial 49 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 85, 'max_depth': 19, 'min_samples_split': 2}. Best is trial 45 with value: 0.7407407407407407.
[I 2025-05-01 22:53:10,767] Trial 50 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 69, 'max_depth': 18, 'min_samples_split': 4}. Best is trial 45 with value: 0.7407407407407407.
[I 2025-05-01 22:53:11,059] Trial 51 finished with value: 0.7530864197530864 and parameters: {'n_estimators': 108, 'max_depth': 17, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:11,444] Trial 52 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 120, 'max_depth': 17, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:11,705] Trial 53 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 70, 'max_depth': 14, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:11,970] Trial 54 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 82, 'max_depth': 15, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:12,491] Trial 55 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 109, 'max_depth': 20, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:12,699] Trial 56 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 63, 'max_depth': 15, 'min_samples_split': 4}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:13,203] Trial 57 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 198, 'max_depth': 18, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:13,563] Trial 58 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 74, 'max_depth': 17, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:13,899] Trial 59 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 135, 'max_depth': 19, 'min_samples_split': 4}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:14,183] Trial 60 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 117, 'max_depth': 16, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:14,450] Trial 61 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 102, 'max_depth': 16, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:14,800] Trial 62 finished with value: 0.691358024691358 and parameters: {'n_estimators': 112, 'max_depth': 17, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:15,091] Trial 63 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 104, 'max_depth': 15, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:15,442] Trial 64 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 126, 'max_depth': 18, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:15,724] Trial 65 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 94, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:16,137] Trial 66 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 151, 'max_depth': 14, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:16,493] Trial 67 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 122, 'max_depth': 17, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:16,886] Trial 68 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 122, 'max_depth': 17, 'min_samples_split': 7}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:17,270] Trial 69 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 139, 'max_depth': 18, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:17,611] Trial 70 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 131, 'max_depth': 19, 'min_samples_split': 6}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:17,915] Trial 71 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 111, 'max_depth': 17, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:18,182] Trial 72 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 99, 'max_depth': 15, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:18,482] Trial 73 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 118, 'max_depth': 16, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:18,815] Trial 74 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 147, 'max_depth': 18, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:19,111] Trial 75 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 124, 'max_depth': 16, 'min_samples_split': 6}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:19,369] Trial 76 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 107, 'max_depth': 17, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:19,765] Trial 77 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 163, 'max_depth': 20, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:19,998] Trial 78 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 91, 'max_depth': 13, 'min_samples_split': 4}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:20,183] Trial 79 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 61, 'max_depth': 13, 'min_samples_split': 4}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:20,426] Trial 80 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 89, 'max_depth': 11, 'min_samples_split': 5}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:20,695] Trial 81 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 96, 'max_depth': 12, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:20,958] Trial 82 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 99, 'max_depth': 14, 'min_samples_split': 4}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:21,119] Trial 83 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 51, 'max_depth': 12, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:21,279] Trial 84 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 52, 'max_depth': 12, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:21,459] Trial 85 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 60, 'max_depth': 13, 'min_samples_split': 5}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:21,723] Trial 86 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 74, 'max_depth': 11, 'min_samples_split': 4}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:22,140] Trial 87 finished with value: 0.7407407407407407 and parameters: {'n_estimators': 56, 'max_depth': 10, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:22,398] Trial 88 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 82, 'max_depth': 9, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:22,568] Trial 89 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 54, 'max_depth': 9, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:22,742] Trial 90 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 55, 'max_depth': 10, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:22,935] Trial 91 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 66, 'max_depth': 7, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:23,120] Trial 92 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 58, 'max_depth': 12, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:23,291] Trial 93 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 51, 'max_depth': 18, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:23,473] Trial 94 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 52, 'max_depth': 19, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:23,651] Trial 95 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 58, 'max_depth': 18, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:23,914] Trial 96 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 114, 'max_depth': 9, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:24,119] Trial 97 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 70, 'max_depth': 10, 'min_samples_split': 10}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:24,274] Trial 98 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 50, 'max_depth': 18, 'min_samples_split': 3}. Best is trial 51 with value: 0.7530864197530864.
[I 2025-05-01 22:53:24,580] Trial 99 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 129, 'max_depth': 11, 'min_samples_split': 2}. Best is trial 51 with value: 0.7530864197530864.
Best params for RandomForest: {'n_estimators': 108, 'max_depth': 17, 'min_samples_split': 2}
Tuning XGBoost with Optuna...
[I 2025-05-01 22:53:24,861] A new study created in memory with name: no-name-ca68aacd-462e-4bd9-90a0-e7c02c10831c
[I 2025-05-01 22:53:25,139] Trial 0 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 229, 'max_depth': 5, 'learning_rate': 0.047306516517450936}. Best is trial 0 with value: 0.7098765432098766.
[I 2025-05-01 22:53:25,297] Trial 1 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 110, 'max_depth': 7, 'learning_rate': 0.2715754711507908}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:53:25,567] Trial 2 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 285, 'max_depth': 9, 'learning_rate': 0.2571073690101245}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:53:25,851] Trial 3 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 156, 'max_depth': 6, 'learning_rate': 0.026013818114806755}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:53:26,094] Trial 4 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 264, 'max_depth': 4, 'learning_rate': 0.21903320448381486}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:53:26,451] Trial 5 finished with value: 0.691358024691358 and parameters: {'n_estimators': 183, 'max_depth': 7, 'learning_rate': 0.017744616972639488}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:53:26,684] Trial 6 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 186, 'max_depth': 4, 'learning_rate': 0.1481455883195592}. Best is trial 1 with value: 0.7283950617283951.
[I 2025-05-01 22:53:27,018] Trial 7 finished with value: 0.7407407407407407 and parameters: {'n_estimators': 230, 'max_depth': 4, 'learning_rate': 0.015587859161726298}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:27,236] Trial 8 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 128, 'max_depth': 9, 'learning_rate': 0.28809755485800825}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:27,675] Trial 9 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 223, 'max_depth': 9, 'learning_rate': 0.06146254222233353}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:27,920] Trial 10 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 248, 'max_depth': 3, 'learning_rate': 0.11888161210074026}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:28,117] Trial 11 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.20089938237176225}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:28,396] Trial 12 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 152, 'max_depth': 6, 'learning_rate': 0.09200145967497698}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:28,707] Trial 13 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 205, 'max_depth': 8, 'learning_rate': 0.19321385416918718}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:29,077] Trial 14 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 108, 'max_depth': 5, 'learning_rate': 0.2982687231477308}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:29,235] Trial 15 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 155, 'max_depth': 3, 'learning_rate': 0.16006166136692046}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:29,566] Trial 16 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 290, 'max_depth': 8, 'learning_rate': 0.24788317095876206}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:29,952] Trial 17 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 211, 'max_depth': 10, 'learning_rate': 0.10041033646987232}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:30,357] Trial 18 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 255, 'max_depth': 5, 'learning_rate': 0.15350497067074967}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:30,641] Trial 19 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 177, 'max_depth': 4, 'learning_rate': 0.24624936659837812}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:30,947] Trial 20 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 123, 'max_depth': 6, 'learning_rate': 0.06835896618575357}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:31,222] Trial 21 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 182, 'max_depth': 4, 'learning_rate': 0.14857684568858998}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:31,454] Trial 22 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 235, 'max_depth': 3, 'learning_rate': 0.12133488070432981}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:31,663] Trial 23 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 134, 'max_depth': 5, 'learning_rate': 0.18386970689993204}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:31,958] Trial 24 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 191, 'max_depth': 4, 'learning_rate': 0.2672889818792664}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:32,324] Trial 25 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 270, 'max_depth': 7, 'learning_rate': 0.22419124830335907}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:32,735] Trial 26 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 214, 'max_depth': 8, 'learning_rate': 0.13351921035618475}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:33,073] Trial 27 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 242, 'max_depth': 6, 'learning_rate': 0.17494193462284097}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:33,296] Trial 28 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 170, 'max_depth': 4, 'learning_rate': 0.09445437731218223}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:33,630] Trial 29 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 230, 'max_depth': 5, 'learning_rate': 0.04521432406260992}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:33,823] Trial 30 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 195, 'max_depth': 3, 'learning_rate': 0.21971712756477946}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:34,006] Trial 31 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 130, 'max_depth': 10, 'learning_rate': 0.29461683038855413}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:34,193] Trial 32 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 115, 'max_depth': 9, 'learning_rate': 0.28044350523940914}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:34,416] Trial 33 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 141, 'max_depth': 9, 'learning_rate': 0.2688859012060773}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:34,665] Trial 34 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 166, 'max_depth': 8, 'learning_rate': 0.23858647326843851}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:35,012] Trial 35 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 145, 'max_depth': 7, 'learning_rate': 0.010199289289254981}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:35,258] Trial 36 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 117, 'max_depth': 6, 'learning_rate': 0.03915493731643367}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:35,704] Trial 37 finished with value: 0.691358024691358 and parameters: {'n_estimators': 221, 'max_depth': 10, 'learning_rate': 0.07018006264946619}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:36,025] Trial 38 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 279, 'max_depth': 9, 'learning_rate': 0.2748607332970529}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:36,212] Trial 39 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 107, 'max_depth': 5, 'learning_rate': 0.2040169260287007}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:36,454] Trial 40 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 163, 'max_depth': 7, 'learning_rate': 0.28703941210699746}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:36,685] Trial 41 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 189, 'max_depth': 4, 'learning_rate': 0.15107141136156987}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:36,913] Trial 42 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 181, 'max_depth': 4, 'learning_rate': 0.1696125375070745}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:37,163] Trial 43 finished with value: 0.691358024691358 and parameters: {'n_estimators': 204, 'max_depth': 4, 'learning_rate': 0.1349100524313362}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:37,579] Trial 44 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 131, 'max_depth': 5, 'learning_rate': 0.25731272530303156}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:37,712] Trial 45 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 102, 'max_depth': 3, 'learning_rate': 0.1411128306947134}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:37,921] Trial 46 finished with value: 0.691358024691358 and parameters: {'n_estimators': 144, 'max_depth': 4, 'learning_rate': 0.11764815385788564}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:38,167] Trial 47 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 258, 'max_depth': 3, 'learning_rate': 0.20137845637424984}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:38,418] Trial 48 finished with value: 0.691358024691358 and parameters: {'n_estimators': 156, 'max_depth': 6, 'learning_rate': 0.22984038162291054}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:38,793] Trial 49 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.10697832457115142}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:39,092] Trial 50 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.26081542029824134}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:39,432] Trial 51 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 230, 'max_depth': 5, 'learning_rate': 0.05757580814986689}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:39,749] Trial 52 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 242, 'max_depth': 4, 'learning_rate': 0.03193731480230511}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:40,087] Trial 53 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 212, 'max_depth': 5, 'learning_rate': 0.02199651350512523}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:40,355] Trial 54 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 211, 'max_depth': 4, 'learning_rate': 0.0794050522594443}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:40,596] Trial 55 finished with value: 0.7407407407407407 and parameters: {'n_estimators': 173, 'max_depth': 4, 'learning_rate': 0.02104897479046496}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:40,806] Trial 56 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 197, 'max_depth': 3, 'learning_rate': 0.023037682301133497}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:41,095] Trial 57 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 174, 'max_depth': 5, 'learning_rate': 0.016283024580738346}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:41,375] Trial 58 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.03242430666891319}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:41,590] Trial 59 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 204, 'max_depth': 3, 'learning_rate': 0.053772906907328014}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:41,831] Trial 60 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 121, 'max_depth': 6, 'learning_rate': 0.08008408496224889}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:42,070] Trial 61 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 187, 'max_depth': 4, 'learning_rate': 0.044965547340546155}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:42,406] Trial 62 finished with value: 0.7407407407407407 and parameters: {'n_estimators': 180, 'max_depth': 4, 'learning_rate': 0.019895773607795306}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:42,731] Trial 63 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 161, 'max_depth': 5, 'learning_rate': 0.025797881931479746}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:43,003] Trial 64 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 177, 'max_depth': 4, 'learning_rate': 0.011058727957451529}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:43,221] Trial 65 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 150, 'max_depth': 4, 'learning_rate': 0.0345345611159148}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:43,378] Trial 66 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 111, 'max_depth': 3, 'learning_rate': 0.05209452406035562}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:43,604] Trial 67 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 209, 'max_depth': 3, 'learning_rate': 0.02068763922994276}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:43,779] Trial 68 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 136, 'max_depth': 3, 'learning_rate': 0.050961949499288235}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:43,987] Trial 69 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 198, 'max_depth': 3, 'learning_rate': 0.06429435144769398}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:44,396] Trial 70 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 236, 'max_depth': 7, 'learning_rate': 0.04108177995970912}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:44,550] Trial 71 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 107, 'max_depth': 4, 'learning_rate': 0.028778572654336303}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:44,706] Trial 72 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 111, 'max_depth': 4, 'learning_rate': 0.027879423335607004}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:45,098] Trial 73 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 112, 'max_depth': 4, 'learning_rate': 0.02648807508721592}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:45,257] Trial 74 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 107, 'max_depth': 3, 'learning_rate': 0.01123045084132681}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:45,457] Trial 75 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 123, 'max_depth': 4, 'learning_rate': 0.036940173505334656}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:45,628] Trial 76 finished with value: 0.7407407407407407 and parameters: {'n_estimators': 101, 'max_depth': 4, 'learning_rate': 0.03742951137773558}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:45,866] Trial 77 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 116, 'max_depth': 5, 'learning_rate': 0.018924675632381874}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:46,063] Trial 78 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 104, 'max_depth': 5, 'learning_rate': 0.04711780828285797}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:46,239] Trial 79 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.029255746052521184}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:46,402] Trial 80 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 121, 'max_depth': 3, 'learning_rate': 0.07647035563961505}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:46,594] Trial 81 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 125, 'max_depth': 4, 'learning_rate': 0.03865513881465617}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:46,776] Trial 82 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 114, 'max_depth': 4, 'learning_rate': 0.03645724046508799}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:46,955] Trial 83 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 111, 'max_depth': 4, 'learning_rate': 0.060458227891701966}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:47,160] Trial 84 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 125, 'max_depth': 4, 'learning_rate': 0.019631775863456144}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:47,369] Trial 85 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 110, 'max_depth': 4, 'learning_rate': 0.028482202139854872}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:47,646] Trial 86 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 118, 'max_depth': 5, 'learning_rate': 0.05143202205483377}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:47,860] Trial 87 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 138, 'max_depth': 3, 'learning_rate': 0.04198036341888473}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:48,064] Trial 88 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 106, 'max_depth': 4, 'learning_rate': 0.015754215114837128}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:48,244] Trial 89 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 131, 'max_depth': 3, 'learning_rate': 0.02538785758283442}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:48,587] Trial 90 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 225, 'max_depth': 5, 'learning_rate': 0.06878982662772617}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:48,831] Trial 91 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 118, 'max_depth': 5, 'learning_rate': 0.017384706017894103}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:49,008] Trial 92 finished with value: 0.7407407407407407 and parameters: {'n_estimators': 101, 'max_depth': 4, 'learning_rate': 0.03481516663533167}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:49,182] Trial 93 finished with value: 0.7407407407407407 and parameters: {'n_estimators': 107, 'max_depth': 4, 'learning_rate': 0.03463797575485425}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:49,360] Trial 94 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 104, 'max_depth': 4, 'learning_rate': 0.05586871137275494}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:49,524] Trial 95 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.04604044061616613}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:49,690] Trial 96 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 109, 'max_depth': 4, 'learning_rate': 0.030938440826419545}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:49,858] Trial 97 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 113, 'max_depth': 4, 'learning_rate': 0.022422589026306075}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:50,071] Trial 98 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 217, 'max_depth': 3, 'learning_rate': 0.034722454070856335}. Best is trial 7 with value: 0.7407407407407407.
[I 2025-05-01 22:53:50,293] Trial 99 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 105, 'max_depth': 5, 'learning_rate': 0.013028197027850186}. Best is trial 7 with value: 0.7407407407407407.
Best params for XGBoost: {'n_estimators': 230, 'max_depth': 4, 'learning_rate': 0.015587859161726298}
Best model: XGBoost
01-05-2025 22:53:50 Retraining best model on original full dataset...
01-05-2025 22:53:52 ✅ Exported and logged final model to W&B as artifact: final_model_xgboost.pkl
[I 2025-05-01 23:06:30,190] A new study created in memory with name: no-name-9fe4f26d-ff37-4f9b-9e5c-23bc311f4bab
[I 2025-05-01 23:06:33,338] Trial 0 finished with value: 0.8003947368421052 and parameters: {'n_estimators': 157, 'max_depth': 3, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8003947368421052.
[I 2025-05-01 23:06:34,819] Trial 1 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 64, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.8508771929824561.
[I 2025-05-01 23:06:35,725] Trial 2 finished with value: 0.8424780701754386 and parameters: {'n_estimators': 60, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.8508771929824561.
[I 2025-05-01 23:06:36,446] Trial 3 finished with value: 0.8487280701754386 and parameters: {'n_estimators': 58, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.8508771929824561.
[I 2025-05-01 23:06:38,721] Trial 4 finished with value: 0.8487499999999999 and parameters: {'n_estimators': 125, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.8508771929824561.
[I 2025-05-01 23:06:41,231] Trial 5 finished with value: 0.8235526315789474 and parameters: {'n_estimators': 112, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.8508771929824561.
[I 2025-05-01 23:06:43,432] Trial 6 finished with value: 0.7878070175438596 and parameters: {'n_estimators': 113, 'max_depth': 2, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.8508771929824561.
[I 2025-05-01 23:06:47,094] Trial 7 finished with value: 0.8424122807017543 and parameters: {'n_estimators': 176, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.8508771929824561.
[I 2025-05-01 23:06:49,688] Trial 8 finished with value: 0.785701754385965 and parameters: {'n_estimators': 122, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.8508771929824561.
[I 2025-05-01 23:06:50,700] Trial 9 finished with value: 0.8172368421052632 and parameters: {'n_estimators': 88, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.8508771929824561.
[I 2025-05-01 23:06:51,952] Trial 10 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 82, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.8529385964912282.
[I 2025-05-01 23:06:53,289] Trial 11 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 84, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.8529385964912282.
[I 2025-05-01 23:06:56,949] Trial 12 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 100, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.8529385964912282.
[I 2025-05-01 23:06:58,089] Trial 13 finished with value: 0.8403947368421052 and parameters: {'n_estimators': 87, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.8529385964912282.
[I 2025-05-01 23:06:59,258] Trial 14 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 79, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.8529385964912282.
[I 2025-05-01 23:07:02,947] Trial 15 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 158, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 10 with value: 0.8529385964912282.
[I 2025-05-01 23:07:04,889] Trial 16 finished with value: 0.8424561403508772 and parameters: {'n_estimators': 145, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.8529385964912282.
[I 2025-05-01 23:07:06,033] Trial 17 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 76, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.8529385964912282.
[I 2025-05-01 23:07:10,537] Trial 18 finished with value: 0.8382456140350877 and parameters: {'n_estimators': 196, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 10 with value: 0.8529385964912282.
[I 2025-05-01 23:07:13,292] Trial 19 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 100, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 19 with value: 0.8550438596491228.
[I 2025-05-01 23:07:15,498] Trial 20 finished with value: 0.8319298245614034 and parameters: {'n_estimators': 101, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 19 with value: 0.8550438596491228.
[I 2025-05-01 23:07:16,624] Trial 21 finished with value: 0.8571271929824562 and parameters: {'n_estimators': 50, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:18,186] Trial 22 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 51, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:19,419] Trial 23 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 67, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:22,391] Trial 24 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 100, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:25,314] Trial 25 finished with value: 0.8424561403508772 and parameters: {'n_estimators': 73, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:26,018] Trial 26 finished with value: 0.8487499999999999 and parameters: {'n_estimators': 53, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:27,527] Trial 27 finished with value: 0.8571271929824562 and parameters: {'n_estimators': 133, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:29,374] Trial 28 finished with value: 0.8424780701754384 and parameters: {'n_estimators': 140, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:31,964] Trial 29 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 138, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:37,152] Trial 30 finished with value: 0.8466885964912281 and parameters: {'n_estimators': 156, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:38,891] Trial 31 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 93, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:41,218] Trial 32 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 129, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:44,030] Trial 33 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 111, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:45,591] Trial 34 finished with value: 0.8445614035087721 and parameters: {'n_estimators': 94, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:46,779] Trial 35 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 66, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:51,125] Trial 36 finished with value: 0.8445833333333332 and parameters: {'n_estimators': 117, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:56,014] Trial 37 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 130, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:07:57,303] Trial 38 finished with value: 0.8508333333333333 and parameters: {'n_estimators': 108, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8571271929824562.
[I 2025-05-01 23:08:00,105] Trial 39 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 157, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 39 with value: 0.8571710526315789.
[W 2025-05-01 23:08:00,635] Trial 40 failed with parameters: {'n_estimators': 165, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 3} because of the following error: KeyboardInterrupt().
Traceback (most recent call last):
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\optuna\study\_optimize.py", line 197, in _run_trial
    value_or_values = func(trial)
                      ^^^^^^^^^^^
  File "C:\Users\x-hp\AppData\Local\Temp\ipykernel_14124\3991562111.py", line 20, in objective_rf
    score = cross_val_score(model, X_train_scaled, y_train_res, cv=5, scoring='accuracy').mean()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\model_selection\_validation.py", line 712, in cross_val_score
    cv_results = cross_validate(
                 ^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\model_selection\_validation.py", line 423, in cross_validate
    results = parallel(
              ^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\parallel.py", line 74, in __call__
    return super().__call__(iterable_with_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\joblib\parallel.py", line 1918, in __call__
    return output if self.return_generator else list(output)
                                                ^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\joblib\parallel.py", line 1847, in _get_sequential_output
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\parallel.py", line 136, in __call__
    return self.function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\model_selection\_validation.py", line 888, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\base.py", line 1473, in wrapper
    return fit_method(estimator, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\ensemble\_forest.py", line 489, in fit
    trees = Parallel(
            ^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\parallel.py", line 74, in __call__
    return super().__call__(iterable_with_config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\joblib\parallel.py", line 1918, in __call__
    return output if self.return_generator else list(output)
                                                ^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\joblib\parallel.py", line 1847, in _get_sequential_output
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\parallel.py", line 136, in __call__
    return self.function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\ensemble\_forest.py", line 192, in _parallel_build_trees
    tree._fit(
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\tree\_classes.py", line 294, in _fit
    check_classification_targets(y)
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\multiclass.py", line 211, in check_classification_targets
    y_type = type_of_target(y, input_name="y")
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\multiclass.py", line 397, in type_of_target
    if xp.any(data != xp.astype(data, int)):
                      ^^^^^^^^^^^^^^^^^^^^
  File "c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\sklearn\utils\_array_api.py", line 390, in astype
    return x.astype(dtype, copy=copy, casting=casting)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[W 2025-05-01 23:08:00,782] Trial 40 failed with value None.
Requirement already satisfied: wandb in c:\users\x-hp\anaconda3\lib\site-packages (0.19.8)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.1.43)
Requirement already satisfied: platformdirs in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (4.25.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (5.9.0)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.8.2)
Requirement already satisfied: pyyaml in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (6.0.1)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.18.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (75.1.0)
Requirement already satisfied: colorama in c:\users\x-hp\anaconda3\lib\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)
Requirement already satisfied: typing-extensions>=4.6.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)
Requirement already satisfied: smmap<5,>=3.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.login() after wandb.init() has no effect.
01-05-2025 23:08:11 Downloading and reading train artifact
[34m[1mwandb[0m:   1 of 1 files downloaded.
01-05-2025 23:08:13 Spliting data into train/val
01-05-2025 23:08:13 x train: (375, 28)
01-05-2025 23:08:13 y train: (375,)
01-05-2025 23:08:13 x val: (162, 28)
01-05-2025 23:08:13 y val: (162,)
01-05-2025 23:08:13 Outlier Removal
01-05-2025 23:08:14 x_train shape [original]: (375, 28)
01-05-2025 23:08:14 x_train shape [outlier removal]: (357, 28)
01-05-2025 23:08:14 Data after SMOTE: (476, 28)
01-05-2025 23:08:14 [Logistic] Accuracy: 0.7469135802469136
01-05-2025 23:08:14 [Random Forest] Accuracy: 0.7222222222222222
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:08:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
01-05-2025 23:08:15 [XGBoost] Accuracy: 0.7222222222222222
                 Model  Accuracy
0  Logistic Regression  0.746914
1        Random Forest  0.722222
2              XGBoost  0.722222
[I 2025-05-01 23:08:17,187] A new study created in memory with name: no-name-fc9c59fb-7ffd-4292-b294-b7b3626fb2aa
[I 2025-05-01 23:08:19,224] Trial 0 finished with value: 0.8571491228070174 and parameters: {'n_estimators': 117, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:21,424] Trial 1 finished with value: 0.8382456140350877 and parameters: {'n_estimators': 185, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:22,288] Trial 2 finished with value: 0.8445614035087718 and parameters: {'n_estimators': 70, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:24,455] Trial 3 finished with value: 0.8424780701754386 and parameters: {'n_estimators': 196, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:26,219] Trial 4 finished with value: 0.8445394736842105 and parameters: {'n_estimators': 150, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:27,015] Trial 5 finished with value: 0.8298464912280702 and parameters: {'n_estimators': 58, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:28,591] Trial 6 finished with value: 0.7961842105263158 and parameters: {'n_estimators': 154, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:29,507] Trial 7 finished with value: 0.8403728070175438 and parameters: {'n_estimators': 53, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:31,188] Trial 8 finished with value: 0.8382456140350877 and parameters: {'n_estimators': 112, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:33,262] Trial 9 finished with value: 0.7794298245614034 and parameters: {'n_estimators': 178, 'max_depth': 2, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:34,300] Trial 10 finished with value: 0.8508333333333333 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:35,445] Trial 11 finished with value: 0.8508333333333333 and parameters: {'n_estimators': 101, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:36,594] Trial 12 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 87, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:37,563] Trial 13 finished with value: 0.8508991228070176 and parameters: {'n_estimators': 80, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:38,972] Trial 14 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 134, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:39,802] Trial 15 finished with value: 0.8319078947368421 and parameters: {'n_estimators': 81, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:41,179] Trial 16 finished with value: 0.8298026315789473 and parameters: {'n_estimators': 125, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:42,977] Trial 17 finished with value: 0.8382675438596492 and parameters: {'n_estimators': 124, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:43,888] Trial 18 finished with value: 0.8487280701754386 and parameters: {'n_estimators': 86, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:45,301] Trial 19 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 104, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:47,010] Trial 20 finished with value: 0.8172149122807018 and parameters: {'n_estimators': 145, 'max_depth': 4, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:47,976] Trial 21 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 84, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:48,943] Trial 22 finished with value: 0.8466447368421053 and parameters: {'n_estimators': 91, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:49,688] Trial 23 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 69, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.8571491228070174.
[I 2025-05-01 23:08:50,468] Trial 24 finished with value: 0.8655482456140351 and parameters: {'n_estimators': 69, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:08:51,209] Trial 25 finished with value: 0.8424780701754386 and parameters: {'n_estimators': 65, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:08:52,711] Trial 26 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 116, 'max_depth': 8, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:08:54,909] Trial 27 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 163, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:08:56,911] Trial 28 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 164, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:08:58,866] Trial 29 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 167, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:09:00,303] Trial 30 finished with value: 0.8424780701754386 and parameters: {'n_estimators': 137, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:09:02,559] Trial 31 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 158, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:09:04,356] Trial 32 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 167, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:09:06,337] Trial 33 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 185, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:09:08,427] Trial 34 finished with value: 0.8445833333333332 and parameters: {'n_estimators': 173, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:09:10,473] Trial 35 finished with value: 0.8487499999999999 and parameters: {'n_estimators': 192, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:09:12,850] Trial 36 finished with value: 0.8445614035087721 and parameters: {'n_estimators': 200, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 24 with value: 0.8655482456140351.
[I 2025-05-01 23:09:14,434] Trial 37 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 144, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:16,146] Trial 38 finished with value: 0.8550877192982457 and parameters: {'n_estimators': 141, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:17,793] Trial 39 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 129, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:19,098] Trial 40 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 118, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:21,237] Trial 41 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 161, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:24,135] Trial 42 finished with value: 0.8529605263157893 and parameters: {'n_estimators': 150, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:26,895] Trial 43 finished with value: 0.8571491228070174 and parameters: {'n_estimators': 147, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:29,436] Trial 44 finished with value: 0.8676535087719298 and parameters: {'n_estimators': 177, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:31,611] Trial 45 finished with value: 0.8550877192982457 and parameters: {'n_estimators': 176, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:33,827] Trial 46 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 185, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:35,472] Trial 47 finished with value: 0.8529605263157893 and parameters: {'n_estimators': 155, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:36,040] Trial 48 finished with value: 0.7815350877192981 and parameters: {'n_estimators': 56, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:37,627] Trial 49 finished with value: 0.8466447368421053 and parameters: {'n_estimators': 111, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:40,211] Trial 50 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 192, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:42,059] Trial 51 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 161, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:44,459] Trial 52 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 169, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:46,467] Trial 53 finished with value: 0.8487499999999999 and parameters: {'n_estimators': 180, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:48,525] Trial 54 finished with value: 0.8445394736842105 and parameters: {'n_estimators': 164, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 4}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:49,141] Trial 55 finished with value: 0.8487499999999999 and parameters: {'n_estimators': 50, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:49,966] Trial 56 finished with value: 0.8613377192982457 and parameters: {'n_estimators': 76, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:50,750] Trial 57 finished with value: 0.819298245614035 and parameters: {'n_estimators': 78, 'max_depth': 4, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:51,458] Trial 58 finished with value: 0.8529605263157893 and parameters: {'n_estimators': 63, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:52,353] Trial 59 finished with value: 0.8277192982456139 and parameters: {'n_estimators': 74, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:53,493] Trial 60 finished with value: 0.8466447368421053 and parameters: {'n_estimators': 94, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:55,232] Trial 61 finished with value: 0.8613815789473683 and parameters: {'n_estimators': 151, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 37 with value: 0.8697587719298246.
[I 2025-05-01 23:09:56,948] Trial 62 finished with value: 0.8718640350877193 and parameters: {'n_estimators': 152, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:09:58,915] Trial 63 finished with value: 0.8655701754385964 and parameters: {'n_estimators': 133, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:00,377] Trial 64 finished with value: 0.8571491228070176 and parameters: {'n_estimators': 134, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:02,614] Trial 65 finished with value: 0.8718421052631579 and parameters: {'n_estimators': 142, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:04,423] Trial 66 finished with value: 0.8571271929824562 and parameters: {'n_estimators': 142, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:05,847] Trial 67 finished with value: 0.855109649122807 and parameters: {'n_estimators': 129, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:07,249] Trial 68 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 123, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:09,013] Trial 69 finished with value: 0.8571491228070176 and parameters: {'n_estimators': 136, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:10,185] Trial 70 finished with value: 0.8361622807017544 and parameters: {'n_estimators': 117, 'max_depth': 5, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:11,905] Trial 71 finished with value: 0.8613815789473683 and parameters: {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:13,945] Trial 72 finished with value: 0.8466228070175438 and parameters: {'n_estimators': 154, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 4}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:15,532] Trial 73 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 145, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:17,207] Trial 74 finished with value: 0.8676315789473683 and parameters: {'n_estimators': 141, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:18,978] Trial 75 finished with value: 0.8676315789473683 and parameters: {'n_estimators': 141, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:20,509] Trial 76 finished with value: 0.8718421052631579 and parameters: {'n_estimators': 142, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:22,436] Trial 77 finished with value: 0.8676535087719298 and parameters: {'n_estimators': 143, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:24,644] Trial 78 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 144, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:26,208] Trial 79 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 145, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:28,058] Trial 80 finished with value: 0.8529385964912279 and parameters: {'n_estimators': 128, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:30,108] Trial 81 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 145, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:31,874] Trial 82 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 147, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:33,867] Trial 83 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 148, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:35,593] Trial 84 finished with value: 0.8592324561403508 and parameters: {'n_estimators': 156, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:37,259] Trial 85 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 145, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:39,058] Trial 86 finished with value: 0.8550657894736842 and parameters: {'n_estimators': 139, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:40,784] Trial 87 finished with value: 0.8718640350877193 and parameters: {'n_estimators': 158, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:42,433] Trial 88 finished with value: 0.8424561403508772 and parameters: {'n_estimators': 152, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:44,149] Trial 89 finished with value: 0.8004385964912281 and parameters: {'n_estimators': 160, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:45,886] Trial 90 finished with value: 0.8634429824561403 and parameters: {'n_estimators': 157, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:47,593] Trial 91 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 146, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:49,302] Trial 92 finished with value: 0.8676535087719298 and parameters: {'n_estimators': 136, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:51,156] Trial 93 finished with value: 0.8655701754385964 and parameters: {'n_estimators': 132, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:52,873] Trial 94 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 147, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:54,978] Trial 95 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 153, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:56,512] Trial 96 finished with value: 0.8676535087719298 and parameters: {'n_estimators': 139, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:10:58,538] Trial 97 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 169, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:11:00,397] Trial 98 finished with value: 0.8550657894736842 and parameters: {'n_estimators': 149, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
[I 2025-05-01 23:11:02,213] Trial 99 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 159, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 62 with value: 0.8718640350877193.
✅ Best Random Forest Parameters: {'n_estimators': 152, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}
[I 2025-05-01 23:11:02,219] A new study created in memory with name: no-name-d23fe620-131c-4d75-8361-b44d922a80d7
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:02,942] Trial 0 finished with value: 0.8025438596491228 and parameters: {'n_estimators': 284, 'max_depth': 3, 'learning_rate': 0.28943730042543786, 'subsample': 0.9089660237075582, 'colsample_bytree': 0.7670008142722481, 'gamma': 4.4125793403267, 'min_child_weight': 10}. Best is trial 0 with value: 0.8025438596491228.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:03,929] Trial 1 finished with value: 0.8152192982456141 and parameters: {'n_estimators': 116, 'max_depth': 8, 'learning_rate': 0.24541971750529343, 'subsample': 0.6639979565132473, 'colsample_bytree': 0.6048099008416857, 'gamma': 0.0759608571122633, 'min_child_weight': 8}. Best is trial 1 with value: 0.8152192982456141.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:04,864] Trial 2 finished with value: 0.8046491228070174 and parameters: {'n_estimators': 259, 'max_depth': 5, 'learning_rate': 0.2988122765392874, 'subsample': 0.7199725276139581, 'colsample_bytree': 0.818873853448878, 'gamma': 0.9585285909532315, 'min_child_weight': 7}. Best is trial 1 with value: 0.8152192982456141.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:05,454] Trial 3 finished with value: 0.8152192982456141 and parameters: {'n_estimators': 218, 'max_depth': 7, 'learning_rate': 0.16661181075681114, 'subsample': 0.9411145946362603, 'colsample_bytree': 0.7887217874121961, 'gamma': 3.91225140085631, 'min_child_weight': 8}. Best is trial 1 with value: 0.8152192982456141.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:06,166] Trial 4 finished with value: 0.8193421052631578 and parameters: {'n_estimators': 192, 'max_depth': 5, 'learning_rate': 0.07021837703355142, 'subsample': 0.788963815553998, 'colsample_bytree': 0.813154583437679, 'gamma': 0.30416578323183674, 'min_child_weight': 10}. Best is trial 4 with value: 0.8193421052631578.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:07,009] Trial 5 finished with value: 0.8277631578947368 and parameters: {'n_estimators': 229, 'max_depth': 6, 'learning_rate': 0.048239818652540775, 'subsample': 0.7149329367589767, 'colsample_bytree': 0.7040402005995612, 'gamma': 1.5017088012809454, 'min_child_weight': 7}. Best is trial 5 with value: 0.8277631578947368.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:08,023] Trial 6 finished with value: 0.8466447368421053 and parameters: {'n_estimators': 291, 'max_depth': 6, 'learning_rate': 0.09115531460904867, 'subsample': 0.6055983355876109, 'colsample_bytree': 0.6278589809597688, 'gamma': 1.224147041716484, 'min_child_weight': 1}. Best is trial 6 with value: 0.8466447368421053.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:08,861] Trial 7 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 141, 'max_depth': 4, 'learning_rate': 0.06291987984658604, 'subsample': 0.7154637076196393, 'colsample_bytree': 0.9041847494922173, 'gamma': 1.0921083506209495, 'min_child_weight': 2}. Best is trial 7 with value: 0.8508552631578947.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:09,341] Trial 8 finished with value: 0.8298903508771929 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.2211422711396267, 'subsample': 0.9550353223411995, 'colsample_bytree': 0.6462249413253957, 'gamma': 2.375562840848797, 'min_child_weight': 8}. Best is trial 7 with value: 0.8508552631578947.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:10,299] Trial 9 finished with value: 0.8529605263157896 and parameters: {'n_estimators': 182, 'max_depth': 10, 'learning_rate': 0.2176589483626731, 'subsample': 0.6198360074506041, 'colsample_bytree': 0.9187141634038894, 'gamma': 2.3495781384060144, 'min_child_weight': 2}. Best is trial 9 with value: 0.8529605263157896.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:10,868] Trial 10 finished with value: 0.8361184210526316 and parameters: {'n_estimators': 179, 'max_depth': 10, 'learning_rate': 0.1517939534227674, 'subsample': 0.8369111061456063, 'colsample_bytree': 0.9929710664155311, 'gamma': 3.233698406649536, 'min_child_weight': 4}. Best is trial 9 with value: 0.8529605263157896.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:11,453] Trial 11 finished with value: 0.8466447368421054 and parameters: {'n_estimators': 154, 'max_depth': 3, 'learning_rate': 0.13549356057922796, 'subsample': 0.606600184083086, 'colsample_bytree': 0.9356425782448496, 'gamma': 2.3339458729839966, 'min_child_weight': 2}. Best is trial 9 with value: 0.8529605263157896.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:12,250] Trial 12 finished with value: 0.8067324561403509 and parameters: {'n_estimators': 105, 'max_depth': 10, 'learning_rate': 0.013281779373244984, 'subsample': 0.6922339324174919, 'colsample_bytree': 0.9012020184804277, 'gamma': 1.8635324979830759, 'min_child_weight': 3}. Best is trial 9 with value: 0.8529605263157896.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:12,811] Trial 13 finished with value: 0.8382675438596492 and parameters: {'n_estimators': 164, 'max_depth': 4, 'learning_rate': 0.2028298604124815, 'subsample': 0.7659560600551107, 'colsample_bytree': 0.8880796146733636, 'gamma': 3.1833951131262284, 'min_child_weight': 4}. Best is trial 9 with value: 0.8529605263157896.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:13,464] Trial 14 finished with value: 0.8362280701754387 and parameters: {'n_estimators': 142, 'max_depth': 9, 'learning_rate': 0.10605096439317649, 'subsample': 0.6517893244057295, 'colsample_bytree': 0.9790708702098982, 'gamma': 3.2040075347793104, 'min_child_weight': 2}. Best is trial 9 with value: 0.8529605263157896.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:14,357] Trial 15 finished with value: 0.8571052631578947 and parameters: {'n_estimators': 206, 'max_depth': 4, 'learning_rate': 0.18908611458393704, 'subsample': 0.8616194985015413, 'colsample_bytree': 0.8676512623896602, 'gamma': 0.6754969179769146, 'min_child_weight': 1}. Best is trial 15 with value: 0.8571052631578947.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:15,306] Trial 16 finished with value: 0.8718201754385966 and parameters: {'n_estimators': 215, 'max_depth': 7, 'learning_rate': 0.19720673960342375, 'subsample': 0.8755139298709153, 'colsample_bytree': 0.8490656273597046, 'gamma': 0.6379600765351944, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:16,139] Trial 17 finished with value: 0.8340789473684211 and parameters: {'n_estimators': 235, 'max_depth': 7, 'learning_rate': 0.18852227839359992, 'subsample': 0.8540056579980846, 'colsample_bytree': 0.849173012626841, 'gamma': 0.6237304563847634, 'min_child_weight': 5}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:16,797] Trial 18 finished with value: 0.8529166666666667 and parameters: {'n_estimators': 215, 'max_depth': 5, 'learning_rate': 0.2560828165003637, 'subsample': 0.8844146959767392, 'colsample_bytree': 0.7391005947962219, 'gamma': 1.7108539513403045, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:18,038] Trial 19 finished with value: 0.8341008771929825 and parameters: {'n_estimators': 256, 'max_depth': 8, 'learning_rate': 0.17796328937011313, 'subsample': 0.8294389380878697, 'colsample_bytree': 0.8553926048886963, 'gamma': 0.473395003244387, 'min_child_weight': 4}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:19,274] Trial 20 finished with value: 0.8676754385964912 and parameters: {'n_estimators': 205, 'max_depth': 4, 'learning_rate': 0.1342524303925247, 'subsample': 0.9885879267870561, 'colsample_bytree': 0.8535313151522046, 'gamma': 0.04846838617220994, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:20,548] Trial 21 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 209, 'max_depth': 4, 'learning_rate': 0.12646844336685525, 'subsample': 0.977780786963638, 'colsample_bytree': 0.8534919658869918, 'gamma': 0.08413802754006337, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:21,731] Trial 22 finished with value: 0.8593201754385964 and parameters: {'n_estimators': 242, 'max_depth': 6, 'learning_rate': 0.12579557203440972, 'subsample': 0.9913953812371176, 'colsample_bytree': 0.825943037114221, 'gamma': 0.028004927174353718, 'min_child_weight': 3}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:22,533] Trial 23 finished with value: 0.8655263157894737 and parameters: {'n_estimators': 198, 'max_depth': 3, 'learning_rate': 0.14701063891651955, 'subsample': 0.9885539517888737, 'colsample_bytree': 0.950499330967788, 'gamma': 0.7323723306280192, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:23,318] Trial 24 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 193, 'max_depth': 3, 'learning_rate': 0.15142799538740326, 'subsample': 0.9180412626649858, 'colsample_bytree': 0.9451446197045524, 'gamma': 0.7398206920899968, 'min_child_weight': 3}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:24,884] Trial 25 finished with value: 0.8529605263157896 and parameters: {'n_estimators': 169, 'max_depth': 3, 'learning_rate': 0.10795240218750046, 'subsample': 0.9671303179858619, 'colsample_bytree': 0.9629735346682855, 'gamma': 1.4584226688809787, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:25,734] Trial 26 finished with value: 0.8320175438596491 and parameters: {'n_estimators': 256, 'max_depth': 7, 'learning_rate': 0.1606241747556545, 'subsample': 0.9245694591420064, 'colsample_bytree': 0.7470892318478505, 'gamma': 0.9643179145083766, 'min_child_weight': 5}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:26,505] Trial 27 finished with value: 0.8633991228070176 and parameters: {'n_estimators': 226, 'max_depth': 5, 'learning_rate': 0.24115807895603925, 'subsample': 0.8904093211828833, 'colsample_bytree': 0.8875910460901659, 'gamma': 0.4682359339341689, 'min_child_weight': 2}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:27,188] Trial 28 finished with value: 0.8298464912280702 and parameters: {'n_estimators': 197, 'max_depth': 4, 'learning_rate': 0.1432264712878263, 'subsample': 0.9994519140508548, 'colsample_bytree': 0.9384088362405845, 'gamma': 1.9250511070113516, 'min_child_weight': 3}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:27,976] Trial 29 finished with value: 0.8320175438596491 and parameters: {'n_estimators': 271, 'max_depth': 3, 'learning_rate': 0.20712693939113833, 'subsample': 0.9397384338133961, 'colsample_bytree': 0.7778544129251234, 'gamma': 4.509067569136257, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:29,309] Trial 30 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 185, 'max_depth': 9, 'learning_rate': 0.09033903308004636, 'subsample': 0.9033640304338316, 'colsample_bytree': 0.7054262184646939, 'gamma': 1.3736561359010229, 'min_child_weight': 2}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:30,468] Trial 31 finished with value: 0.8634429824561403 and parameters: {'n_estimators': 205, 'max_depth': 4, 'learning_rate': 0.12274074182914972, 'subsample': 0.9741720127937455, 'colsample_bytree': 0.8399956422688901, 'gamma': 0.21494762807698414, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:31,883] Trial 32 finished with value: 0.8634429824561403 and parameters: {'n_estimators': 217, 'max_depth': 3, 'learning_rate': 0.11568562283207622, 'subsample': 0.9779940482491332, 'colsample_bytree': 0.8683120224006577, 'gamma': 0.0007061209382342309, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:32,855] Trial 33 finished with value: 0.861359649122807 and parameters: {'n_estimators': 242, 'max_depth': 5, 'learning_rate': 0.17253950339400786, 'subsample': 0.9543582437314568, 'colsample_bytree': 0.8050456816091608, 'gamma': 0.8501364205859412, 'min_child_weight': 2}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:33,914] Trial 34 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 207, 'max_depth': 4, 'learning_rate': 0.27266257890304857, 'subsample': 0.9977752303054722, 'colsample_bytree': 0.7576739093209456, 'gamma': 0.2780488115742387, 'min_child_weight': 3}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:35,053] Trial 35 finished with value: 0.8676535087719298 and parameters: {'n_estimators': 168, 'max_depth': 6, 'learning_rate': 0.09345826091596521, 'subsample': 0.9328433649395952, 'colsample_bytree': 0.835649290324525, 'gamma': 0.3845460830830239, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:35,976] Trial 36 finished with value: 0.8382675438596492 and parameters: {'n_estimators': 167, 'max_depth': 8, 'learning_rate': 0.08635896321319869, 'subsample': 0.9264849095055574, 'colsample_bytree': 0.7922615966885469, 'gamma': 0.4526035193540131, 'min_child_weight': 6}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:36,526] Trial 37 finished with value: 0.8088815789473685 and parameters: {'n_estimators': 154, 'max_depth': 6, 'learning_rate': 0.06884138387145601, 'subsample': 0.8789392104069947, 'colsample_bytree': 0.8345905287681727, 'gamma': 4.073771384638783, 'min_child_weight': 10}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:37,232] Trial 38 finished with value: 0.8193640350877193 and parameters: {'n_estimators': 125, 'max_depth': 7, 'learning_rate': 0.03817486685084733, 'subsample': 0.8031427430151453, 'colsample_bytree': 0.815982841909595, 'gamma': 1.2222099663389439, 'min_child_weight': 9}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:37,911] Trial 39 finished with value: 0.863486842105263 and parameters: {'n_estimators': 176, 'max_depth': 5, 'learning_rate': 0.23300865027687045, 'subsample': 0.9446267091332539, 'colsample_bytree': 0.8803757409249915, 'gamma': 0.968363140782527, 'min_child_weight': 2}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:38,710] Trial 40 finished with value: 0.8613596491228069 and parameters: {'n_estimators': 190, 'max_depth': 6, 'learning_rate': 0.13795435305020468, 'subsample': 0.90156929751194, 'colsample_bytree': 0.9164418963858609, 'gamma': 2.8233568685337413, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:39,757] Trial 41 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 177, 'max_depth': 5, 'learning_rate': 0.2419598593236312, 'subsample': 0.9536704054702982, 'colsample_bytree': 0.8746994960988417, 'gamma': 0.9768259119152516, 'min_child_weight': 2}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:40,898] Trial 42 finished with value: 0.8571491228070174 and parameters: {'n_estimators': 224, 'max_depth': 6, 'learning_rate': 0.2169837500343987, 'subsample': 0.9389040082700055, 'colsample_bytree': 0.9643319764735467, 'gamma': 0.44442343827335357, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:41,631] Trial 43 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 199, 'max_depth': 5, 'learning_rate': 0.18798844240964868, 'subsample': 0.9602345217340326, 'colsample_bytree': 0.8874342249626304, 'gamma': 0.7968538041485784, 'min_child_weight': 2}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:42,398] Trial 44 finished with value: 0.861359649122807 and parameters: {'n_estimators': 159, 'max_depth': 7, 'learning_rate': 0.22830667899676316, 'subsample': 0.936332620863141, 'colsample_bytree': 0.9196779393309682, 'gamma': 0.27038105854547445, 'min_child_weight': 2}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:43,087] Trial 45 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 178, 'max_depth': 6, 'learning_rate': 0.28556240049233983, 'subsample': 0.9141130777311146, 'colsample_bytree': 0.820394830596668, 'gamma': 1.1380996997956694, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:43,945] Trial 46 finished with value: 0.8193421052631578 and parameters: {'n_estimators': 173, 'max_depth': 5, 'learning_rate': 0.2599861978250823, 'subsample': 0.7510146989981269, 'colsample_bytree': 0.8004611729634205, 'gamma': 0.6240127477214513, 'min_child_weight': 7}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:44,958] Trial 47 finished with value: 0.8445394736842105 and parameters: {'n_estimators': 146, 'max_depth': 3, 'learning_rate': 0.09956205043745821, 'subsample': 0.8663168873182193, 'colsample_bytree': 0.905736499349311, 'gamma': 2.031566158567239, 'min_child_weight': 2}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:45,558] Trial 48 finished with value: 0.8488157894736842 and parameters: {'n_estimators': 125, 'max_depth': 7, 'learning_rate': 0.16338183692018085, 'subsample': 0.9769986574715908, 'colsample_bytree': 0.8730023946028975, 'gamma': 1.3124166686001746, 'min_child_weight': 3}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:46,237] Trial 49 finished with value: 0.8214692982456141 and parameters: {'n_estimators': 190, 'max_depth': 4, 'learning_rate': 0.20497453880422967, 'subsample': 0.9502490658452324, 'colsample_bytree': 0.994435092729495, 'gamma': 4.877499521991158, 'min_child_weight': 1}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:47,237] Trial 50 finished with value: 0.8361842105263158 and parameters: {'n_estimators': 236, 'max_depth': 5, 'learning_rate': 0.05754551838057853, 'subsample': 0.8067149162973095, 'colsample_bytree': 0.8457528370978186, 'gamma': 1.0357207949174398, 'min_child_weight': 4}. Best is trial 16 with value: 0.8718201754385966.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:48,297] Trial 51 finished with value: 0.8718640350877193 and parameters: {'n_estimators': 214, 'max_depth': 4, 'learning_rate': 0.12762117226565123, 'subsample': 0.9901445308345911, 'colsample_bytree': 0.8590241050271552, 'gamma': 0.12492456819743009, 'min_child_weight': 1}. Best is trial 51 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:49,286] Trial 52 finished with value: 0.8676754385964912 and parameters: {'n_estimators': 214, 'max_depth': 4, 'learning_rate': 0.1492122599863461, 'subsample': 0.9840464973877714, 'colsample_bytree': 0.7758009408097791, 'gamma': 0.28580743661252367, 'min_child_weight': 1}. Best is trial 51 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:50,383] Trial 53 finished with value: 0.8739254385964912 and parameters: {'n_estimators': 214, 'max_depth': 4, 'learning_rate': 0.14393641787490294, 'subsample': 0.988351978362984, 'colsample_bytree': 0.776855122846359, 'gamma': 0.2049959736625962, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:51,911] Trial 54 finished with value: 0.8676315789473685 and parameters: {'n_estimators': 215, 'max_depth': 4, 'learning_rate': 0.08013768017131415, 'subsample': 0.9680221716903985, 'colsample_bytree': 0.7204566890722252, 'gamma': 0.17091036149649355, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:52,832] Trial 55 finished with value: 0.8530043859649122 and parameters: {'n_estimators': 222, 'max_depth': 8, 'learning_rate': 0.11444240233264417, 'subsample': 0.9842632007977276, 'colsample_bytree': 0.7808550457067277, 'gamma': 0.4450089863862212, 'min_child_weight': 2}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:53,867] Trial 56 finished with value: 0.8697368421052631 and parameters: {'n_estimators': 234, 'max_depth': 4, 'learning_rate': 0.13215345431490721, 'subsample': 0.9982921004374163, 'colsample_bytree': 0.6673051060915649, 'gamma': 0.2592201144746919, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:55,257] Trial 57 finished with value: 0.8613815789473683 and parameters: {'n_estimators': 248, 'max_depth': 4, 'learning_rate': 0.1329782312678381, 'subsample': 0.9848157635246146, 'colsample_bytree': 0.6567181362118738, 'gamma': 0.008626303417269068, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:56,117] Trial 58 finished with value: 0.8655701754385966 and parameters: {'n_estimators': 232, 'max_depth': 4, 'learning_rate': 0.15623221452467245, 'subsample': 0.9978339772297012, 'colsample_bytree': 0.6063039709064533, 'gamma': 0.589819573844959, 'min_child_weight': 2}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:57,151] Trial 59 finished with value: 0.8550219298245614 and parameters: {'n_estimators': 269, 'max_depth': 3, 'learning_rate': 0.17799342137343133, 'subsample': 0.9649078562881569, 'colsample_bytree': 0.6834774059295601, 'gamma': 0.22117269354285649, 'min_child_weight': 3}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:58,437] Trial 60 finished with value: 0.8466885964912281 and parameters: {'n_estimators': 209, 'max_depth': 4, 'learning_rate': 0.1683952497506605, 'subsample': 0.8404970448295329, 'colsample_bytree': 0.7624242512802943, 'gamma': 0.14258124548999068, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:11:59,495] Trial 61 finished with value: 0.865548245614035 and parameters: {'n_estimators': 220, 'max_depth': 4, 'learning_rate': 0.09842775773799424, 'subsample': 0.9299241071556553, 'colsample_bytree': 0.8352135340556445, 'gamma': 0.3186386309242685, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:11:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:00,753] Trial 62 finished with value: 0.8634429824561403 and parameters: {'n_estimators': 229, 'max_depth': 5, 'learning_rate': 0.13191396161277338, 'subsample': 0.9656878641202628, 'colsample_bytree': 0.8562533406972849, 'gamma': 0.3598663002432792, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:01,564] Trial 63 finished with value: 0.8634429824561403 and parameters: {'n_estimators': 213, 'max_depth': 4, 'learning_rate': 0.14696258494646336, 'subsample': 0.9882926939355665, 'colsample_bytree': 0.6721454831428105, 'gamma': 0.6003708431061608, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:02,321] Trial 64 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 204, 'max_depth': 9, 'learning_rate': 0.11618128226379483, 'subsample': 0.9978240518348838, 'colsample_bytree': 0.7360793881065637, 'gamma': 1.6204945058592104, 'min_child_weight': 2}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:03,242] Trial 65 finished with value: 0.8676973684210527 and parameters: {'n_estimators': 245, 'max_depth': 3, 'learning_rate': 0.1397640607811986, 'subsample': 0.978469200128696, 'colsample_bytree': 0.808993364608527, 'gamma': 0.5433900492840682, 'min_child_weight': 2}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:04,070] Trial 66 finished with value: 0.8571491228070176 and parameters: {'n_estimators': 243, 'max_depth': 3, 'learning_rate': 0.1393068168570856, 'subsample': 0.9756357225244449, 'colsample_bytree': 0.7772749173747622, 'gamma': 0.826639134746024, 'min_child_weight': 2}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:05,185] Trial 67 finished with value: 0.8634868421052632 and parameters: {'n_estimators': 236, 'max_depth': 3, 'learning_rate': 0.1540810896226091, 'subsample': 0.9559535176635239, 'colsample_bytree': 0.7929455670874453, 'gamma': 0.14876454278449566, 'min_child_weight': 2}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:06,549] Trial 68 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 254, 'max_depth': 3, 'learning_rate': 0.12465019109684347, 'subsample': 0.6968472197132637, 'colsample_bytree': 0.6274816308757833, 'gamma': 0.5258142196040441, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:07,903] Trial 69 finished with value: 0.8550877192982457 and parameters: {'n_estimators': 264, 'max_depth': 4, 'learning_rate': 0.1820112584116332, 'subsample': 0.9849158949958582, 'colsample_bytree': 0.8063265381400844, 'gamma': 0.002350923292664564, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:09,270] Trial 70 finished with value: 0.8341008771929825 and parameters: {'n_estimators': 284, 'max_depth': 3, 'learning_rate': 0.19651432718949646, 'subsample': 0.773865482801423, 'colsample_bytree': 0.8604715531568371, 'gamma': 0.7073883231256186, 'min_child_weight': 6}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:10,572] Trial 71 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 225, 'max_depth': 4, 'learning_rate': 0.10855585256658247, 'subsample': 0.97341221293296, 'colsample_bytree': 0.8278424419958205, 'gamma': 0.36125363887477213, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:11,372] Trial 72 finished with value: 0.846688596491228 and parameters: {'n_estimators': 211, 'max_depth': 7, 'learning_rate': 0.14573735054499373, 'subsample': 0.9458326461417481, 'colsample_bytree': 0.8422472595348663, 'gamma': 3.5526202396336126, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:12,366] Trial 73 finished with value: 0.8571271929824562 and parameters: {'n_estimators': 186, 'max_depth': 4, 'learning_rate': 0.16209618987860094, 'subsample': 0.896708308726941, 'colsample_bytree': 0.8205033695505013, 'gamma': 0.3665988917465416, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:13,357] Trial 74 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 201, 'max_depth': 5, 'learning_rate': 0.12969913225593585, 'subsample': 0.9163851987371038, 'colsample_bytree': 0.7461165745280977, 'gamma': 0.1406519980183032, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:14,330] Trial 75 finished with value: 0.8634868421052632 and parameters: {'n_estimators': 196, 'max_depth': 3, 'learning_rate': 0.13948257533306122, 'subsample': 0.989910773521025, 'colsample_bytree': 0.8106462763499671, 'gamma': 0.8631804749668421, 'min_child_weight': 2}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:15,255] Trial 76 finished with value: 0.8194078947368422 and parameters: {'n_estimators': 247, 'max_depth': 4, 'learning_rate': 0.11785377496731438, 'subsample': 0.96093376990332, 'colsample_bytree': 0.7703320921431667, 'gamma': 0.4937968914791946, 'min_child_weight': 9}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:16,515] Trial 77 finished with value: 0.8425438596491228 and parameters: {'n_estimators': 220, 'max_depth': 5, 'learning_rate': 0.17301530862292092, 'subsample': 0.9332990180997333, 'colsample_bytree': 0.8623930155011044, 'gamma': 0.2390641469254633, 'min_child_weight': 2}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:17,547] Trial 78 finished with value: 0.859298245614035 and parameters: {'n_estimators': 229, 'max_depth': 6, 'learning_rate': 0.0814752575405349, 'subsample': 0.9790556716513651, 'colsample_bytree': 0.8971527871960834, 'gamma': 0.6880784626105085, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:18,706] Trial 79 finished with value: 0.8487938596491228 and parameters: {'n_estimators': 239, 'max_depth': 7, 'learning_rate': 0.09843937410182389, 'subsample': 0.9991256602653633, 'colsample_bytree': 0.8307695639985804, 'gamma': 0.11217810062993816, 'min_child_weight': 3}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:19,585] Trial 80 finished with value: 0.8341008771929823 and parameters: {'n_estimators': 204, 'max_depth': 4, 'learning_rate': 0.15230794189934338, 'subsample': 0.6404895002099482, 'colsample_bytree': 0.7888079044295743, 'gamma': 2.663548395344517, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:20,941] Trial 81 finished with value: 0.8739254385964912 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.0765545135722932, 'subsample': 0.9703986352526371, 'colsample_bytree': 0.7191422603368732, 'gamma': 0.21883061280484054, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:22,445] Trial 82 finished with value: 0.865548245614035 and parameters: {'n_estimators': 216, 'max_depth': 4, 'learning_rate': 0.057227010788627505, 'subsample': 0.9685112926924812, 'colsample_bytree': 0.7285548486468364, 'gamma': 0.3345134880741848, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:23,560] Trial 83 finished with value: 0.8655701754385964 and parameters: {'n_estimators': 233, 'max_depth': 8, 'learning_rate': 0.07644937115483716, 'subsample': 0.9472182162447553, 'colsample_bytree': 0.7086552864393358, 'gamma': 0.5848662656110416, 'min_child_weight': 2}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:24,483] Trial 84 finished with value: 0.8718201754385964 and parameters: {'n_estimators': 211, 'max_depth': 4, 'learning_rate': 0.10939267366642152, 'subsample': 0.958998401102183, 'colsample_bytree': 0.6903906171868784, 'gamma': 0.42481890459733584, 'min_child_weight': 1}. Best is trial 53 with value: 0.8739254385964912.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:25,741] Trial 85 finished with value: 0.8760745614035088 and parameters: {'n_estimators': 226, 'max_depth': 4, 'learning_rate': 0.10773910850586058, 'subsample': 0.9895671416044908, 'colsample_bytree': 0.688286467478683, 'gamma': 0.22999718425706003, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:27,011] Trial 86 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 226, 'max_depth': 4, 'learning_rate': 0.02767480471889061, 'subsample': 0.9579512083078252, 'colsample_bytree': 0.681537812572274, 'gamma': 0.5046205366638434, 'min_child_weight': 2}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:27,909] Trial 87 finished with value: 0.8571052631578947 and parameters: {'n_estimators': 252, 'max_depth': 3, 'learning_rate': 0.10745139551652885, 'subsample': 0.9914367822791712, 'colsample_bytree': 0.6846259171360668, 'gamma': 0.8953909881724853, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:29,546] Trial 88 finished with value: 0.8613815789473683 and parameters: {'n_estimators': 220, 'max_depth': 5, 'learning_rate': 0.12128916171465713, 'subsample': 0.9729441859108613, 'colsample_bytree': 0.6625792747347645, 'gamma': 0.04475947472997373, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:30,302] Trial 89 finished with value: 0.8487280701754386 and parameters: {'n_estimators': 209, 'max_depth': 3, 'learning_rate': 0.11001554185159312, 'subsample': 0.9895354906090785, 'colsample_bytree': 0.6989341132219942, 'gamma': 2.16609243862953, 'min_child_weight': 2}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:31,436] Trial 90 finished with value: 0.8718421052631579 and parameters: {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.09062880733640112, 'subsample': 0.9514381939852651, 'colsample_bytree': 0.6957944242455302, 'gamma': 1.126438121128946, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:32,542] Trial 91 finished with value: 0.8676096491228071 and parameters: {'n_estimators': 193, 'max_depth': 4, 'learning_rate': 0.06860183994837574, 'subsample': 0.9521482310351779, 'colsample_bytree': 0.6451557541510043, 'gamma': 0.23686097214610335, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:33,512] Trial 92 finished with value: 0.8633991228070176 and parameters: {'n_estimators': 203, 'max_depth': 4, 'learning_rate': 0.09022236362330814, 'subsample': 0.818700197239353, 'colsample_bytree': 0.6932362142740854, 'gamma': 0.693758801262599, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:34,274] Trial 93 finished with value: 0.8676535087719298 and parameters: {'n_estimators': 210, 'max_depth': 4, 'learning_rate': 0.10177078152408872, 'subsample': 0.9664033262894317, 'colsample_bytree': 0.7157106188420622, 'gamma': 1.0827336583931029, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:35,091] Trial 94 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 197, 'max_depth': 4, 'learning_rate': 0.12636593837670748, 'subsample': 0.9796539735544157, 'colsample_bytree': 0.6490711030591569, 'gamma': 0.42359463873751513, 'min_child_weight': 2}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:36,797] Trial 95 finished with value: 0.8424780701754386 and parameters: {'n_estimators': 229, 'max_depth': 5, 'learning_rate': 0.13514247426447676, 'subsample': 0.92435987644277, 'colsample_bytree': 0.671519938085838, 'gamma': 0.14134704374795928, 'min_child_weight': 5}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:37,801] Trial 96 finished with value: 0.8592105263157895 and parameters: {'n_estimators': 217, 'max_depth': 4, 'learning_rate': 0.08527260692738407, 'subsample': 0.8720538242304268, 'colsample_bytree': 0.6903886341968203, 'gamma': 0.5428619586044972, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:38,449] Trial 97 finished with value: 0.8592105263157895 and parameters: {'n_estimators': 182, 'max_depth': 3, 'learning_rate': 0.09373826365313895, 'subsample': 0.9924464338026749, 'colsample_bytree': 0.7120924600647329, 'gamma': 0.7981334990115003, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:39,351] Trial 98 finished with value: 0.8571491228070176 and parameters: {'n_estimators': 223, 'max_depth': 4, 'learning_rate': 0.11356062286932939, 'subsample': 0.9726337250125993, 'colsample_bytree': 0.6710342001134686, 'gamma': 1.1957036662325597, 'min_child_weight': 2}. Best is trial 85 with value: 0.8760745614035088.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:12:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:12:40,590] Trial 99 finished with value: 0.8739692982456141 and parameters: {'n_estimators': 246, 'max_depth': 4, 'learning_rate': 0.1025996794226064, 'subsample': 0.9997340789376308, 'colsample_bytree': 0.7250948251587906, 'gamma': 0.08656275583561687, 'min_child_weight': 1}. Best is trial 85 with value: 0.8760745614035088.
✅ Best XGBoost Parameters: {'n_estimators': 226, 'max_depth': 4, 'learning_rate': 0.10773910850586058, 'subsample': 0.9895671416044908, 'colsample_bytree': 0.688286467478683, 'gamma': 0.22999718425706003, 'min_child_weight': 1}
Tuning RandomForest with Optuna...
[I 2025-05-01 23:12:40,793] A new study created in memory with name: no-name-cf6475ad-799f-4901-bfc2-dd8b6d37d9d8
[I 2025-05-01 23:12:41,818] Trial 0 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 172, 'max_depth': 8, 'min_samples_split': 10}. Best is trial 0 with value: 0.6975308641975309.
[I 2025-05-01 23:12:42,030] Trial 1 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 65, 'max_depth': 13, 'min_samples_split': 10}. Best is trial 1 with value: 0.7160493827160493.
[I 2025-05-01 23:12:42,437] Trial 2 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 148, 'max_depth': 17, 'min_samples_split': 6}. Best is trial 1 with value: 0.7160493827160493.
[I 2025-05-01 23:12:42,834] Trial 3 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 152, 'max_depth': 10, 'min_samples_split': 10}. Best is trial 1 with value: 0.7160493827160493.
[I 2025-05-01 23:12:43,050] Trial 4 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 88, 'max_depth': 14, 'min_samples_split': 5}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 23:12:43,285] Trial 5 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 103, 'max_depth': 18, 'min_samples_split': 6}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 23:12:43,633] Trial 6 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 159, 'max_depth': 20, 'min_samples_split': 8}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 23:12:43,951] Trial 7 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 152, 'max_depth': 9, 'min_samples_split': 10}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 23:12:44,218] Trial 8 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 105, 'max_depth': 15, 'min_samples_split': 7}. Best is trial 4 with value: 0.7222222222222222.
[I 2025-05-01 23:12:44,382] Trial 9 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 71, 'max_depth': 13, 'min_samples_split': 10}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:44,544] Trial 10 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 55, 'max_depth': 5, 'min_samples_split': 2}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:44,796] Trial 11 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 84, 'max_depth': 13, 'min_samples_split': 4}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:45,032] Trial 12 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 84, 'max_depth': 15, 'min_samples_split': 4}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:45,374] Trial 13 finished with value: 0.691358024691358 and parameters: {'n_estimators': 123, 'max_depth': 11, 'min_samples_split': 8}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:45,932] Trial 14 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 194, 'max_depth': 15, 'min_samples_split': 4}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:46,204] Trial 15 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 76, 'max_depth': 12, 'min_samples_split': 5}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:46,524] Trial 16 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 105, 'max_depth': 7, 'min_samples_split': 2}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:46,681] Trial 17 finished with value: 0.691358024691358 and parameters: {'n_estimators': 53, 'max_depth': 17, 'min_samples_split': 8}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:47,059] Trial 18 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 127, 'max_depth': 14, 'min_samples_split': 5}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:47,318] Trial 19 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 93, 'max_depth': 11, 'min_samples_split': 3}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:47,634] Trial 20 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 124, 'max_depth': 19, 'min_samples_split': 7}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:47,887] Trial 21 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 99, 'max_depth': 17, 'min_samples_split': 6}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:48,073] Trial 22 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 69, 'max_depth': 18, 'min_samples_split': 6}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:48,348] Trial 23 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 113, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:48,562] Trial 24 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 86, 'max_depth': 20, 'min_samples_split': 7}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:48,751] Trial 25 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 69, 'max_depth': 14, 'min_samples_split': 9}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:49,039] Trial 26 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 117, 'max_depth': 12, 'min_samples_split': 6}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:49,360] Trial 27 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 132, 'max_depth': 12, 'min_samples_split': 9}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:49,608] Trial 28 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 89, 'max_depth': 10, 'min_samples_split': 3}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:50,067] Trial 29 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 185, 'max_depth': 8, 'min_samples_split': 3}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:50,412] Trial 30 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 138, 'max_depth': 10, 'min_samples_split': 9}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:50,897] Trial 31 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 91, 'max_depth': 11, 'min_samples_split': 3}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:51,286] Trial 32 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 111, 'max_depth': 11, 'min_samples_split': 3}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:51,554] Trial 33 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 77, 'max_depth': 9, 'min_samples_split': 2}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:51,754] Trial 34 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 59, 'max_depth': 7, 'min_samples_split': 3}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:52,002] Trial 35 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 93, 'max_depth': 12, 'min_samples_split': 4}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:52,304] Trial 36 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 113, 'max_depth': 10, 'min_samples_split': 3}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:52,495] Trial 37 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 64, 'max_depth': 13, 'min_samples_split': 10}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:52,745] Trial 38 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 96, 'max_depth': 9, 'min_samples_split': 2}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:52,954] Trial 39 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 76, 'max_depth': 11, 'min_samples_split': 5}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:53,825] Trial 40 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 162, 'max_depth': 13, 'min_samples_split': 7}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:54,334] Trial 41 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 92, 'max_depth': 12, 'min_samples_split': 4}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:54,707] Trial 42 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 78, 'max_depth': 12, 'min_samples_split': 4}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:55,003] Trial 43 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 101, 'max_depth': 10, 'min_samples_split': 3}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:55,303] Trial 44 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 117, 'max_depth': 14, 'min_samples_split': 4}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:55,599] Trial 45 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 90, 'max_depth': 13, 'min_samples_split': 2}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:55,837] Trial 46 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 83, 'max_depth': 11, 'min_samples_split': 6}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:56,270] Trial 47 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 137, 'max_depth': 9, 'min_samples_split': 4}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:56,611] Trial 48 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 106, 'max_depth': 8, 'min_samples_split': 5}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:56,819] Trial 49 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 59, 'max_depth': 10, 'min_samples_split': 3}. Best is trial 9 with value: 0.7283950617283951.
[I 2025-05-01 23:12:56,994] Trial 50 finished with value: 0.7469135802469136 and parameters: {'n_estimators': 50, 'max_depth': 15, 'min_samples_split': 8}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:57,301] Trial 51 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 51, 'max_depth': 16, 'min_samples_split': 10}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:57,520] Trial 52 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 70, 'max_depth': 12, 'min_samples_split': 8}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:57,698] Trial 53 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 58, 'max_depth': 15, 'min_samples_split': 9}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:57,903] Trial 54 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 65, 'max_depth': 13, 'min_samples_split': 8}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:58,132] Trial 55 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 82, 'max_depth': 16, 'min_samples_split': 7}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:58,387] Trial 56 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 96, 'max_depth': 14, 'min_samples_split': 10}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:58,561] Trial 57 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 72, 'max_depth': 11, 'min_samples_split': 8}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:58,786] Trial 58 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 88, 'max_depth': 14, 'min_samples_split': 6}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:58,949] Trial 59 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 63, 'max_depth': 13, 'min_samples_split': 9}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:59,143] Trial 60 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 81, 'max_depth': 12, 'min_samples_split': 5}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:59,412] Trial 61 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 121, 'max_depth': 15, 'min_samples_split': 4}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:12:59,753] Trial 62 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 130, 'max_depth': 15, 'min_samples_split': 4}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:00,272] Trial 63 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 107, 'max_depth': 15, 'min_samples_split': 3}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:00,702] Trial 64 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 122, 'max_depth': 16, 'min_samples_split': 2}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:01,199] Trial 65 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 120, 'max_depth': 11, 'min_samples_split': 4}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:01,805] Trial 66 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 146, 'max_depth': 18, 'min_samples_split': 5}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:01,994] Trial 67 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 50, 'max_depth': 14, 'min_samples_split': 3}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:02,281] Trial 68 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 97, 'max_depth': 11, 'min_samples_split': 7}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:02,580] Trial 69 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 109, 'max_depth': 17, 'min_samples_split': 6}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:02,835] Trial 70 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 102, 'max_depth': 5, 'min_samples_split': 3}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:03,185] Trial 71 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 116, 'max_depth': 14, 'min_samples_split': 4}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:03,502] Trial 72 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 118, 'max_depth': 13, 'min_samples_split': 4}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:03,779] Trial 73 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 114, 'max_depth': 15, 'min_samples_split': 4}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:04,078] Trial 74 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 128, 'max_depth': 12, 'min_samples_split': 3}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:04,426] Trial 75 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 144, 'max_depth': 14, 'min_samples_split': 4}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:04,743] Trial 76 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 134, 'max_depth': 10, 'min_samples_split': 5}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:05,143] Trial 77 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 125, 'max_depth': 13, 'min_samples_split': 2}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:05,369] Trial 78 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 75, 'max_depth': 12, 'min_samples_split': 3}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:05,622] Trial 79 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 92, 'max_depth': 15, 'min_samples_split': 4}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:05,860] Trial 80 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 86, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:06,115] Trial 81 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 94, 'max_depth': 13, 'min_samples_split': 2}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:06,391] Trial 82 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 102, 'max_depth': 12, 'min_samples_split': 2}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:06,726] Trial 83 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 90, 'max_depth': 14, 'min_samples_split': 2}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:06,976] Trial 84 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 79, 'max_depth': 13, 'min_samples_split': 3}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:07,292] Trial 85 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 109, 'max_depth': 9, 'min_samples_split': 6}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:07,501] Trial 86 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 73, 'max_depth': 11, 'min_samples_split': 3}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:07,737] Trial 87 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 88, 'max_depth': 14, 'min_samples_split': 8}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:07,943] Trial 88 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 68, 'max_depth': 15, 'min_samples_split': 2}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:08,206] Trial 89 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 98, 'max_depth': 10, 'min_samples_split': 9}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:08,384] Trial 90 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 54, 'max_depth': 12, 'min_samples_split': 10}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:08,563] Trial 91 finished with value: 0.691358024691358 and parameters: {'n_estimators': 60, 'max_depth': 13, 'min_samples_split': 8}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:08,709] Trial 92 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 56, 'max_depth': 13, 'min_samples_split': 7}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:08,864] Trial 93 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 63, 'max_depth': 13, 'min_samples_split': 9}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:09,050] Trial 94 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 67, 'max_depth': 12, 'min_samples_split': 4}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:09,383] Trial 95 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 81, 'max_depth': 14, 'min_samples_split': 7}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:09,683] Trial 96 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 84, 'max_depth': 14, 'min_samples_split': 7}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:10,023] Trial 97 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 104, 'max_depth': 15, 'min_samples_split': 6}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:10,429] Trial 98 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 161, 'max_depth': 16, 'min_samples_split': 5}. Best is trial 50 with value: 0.7469135802469136.
[I 2025-05-01 23:13:10,667] Trial 99 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 89, 'max_depth': 11, 'min_samples_split': 8}. Best is trial 50 with value: 0.7469135802469136.
Best params for RandomForest: {'n_estimators': 50, 'max_depth': 15, 'min_samples_split': 8}
Tuning XGBoost with Optuna...
[I 2025-05-01 23:13:10,804] A new study created in memory with name: no-name-0d7e017e-7e88-4212-80a1-2d251eeba6e8
[I 2025-05-01 23:13:11,067] Trial 0 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 169, 'max_depth': 6, 'learning_rate': 0.04442926395759113}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:11,318] Trial 1 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 108, 'max_depth': 9, 'learning_rate': 0.05670558569476221}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:11,769] Trial 2 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 223, 'max_depth': 8, 'learning_rate': 0.07454515206925666}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:12,018] Trial 3 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 164, 'max_depth': 10, 'learning_rate': 0.250788474341242}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:12,525] Trial 4 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 298, 'max_depth': 8, 'learning_rate': 0.05988641940591535}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:12,811] Trial 5 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 299, 'max_depth': 5, 'learning_rate': 0.2600649266439286}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:13,063] Trial 6 finished with value: 0.691358024691358 and parameters: {'n_estimators': 216, 'max_depth': 5, 'learning_rate': 0.12790055900229966}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:13,419] Trial 7 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 280, 'max_depth': 7, 'learning_rate': 0.1643449274744039}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:13,677] Trial 8 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 118, 'max_depth': 9, 'learning_rate': 0.04569731538976856}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:13,921] Trial 9 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 273, 'max_depth': 4, 'learning_rate': 0.13388871356031162}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:14,093] Trial 10 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 165, 'max_depth': 3, 'learning_rate': 0.014360033155744557}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:14,363] Trial 11 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.14086846453033264}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:14,620] Trial 12 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 176, 'max_depth': 6, 'learning_rate': 0.20171306392411148}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:14,844] Trial 13 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 247, 'max_depth': 3, 'learning_rate': 0.11086371172171328}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:15,097] Trial 14 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 196, 'max_depth': 5, 'learning_rate': 0.19362363868686855}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:15,350] Trial 15 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 145, 'max_depth': 6, 'learning_rate': 0.09242612139017979}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:15,666] Trial 16 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 256, 'max_depth': 4, 'learning_rate': 0.01840362991002531}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:15,879] Trial 17 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 140, 'max_depth': 7, 'learning_rate': 0.2978071525537062}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:16,125] Trial 18 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 201, 'max_depth': 4, 'learning_rate': 0.16994993662649738}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:16,450] Trial 19 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 229, 'max_depth': 6, 'learning_rate': 0.10290143508154653}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:16,705] Trial 20 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 187, 'max_depth': 5, 'learning_rate': 0.22749539582382097}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:17,098] Trial 21 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 263, 'max_depth': 4, 'learning_rate': 0.1378504577786015}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:17,374] Trial 22 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 271, 'max_depth': 3, 'learning_rate': 0.14074728382758286}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:17,659] Trial 23 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 242, 'max_depth': 4, 'learning_rate': 0.1878784583404266}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:17,924] Trial 24 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 236, 'max_depth': 4, 'learning_rate': 0.19349602448078473}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:18,432] Trial 25 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 211, 'max_depth': 5, 'learning_rate': 0.21769550507781005}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:18,708] Trial 26 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 280, 'max_depth': 3, 'learning_rate': 0.03548422920505931}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:19,057] Trial 27 finished with value: 0.691358024691358 and parameters: {'n_estimators': 237, 'max_depth': 7, 'learning_rate': 0.08435549830222944}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:19,255] Trial 28 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 133, 'max_depth': 6, 'learning_rate': 0.178399792709893}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:19,471] Trial 29 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 106, 'max_depth': 8, 'learning_rate': 0.1202029628896959}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:19,770] Trial 30 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 281, 'max_depth': 4, 'learning_rate': 0.1522078973473631}. Best is trial 0 with value: 0.7283950617283951.
[I 2025-05-01 23:13:20,056] Trial 31 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.14988731074173264}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:20,310] Trial 32 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 267, 'max_depth': 3, 'learning_rate': 0.06707368220895643}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:20,592] Trial 33 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 223, 'max_depth': 5, 'learning_rate': 0.22882935285110712}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:20,857] Trial 34 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 241, 'max_depth': 4, 'learning_rate': 0.18101133659813362}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:21,153] Trial 35 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 240, 'max_depth': 5, 'learning_rate': 0.1824137057310694}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:21,441] Trial 36 finished with value: 0.6851851851851852 and parameters: {'n_estimators': 211, 'max_depth': 9, 'learning_rate': 0.2421280808644352}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:21,674] Trial 37 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 156, 'max_depth': 10, 'learning_rate': 0.27597852311999616}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:22,023] Trial 38 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 180, 'max_depth': 8, 'learning_rate': 0.21356185449839246}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:22,386] Trial 39 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 291, 'max_depth': 4, 'learning_rate': 0.15528278567823262}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:22,657] Trial 40 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 197, 'max_depth': 5, 'learning_rate': 0.26772969128052926}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:22,948] Trial 41 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 255, 'max_depth': 4, 'learning_rate': 0.17713454824402636}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:23,181] Trial 42 finished with value: 0.691358024691358 and parameters: {'n_estimators': 225, 'max_depth': 3, 'learning_rate': 0.16012495108204258}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:23,463] Trial 43 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 247, 'max_depth': 4, 'learning_rate': 0.12336387622933721}. Best is trial 31 with value: 0.7345679012345679.
[I 2025-05-01 23:13:23,726] Trial 44 finished with value: 0.7469135802469136 and parameters: {'n_estimators': 293, 'max_depth': 3, 'learning_rate': 0.19968984539879137}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:23,980] Trial 45 finished with value: 0.6790123456790124 and parameters: {'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.20409695599687588}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:24,203] Trial 46 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 287, 'max_depth': 3, 'learning_rate': 0.24470197245956102}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:24,365] Trial 47 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 164, 'max_depth': 3, 'learning_rate': 0.1917608403304923}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:24,657] Trial 48 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 262, 'max_depth': 5, 'learning_rate': 0.2162108938024671}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:24,941] Trial 49 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 218, 'max_depth': 4, 'learning_rate': 0.053015270992660596}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:25,189] Trial 50 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 116, 'max_depth': 6, 'learning_rate': 0.032594306359844505}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:25,488] Trial 51 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 275, 'max_depth': 4, 'learning_rate': 0.1675887072636321}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:25,790] Trial 52 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 277, 'max_depth': 4, 'learning_rate': 0.14838196295210565}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:26,043] Trial 53 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 257, 'max_depth': 3, 'learning_rate': 0.16445587985192994}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:26,358] Trial 54 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 291, 'max_depth': 5, 'learning_rate': 0.20482194587041008}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:26,631] Trial 55 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 245, 'max_depth': 4, 'learning_rate': 0.18469192121332675}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:27,136] Trial 56 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 232, 'max_depth': 3, 'learning_rate': 0.1668945554172939}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:27,431] Trial 57 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 271, 'max_depth': 4, 'learning_rate': 0.23107385158130456}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:27,801] Trial 58 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 286, 'max_depth': 5, 'learning_rate': 0.09637334025495159}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:28,139] Trial 59 finished with value: 0.691358024691358 and parameters: {'n_estimators': 255, 'max_depth': 6, 'learning_rate': 0.17282024816498848}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:28,358] Trial 60 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 206, 'max_depth': 3, 'learning_rate': 0.19989325498429683}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:28,681] Trial 61 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 272, 'max_depth': 4, 'learning_rate': 0.14299606731148082}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:29,004] Trial 62 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 265, 'max_depth': 4, 'learning_rate': 0.1328767237741464}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:29,318] Trial 63 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 274, 'max_depth': 4, 'learning_rate': 0.11601522902028033}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:29,557] Trial 64 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 173, 'max_depth': 4, 'learning_rate': 0.11622264022362128}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:30,022] Trial 65 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 294, 'max_depth': 7, 'learning_rate': 0.07650817636838157}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:30,316] Trial 66 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 242, 'max_depth': 5, 'learning_rate': 0.18561787204720953}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:30,639] Trial 67 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 284, 'max_depth': 4, 'learning_rate': 0.10903724092699807}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:30,909] Trial 68 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 276, 'max_depth': 3, 'learning_rate': 0.15901298911218856}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:31,240] Trial 69 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 253, 'max_depth': 5, 'learning_rate': 0.12925330374375765}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:31,565] Trial 70 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 261, 'max_depth': 4, 'learning_rate': 0.025267259604878828}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:31,863] Trial 71 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 271, 'max_depth': 4, 'learning_rate': 0.14611086346757443}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:32,181] Trial 72 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 295, 'max_depth': 4, 'learning_rate': 0.17123374341271327}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:32,636] Trial 73 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 267, 'max_depth': 8, 'learning_rate': 0.14282670132805064}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:32,964] Trial 74 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 280, 'max_depth': 4, 'learning_rate': 0.19409572643517112}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:33,207] Trial 75 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 234, 'max_depth': 3, 'learning_rate': 0.1536345272707919}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:33,566] Trial 76 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 274, 'max_depth': 9, 'learning_rate': 0.2114662507422651}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:33,848] Trial 77 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 248, 'max_depth': 4, 'learning_rate': 0.1781852034910971}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:34,194] Trial 78 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 288, 'max_depth': 5, 'learning_rate': 0.13887840534925616}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:34,443] Trial 79 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 261, 'max_depth': 3, 'learning_rate': 0.18760302120679898}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:34,701] Trial 80 finished with value: 0.691358024691358 and parameters: {'n_estimators': 149, 'max_depth': 7, 'learning_rate': 0.08594311828763776}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:35,014] Trial 81 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 252, 'max_depth': 5, 'learning_rate': 0.10261575397673695}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:35,346] Trial 82 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 252, 'max_depth': 5, 'learning_rate': 0.13232839226337326}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:35,700] Trial 83 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 241, 'max_depth': 6, 'learning_rate': 0.12039562525861877}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:35,996] Trial 84 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 258, 'max_depth': 4, 'learning_rate': 0.1292797913078502}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:36,315] Trial 85 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 268, 'max_depth': 5, 'learning_rate': 0.1630263001361154}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:36,648] Trial 86 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 227, 'max_depth': 5, 'learning_rate': 0.06431815768787515}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:37,046] Trial 87 finished with value: 0.7283950617283951 and parameters: {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1503518144721582}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:37,521] Trial 88 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 282, 'max_depth': 6, 'learning_rate': 0.043330564308249456}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:37,827] Trial 89 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 191, 'max_depth': 4, 'learning_rate': 0.22396104655229113}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:38,154] Trial 90 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 218, 'max_depth': 5, 'learning_rate': 0.17166796806383938}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:38,557] Trial 91 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 238, 'max_depth': 5, 'learning_rate': 0.06169232264039707}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:38,949] Trial 92 finished with value: 0.7098765432098766 and parameters: {'n_estimators': 229, 'max_depth': 6, 'learning_rate': 0.04393124846976689}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:39,239] Trial 93 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 228, 'max_depth': 4, 'learning_rate': 0.05259808240344099}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:39,429] Trial 94 finished with value: 0.7160493827160493 and parameters: {'n_estimators': 128, 'max_depth': 4, 'learning_rate': 0.07024316785076269}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:39,681] Trial 95 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 252, 'max_depth': 3, 'learning_rate': 0.013479594221781872}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:39,933] Trial 96 finished with value: 0.7037037037037037 and parameters: {'n_estimators': 244, 'max_depth': 5, 'learning_rate': 0.19792376479414686}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:40,204] Trial 97 finished with value: 0.7345679012345679 and parameters: {'n_estimators': 248, 'max_depth': 4, 'learning_rate': 0.12644903360765522}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:40,530] Trial 98 finished with value: 0.7222222222222222 and parameters: {'n_estimators': 297, 'max_depth': 4, 'learning_rate': 0.11117380589507966}. Best is trial 44 with value: 0.7469135802469136.
[I 2025-05-01 23:13:40,792] Trial 99 finished with value: 0.6975308641975309 and parameters: {'n_estimators': 268, 'max_depth': 3, 'learning_rate': 0.12641194597041636}. Best is trial 44 with value: 0.7469135802469136.
Best params for XGBoost: {'n_estimators': 293, 'max_depth': 3, 'learning_rate': 0.19968984539879137}
Best model: XGBoost
01-05-2025 23:13:41 Retraining best model on original full dataset...
01-05-2025 23:13:41 ✅ Exported and logged final model to W&B as artifact: final_model_xgboost.pkl
Requirement already satisfied: wandb in c:\users\x-hp\anaconda3\lib\site-packages (0.19.8)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.1.43)
Requirement already satisfied: platformdirs in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (4.25.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (5.9.0)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.8.2)
Requirement already satisfied: pyyaml in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (6.0.1)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.18.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (75.1.0)
Requirement already satisfied: colorama in c:\users\x-hp\anaconda3\lib\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)
Requirement already satisfied: typing-extensions>=4.6.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)
Requirement already satisfied: smmap<5,>=3.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.login() after wandb.init() has no effect.
01-05-2025 23:27:44 Downloading and reading train artifact
[34m[1mwandb[0m:   1 of 1 files downloaded.
01-05-2025 23:27:46 Spliting data into train/val
01-05-2025 23:27:46 x train: (375, 28)
01-05-2025 23:27:46 y train: (375,)
01-05-2025 23:27:46 x val: (162, 28)
01-05-2025 23:27:46 y val: (162,)
01-05-2025 23:27:46 Outlier Removal
01-05-2025 23:27:46 x_train shape [original]: (375, 28)
01-05-2025 23:27:46 x_train shape [outlier removal]: (357, 28)
01-05-2025 23:27:47 Data after SMOTE: (476, 28)
01-05-2025 23:27:47 [Logistic] Accuracy: 0.7469135802469136
01-05-2025 23:27:47 [Random Forest] Accuracy: 0.7222222222222222
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:27:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
01-05-2025 23:27:48 [XGBoost] Accuracy: 0.7222222222222222
                 Model  Accuracy
0  Logistic Regression  0.746914
1        Random Forest  0.722222
2              XGBoost  0.722222
[I 2025-05-01 23:27:52,421] A new study created in memory with name: no-name-77fd9985-5e0a-41ba-8ddb-e6552e1deb1f
[I 2025-05-01 23:27:56,125] Trial 0 finished with value: 0.8277631578947368 and parameters: {'n_estimators': 162, 'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.8277631578947368.
[I 2025-05-01 23:27:58,065] Trial 1 finished with value: 0.8466447368421053 and parameters: {'n_estimators': 173, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.8466447368421053.
[I 2025-05-01 23:28:00,097] Trial 2 finished with value: 0.7815350877192982 and parameters: {'n_estimators': 180, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.8466447368421053.
[I 2025-05-01 23:28:01,211] Trial 3 finished with value: 0.7962280701754386 and parameters: {'n_estimators': 113, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.8466447368421053.
[I 2025-05-01 23:28:03,007] Trial 4 finished with value: 0.7983114035087719 and parameters: {'n_estimators': 131, 'max_depth': 3, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 1 with value: 0.8466447368421053.
[I 2025-05-01 23:28:05,286] Trial 5 finished with value: 0.8172587719298245 and parameters: {'n_estimators': 69, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.8466447368421053.
[I 2025-05-01 23:28:10,480] Trial 6 finished with value: 0.8403728070175438 and parameters: {'n_estimators': 110, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.8466447368421053.
[I 2025-05-01 23:28:12,130] Trial 7 finished with value: 0.8424561403508772 and parameters: {'n_estimators': 146, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 1 with value: 0.8466447368421053.
[I 2025-05-01 23:28:13,157] Trial 8 finished with value: 0.8403947368421052 and parameters: {'n_estimators': 79, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.8466447368421053.
[I 2025-05-01 23:28:14,487] Trial 9 finished with value: 0.7899122807017542 and parameters: {'n_estimators': 102, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 1 with value: 0.8466447368421053.
[I 2025-05-01 23:28:17,598] Trial 10 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 198, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 10 with value: 0.8466666666666667.
[I 2025-05-01 23:28:20,201] Trial 11 finished with value: 0.8529385964912279 and parameters: {'n_estimators': 199, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.8529385964912279.
[I 2025-05-01 23:28:22,326] Trial 12 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 199, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.8529385964912279.
[I 2025-05-01 23:28:24,781] Trial 13 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 197, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:26,525] Trial 14 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 151, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:28,275] Trial 15 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 152, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:30,472] Trial 16 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 134, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:32,393] Trial 17 finished with value: 0.8445394736842105 and parameters: {'n_estimators': 178, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:34,302] Trial 18 finished with value: 0.8424561403508772 and parameters: {'n_estimators': 161, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:35,582] Trial 19 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 93, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:37,184] Trial 20 finished with value: 0.8445833333333332 and parameters: {'n_estimators': 142, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:38,703] Trial 21 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 128, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:40,551] Trial 22 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 135, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:41,936] Trial 23 finished with value: 0.8529166666666667 and parameters: {'n_estimators': 123, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:42,556] Trial 24 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 53, 'max_depth': 8, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:45,108] Trial 25 finished with value: 0.8445394736842105 and parameters: {'n_estimators': 183, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:46,972] Trial 26 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 162, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:48,397] Trial 27 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 117, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:50,312] Trial 28 finished with value: 0.8445394736842105 and parameters: {'n_estimators': 150, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:52,133] Trial 29 finished with value: 0.8424780701754386 and parameters: {'n_estimators': 164, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:54,189] Trial 30 finished with value: 0.8340570175438596 and parameters: {'n_estimators': 186, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:56,605] Trial 31 finished with value: 0.8529385964912279 and parameters: {'n_estimators': 190, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:28:58,486] Trial 32 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 173, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:29:00,999] Trial 33 finished with value: 0.8445175438596492 and parameters: {'n_estimators': 191, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:29:02,874] Trial 34 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 171, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8529385964912282.
[I 2025-05-01 23:29:04,485] Trial 35 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 139, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:06,347] Trial 36 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 143, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:08,377] Trial 37 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 155, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 3}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:10,299] Trial 38 finished with value: 0.8340350877192982 and parameters: {'n_estimators': 136, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:11,601] Trial 39 finished with value: 0.8487499999999999 and parameters: {'n_estimators': 105, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:12,904] Trial 40 finished with value: 0.8466885964912281 and parameters: {'n_estimators': 116, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:14,437] Trial 41 finished with value: 0.8529385964912279 and parameters: {'n_estimators': 125, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:16,867] Trial 42 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 195, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:18,704] Trial 43 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 173, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:20,525] Trial 44 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 138, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:21,666] Trial 45 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 88, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:23,314] Trial 46 finished with value: 0.7961842105263157 and parameters: {'n_estimators': 157, 'max_depth': 3, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:25,674] Trial 47 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 168, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:27,882] Trial 48 finished with value: 0.8445394736842105 and parameters: {'n_estimators': 195, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:29,910] Trial 49 finished with value: 0.8445175438596492 and parameters: {'n_estimators': 121, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:32,923] Trial 50 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 178, 'max_depth': 7, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:35,289] Trial 51 finished with value: 0.8529385964912279 and parameters: {'n_estimators': 198, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:37,609] Trial 52 finished with value: 0.8508333333333333 and parameters: {'n_estimators': 192, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:39,636] Trial 53 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 187, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:41,510] Trial 54 finished with value: 0.8445614035087718 and parameters: {'n_estimators': 148, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:43,510] Trial 55 finished with value: 0.8466228070175438 and parameters: {'n_estimators': 187, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:45,671] Trial 56 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 181, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:47,359] Trial 57 finished with value: 0.8529605263157896 and parameters: {'n_estimators': 130, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:48,847] Trial 58 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 130, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:50,212] Trial 59 finished with value: 0.8445614035087721 and parameters: {'n_estimators': 111, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:52,097] Trial 60 finished with value: 0.8277192982456139 and parameters: {'n_estimators': 133, 'max_depth': 4, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:53,680] Trial 61 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 145, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:55,306] Trial 62 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 139, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:57,512] Trial 63 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 179, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:29:59,802] Trial 64 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 200, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:30:01,504] Trial 65 finished with value: 0.8550219298245614 and parameters: {'n_estimators': 130, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 35 with value: 0.8550438596491228.
[I 2025-05-01 23:30:03,068] Trial 66 finished with value: 0.8592982456140351 and parameters: {'n_estimators': 127, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:04,371] Trial 67 finished with value: 0.8466885964912281 and parameters: {'n_estimators': 120, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:05,934] Trial 68 finished with value: 0.8592982456140351 and parameters: {'n_estimators': 127, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:07,639] Trial 69 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 128, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:09,100] Trial 70 finished with value: 0.8592982456140351 and parameters: {'n_estimators': 127, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:10,690] Trial 71 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 128, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:12,383] Trial 72 finished with value: 0.8592982456140351 and parameters: {'n_estimators': 127, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:13,934] Trial 73 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 128, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:15,427] Trial 74 finished with value: 0.8592982456140351 and parameters: {'n_estimators': 125, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:17,318] Trial 75 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 108, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:18,679] Trial 76 finished with value: 0.8592982456140351 and parameters: {'n_estimators': 115, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 66 with value: 0.8592982456140351.
[I 2025-05-01 23:30:19,879] Trial 77 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 77 with value: 0.8634649122807018.
[I 2025-05-01 23:30:21,115] Trial 78 finished with value: 0.8655701754385964 and parameters: {'n_estimators': 95, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:22,564] Trial 79 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 98, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:23,784] Trial 80 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 99, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:25,098] Trial 81 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 99, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:26,431] Trial 82 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 99, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:27,881] Trial 83 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 98, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:29,305] Trial 84 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 96, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:30,780] Trial 85 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 96, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:32,418] Trial 86 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 97, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:33,534] Trial 87 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 86, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:34,465] Trial 88 finished with value: 0.7815131578947369 and parameters: {'n_estimators': 99, 'max_depth': 2, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:35,381] Trial 89 finished with value: 0.8424999999999999 and parameters: {'n_estimators': 75, 'max_depth': 6, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:36,496] Trial 90 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 92, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:37,964] Trial 91 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 99, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:39,139] Trial 92 finished with value: 0.8655701754385964 and parameters: {'n_estimators': 95, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:40,137] Trial 93 finished with value: 0.8655482456140351 and parameters: {'n_estimators': 82, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:41,131] Trial 94 finished with value: 0.8655482456140351 and parameters: {'n_estimators': 82, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:42,277] Trial 95 finished with value: 0.8655482456140351 and parameters: {'n_estimators': 82, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:43,370] Trial 96 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 82, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:44,178] Trial 97 finished with value: 0.8571271929824562 and parameters: {'n_estimators': 66, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:45,631] Trial 98 finished with value: 0.8466008771929824 and parameters: {'n_estimators': 75, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
[I 2025-05-01 23:30:47,531] Trial 99 finished with value: 0.8655701754385964 and parameters: {'n_estimators': 104, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 78 with value: 0.8655701754385964.
✅ Best Random Forest Parameters: {'n_estimators': 95, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}
[I 2025-05-01 23:30:47,536] A new study created in memory with name: no-name-f0bc78aa-aaf1-4296-b9f7-50f5cc68c006
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:48,346] Trial 0 finished with value: 0.8277850877192983 and parameters: {'n_estimators': 263, 'max_depth': 4, 'learning_rate': 0.20016882947362757, 'subsample': 0.9689724886389787, 'colsample_bytree': 0.8828883990063396, 'gamma': 3.655330314061065, 'min_child_weight': 7}. Best is trial 0 with value: 0.8277850877192983.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:49,256] Trial 1 finished with value: 0.8130701754385964 and parameters: {'n_estimators': 258, 'max_depth': 9, 'learning_rate': 0.15166991733650856, 'subsample': 0.7841871698823624, 'colsample_bytree': 0.8025254521325342, 'gamma': 1.1150221222662327, 'min_child_weight': 10}. Best is trial 0 with value: 0.8277850877192983.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:49,810] Trial 2 finished with value: 0.8340350877192982 and parameters: {'n_estimators': 199, 'max_depth': 6, 'learning_rate': 0.2721603437209152, 'subsample': 0.9400930605455955, 'colsample_bytree': 0.6979533837223999, 'gamma': 4.572307706395652, 'min_child_weight': 5}. Best is trial 2 with value: 0.8340350877192982.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:50,395] Trial 3 finished with value: 0.8404166666666667 and parameters: {'n_estimators': 178, 'max_depth': 10, 'learning_rate': 0.2519617352755154, 'subsample': 0.9019093199353151, 'colsample_bytree': 0.7911871511672367, 'gamma': 2.7319243817473065, 'min_child_weight': 7}. Best is trial 3 with value: 0.8404166666666667.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:50,973] Trial 4 finished with value: 0.8445614035087718 and parameters: {'n_estimators': 161, 'max_depth': 4, 'learning_rate': 0.22248555919034718, 'subsample': 0.9437783865067079, 'colsample_bytree': 0.9515450698638277, 'gamma': 1.428855935188174, 'min_child_weight': 3}. Best is trial 4 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:51,965] Trial 5 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 110, 'max_depth': 5, 'learning_rate': 0.15976918168634507, 'subsample': 0.9770734591313771, 'colsample_bytree': 0.8790748927788254, 'gamma': 0.23867290528428453, 'min_child_weight': 1}. Best is trial 5 with value: 0.8592763157894737.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:52,855] Trial 6 finished with value: 0.8257236842105262 and parameters: {'n_estimators': 282, 'max_depth': 9, 'learning_rate': 0.2727118634853819, 'subsample': 0.6365242148264227, 'colsample_bytree': 0.6747670985968458, 'gamma': 2.7001229683858745, 'min_child_weight': 2}. Best is trial 5 with value: 0.8592763157894737.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:53,892] Trial 7 finished with value: 0.8362061403508771 and parameters: {'n_estimators': 187, 'max_depth': 10, 'learning_rate': 0.10479446005192329, 'subsample': 0.6580490590847504, 'colsample_bytree': 0.8198337876412999, 'gamma': 3.1861727373340405, 'min_child_weight': 5}. Best is trial 5 with value: 0.8592763157894737.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:54,620] Trial 8 finished with value: 0.7921052631578946 and parameters: {'n_estimators': 209, 'max_depth': 7, 'learning_rate': 0.16204266948772988, 'subsample': 0.6207916971897522, 'colsample_bytree': 0.9020058278891294, 'gamma': 1.0068062581909687, 'min_child_weight': 9}. Best is trial 5 with value: 0.8592763157894737.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:55,647] Trial 9 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 202, 'max_depth': 10, 'learning_rate': 0.13676580222199297, 'subsample': 0.7158445732964035, 'colsample_bytree': 0.9057947270847804, 'gamma': 4.011769262723022, 'min_child_weight': 2}. Best is trial 5 with value: 0.8592763157894737.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:56,743] Trial 10 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 103, 'max_depth': 6, 'learning_rate': 0.01987916707479001, 'subsample': 0.8522494394606623, 'colsample_bytree': 0.985634340392205, 'gamma': 0.055762764670758, 'min_child_weight': 1}. Best is trial 5 with value: 0.8592763157894737.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:58,002] Trial 11 finished with value: 0.8382456140350877 and parameters: {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.010549579508592893, 'subsample': 0.8535704626133233, 'colsample_bytree': 0.9904331964259693, 'gamma': 0.12238749448095144, 'min_child_weight': 1}. Best is trial 5 with value: 0.8592763157894737.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:58,664] Trial 12 finished with value: 0.8445614035087718 and parameters: {'n_estimators': 101, 'max_depth': 3, 'learning_rate': 0.05661185214022729, 'subsample': 0.8327660875840499, 'colsample_bytree': 0.9996760466477448, 'gamma': 0.17353099594501753, 'min_child_weight': 3}. Best is trial 5 with value: 0.8592763157894737.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:59,311] Trial 13 finished with value: 0.8613377192982457 and parameters: {'n_estimators': 140, 'max_depth': 7, 'learning_rate': 0.08477109355702489, 'subsample': 0.9977465847238745, 'colsample_bytree': 0.8526591687146864, 'gamma': 1.7738058437399147, 'min_child_weight': 1}. Best is trial 13 with value: 0.8613377192982457.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:30:59,878] Trial 14 finished with value: 0.8487500000000001 and parameters: {'n_estimators': 143, 'max_depth': 7, 'learning_rate': 0.08830080888372714, 'subsample': 0.9985653737839334, 'colsample_bytree': 0.7452435849761686, 'gamma': 1.8710944063466113, 'min_child_weight': 4}. Best is trial 13 with value: 0.8613377192982457.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:30:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:00,570] Trial 15 finished with value: 0.8550657894736842 and parameters: {'n_estimators': 135, 'max_depth': 5, 'learning_rate': 0.09697948362776554, 'subsample': 0.9115769557534612, 'colsample_bytree': 0.6090719941110337, 'gamma': 1.9971284043517383, 'min_child_weight': 1}. Best is trial 13 with value: 0.8613377192982457.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:01,716] Trial 16 finished with value: 0.838311403508772 and parameters: {'n_estimators': 135, 'max_depth': 8, 'learning_rate': 0.1869705533198805, 'subsample': 0.7585286198994613, 'colsample_bytree': 0.8556124789556828, 'gamma': 0.6710702686901735, 'min_child_weight': 3}. Best is trial 13 with value: 0.8613377192982457.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:03,036] Trial 17 finished with value: 0.8298464912280702 and parameters: {'n_estimators': 231, 'max_depth': 5, 'learning_rate': 0.06775614411059205, 'subsample': 0.9986650710449128, 'colsample_bytree': 0.8436572838894866, 'gamma': 2.024859656022602, 'min_child_weight': 7}. Best is trial 13 with value: 0.8613377192982457.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:03,713] Trial 18 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 128, 'max_depth': 8, 'learning_rate': 0.12862190825095388, 'subsample': 0.8917590926832322, 'colsample_bytree': 0.7596544940272931, 'gamma': 1.6722231725921133, 'min_child_weight': 2}. Best is trial 13 with value: 0.8613377192982457.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:04,901] Trial 19 finished with value: 0.8551096491228071 and parameters: {'n_estimators': 162, 'max_depth': 5, 'learning_rate': 0.042312818337483335, 'subsample': 0.9573195568809436, 'colsample_bytree': 0.9409693760417136, 'gamma': 0.5828843029574164, 'min_child_weight': 4}. Best is trial 13 with value: 0.8613377192982457.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:05,479] Trial 20 finished with value: 0.8403728070175438 and parameters: {'n_estimators': 157, 'max_depth': 3, 'learning_rate': 0.11038124922679496, 'subsample': 0.8718689191934375, 'colsample_bytree': 0.864050629489702, 'gamma': 2.510200496871097, 'min_child_weight': 6}. Best is trial 13 with value: 0.8613377192982457.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:06,059] Trial 21 finished with value: 0.857171052631579 and parameters: {'n_estimators': 122, 'max_depth': 8, 'learning_rate': 0.1259577710984548, 'subsample': 0.9046752993457448, 'colsample_bytree': 0.7582950614868574, 'gamma': 1.5540489006484726, 'min_child_weight': 2}. Best is trial 13 with value: 0.8613377192982457.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:06,631] Trial 22 finished with value: 0.8760964912280702 and parameters: {'n_estimators': 117, 'max_depth': 8, 'learning_rate': 0.16334865624436007, 'subsample': 0.9239199089668532, 'colsample_bytree': 0.763399102069695, 'gamma': 1.2878301473077314, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:07,279] Trial 23 finished with value: 0.8634429824561403 and parameters: {'n_estimators': 117, 'max_depth': 7, 'learning_rate': 0.174923929512431, 'subsample': 0.9738610833490587, 'colsample_bytree': 0.7198262728453775, 'gamma': 0.7307031022344823, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:08,009] Trial 24 finished with value: 0.8529605263157893 and parameters: {'n_estimators': 148, 'max_depth': 7, 'learning_rate': 0.17994783413642418, 'subsample': 0.9277719142379577, 'colsample_bytree': 0.7031318168472273, 'gamma': 1.1389612331755652, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:08,902] Trial 25 finished with value: 0.8488157894736842 and parameters: {'n_estimators': 116, 'max_depth': 9, 'learning_rate': 0.21891550836547213, 'subsample': 0.9712925429670999, 'colsample_bytree': 0.6405972476565461, 'gamma': 0.7118867745023314, 'min_child_weight': 3}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:09,492] Trial 26 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 177, 'max_depth': 8, 'learning_rate': 0.22922000411136134, 'subsample': 0.929634345863203, 'colsample_bytree': 0.7299255499737196, 'gamma': 2.262284686358445, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:10,056] Trial 27 finished with value: 0.8362280701754387 and parameters: {'n_estimators': 124, 'max_depth': 7, 'learning_rate': 0.17822626846503908, 'subsample': 0.8131467000253833, 'colsample_bytree': 0.779274430521446, 'gamma': 1.2758297650044987, 'min_child_weight': 4}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:10,917] Trial 28 finished with value: 0.8655482456140351 and parameters: {'n_estimators': 149, 'max_depth': 8, 'learning_rate': 0.14892928031648903, 'subsample': 0.990138998532321, 'colsample_bytree': 0.8310304400242693, 'gamma': 0.8166226501334632, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:11,640] Trial 29 finished with value: 0.8173245614035087 and parameters: {'n_estimators': 222, 'max_depth': 9, 'learning_rate': 0.2010756785152673, 'subsample': 0.9645064013954339, 'colsample_bytree': 0.718812100127681, 'gamma': 0.7426307936879597, 'min_child_weight': 8}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:12,336] Trial 30 finished with value: 0.8424122807017544 and parameters: {'n_estimators': 154, 'max_depth': 8, 'learning_rate': 0.19982703715702826, 'subsample': 0.9750374227143984, 'colsample_bytree': 0.6700799240859725, 'gamma': 0.4649679176337891, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:13,084] Trial 31 finished with value: 0.8676754385964912 and parameters: {'n_estimators': 138, 'max_depth': 7, 'learning_rate': 0.14391047491662345, 'subsample': 0.9941645060818888, 'colsample_bytree': 0.8199742296031591, 'gamma': 0.9109450252061957, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:13,776] Trial 32 finished with value: 0.8739473684210527 and parameters: {'n_estimators': 120, 'max_depth': 8, 'learning_rate': 0.14435308045419315, 'subsample': 0.9612621134360072, 'colsample_bytree': 0.8164239194295266, 'gamma': 0.9138706218894355, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:14,773] Trial 33 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 133, 'max_depth': 8, 'learning_rate': 0.14415709849509978, 'subsample': 0.9437849078907868, 'colsample_bytree': 0.8054460723535681, 'gamma': 1.0280649618606204, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:15,366] Trial 34 finished with value: 0.8340570175438596 and parameters: {'n_estimators': 166, 'max_depth': 9, 'learning_rate': 0.123494943546896, 'subsample': 0.878423743506562, 'colsample_bytree': 0.8266452047223078, 'gamma': 4.896491847394419, 'min_child_weight': 3}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:16,092] Trial 35 finished with value: 0.855043859649123 and parameters: {'n_estimators': 175, 'max_depth': 6, 'learning_rate': 0.15426760886436028, 'subsample': 0.9323960543792161, 'colsample_bytree': 0.781776131179712, 'gamma': 1.3894155629856895, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:17,114] Trial 36 finished with value: 0.8088815789473685 and parameters: {'n_estimators': 113, 'max_depth': 9, 'learning_rate': 0.2928359620529126, 'subsample': 0.7779933677399014, 'colsample_bytree': 0.8196569780884373, 'gamma': 0.9739071580346307, 'min_child_weight': 10}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:17,981] Trial 37 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 149, 'max_depth': 8, 'learning_rate': 0.16287782279574897, 'subsample': 0.9565796016732058, 'colsample_bytree': 0.7885988565093699, 'gamma': 0.4151497250732742, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:18,823] Trial 38 finished with value: 0.8466228070175438 and parameters: {'n_estimators': 298, 'max_depth': 9, 'learning_rate': 0.14897823227391627, 'subsample': 0.9136298804329699, 'colsample_bytree': 0.8870167022156472, 'gamma': 3.332743046908209, 'min_child_weight': 3}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:19,601] Trial 39 finished with value: 0.8424780701754386 and parameters: {'n_estimators': 244, 'max_depth': 8, 'learning_rate': 0.11465418681112338, 'subsample': 0.9876697705671647, 'colsample_bytree': 0.8336786469355475, 'gamma': 1.394512307861526, 'min_child_weight': 5}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:20,326] Trial 40 finished with value: 0.8698245614035087 and parameters: {'n_estimators': 193, 'max_depth': 10, 'learning_rate': 0.19444565292745358, 'subsample': 0.9501709661663479, 'colsample_bytree': 0.8027333708683416, 'gamma': 0.9426239293850713, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:21,180] Trial 41 finished with value: 0.8698026315789473 and parameters: {'n_estimators': 259, 'max_depth': 10, 'learning_rate': 0.21064501293900462, 'subsample': 0.9556762770407764, 'colsample_bytree': 0.8024513896848761, 'gamma': 0.8972493102598448, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:22,445] Trial 42 finished with value: 0.8655263157894737 and parameters: {'n_estimators': 266, 'max_depth': 10, 'learning_rate': 0.23609505236697442, 'subsample': 0.9465539964366361, 'colsample_bytree': 0.8029404459853913, 'gamma': 0.38795443038845523, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:23,393] Trial 43 finished with value: 0.857171052631579 and parameters: {'n_estimators': 188, 'max_depth': 10, 'learning_rate': 0.19450688481323491, 'subsample': 0.9205569031615373, 'colsample_bytree': 0.7682121060773717, 'gamma': 1.2062994312349384, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:24,255] Trial 44 finished with value: 0.8676535087719298 and parameters: {'n_estimators': 253, 'max_depth': 10, 'learning_rate': 0.2112319342683472, 'subsample': 0.8893243268992015, 'colsample_bytree': 0.8106889122684441, 'gamma': 0.9536143249501008, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:24,951] Trial 45 finished with value: 0.8572149122807018 and parameters: {'n_estimators': 216, 'max_depth': 9, 'learning_rate': 0.2551313429754627, 'subsample': 0.9535782484727304, 'colsample_bytree': 0.7406121042034924, 'gamma': 1.5935312259863936, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:25,672] Trial 46 finished with value: 0.8403947368421052 and parameters: {'n_estimators': 194, 'max_depth': 10, 'learning_rate': 0.16622330621005402, 'subsample': 0.6904222414343232, 'colsample_bytree': 0.8005983793555581, 'gamma': 2.23321004871237, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:26,696] Trial 47 finished with value: 0.8068201754385965 and parameters: {'n_estimators': 275, 'max_depth': 6, 'learning_rate': 0.24485432758670866, 'subsample': 0.9442170612282534, 'colsample_bytree': 0.7705218287934359, 'gamma': 0.27988635663222916, 'min_child_weight': 9}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:27,606] Trial 48 finished with value: 0.8676973684210527 and parameters: {'n_estimators': 108, 'max_depth': 9, 'learning_rate': 0.13502499652120523, 'subsample': 0.8500714749196968, 'colsample_bytree': 0.9234599869951967, 'gamma': 1.2364330405350883, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:28,971] Trial 49 finished with value: 0.8550219298245614 and parameters: {'n_estimators': 231, 'max_depth': 10, 'learning_rate': 0.21210513987134216, 'subsample': 0.8554082620418286, 'colsample_bytree': 0.9263116547006154, 'gamma': 1.2419955080542715, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:29,845] Trial 50 finished with value: 0.8256578947368421 and parameters: {'n_estimators': 297, 'max_depth': 9, 'learning_rate': 0.13424119352203065, 'subsample': 0.8395174022970406, 'colsample_bytree': 0.9639338549153232, 'gamma': 3.8402248318394037, 'min_child_weight': 6}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:30,405] Trial 51 finished with value: 0.8676315789473683 and parameters: {'n_estimators': 108, 'max_depth': 10, 'learning_rate': 0.19071053485327033, 'subsample': 0.9822958337819105, 'colsample_bytree': 0.8759353310214554, 'gamma': 0.8614074463178486, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:31,056] Trial 52 finished with value: 0.8571052631578947 and parameters: {'n_estimators': 108, 'max_depth': 9, 'learning_rate': 0.16494889571789934, 'subsample': 0.8166347270661914, 'colsample_bytree': 0.9100454631972947, 'gamma': 1.5243008814070078, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:31,894] Trial 53 finished with value: 0.869736842105263 and parameters: {'n_estimators': 124, 'max_depth': 10, 'learning_rate': 0.13729896826293025, 'subsample': 0.9015818940783709, 'colsample_bytree': 0.8433393124539841, 'gamma': 0.5270811449804033, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:32,877] Trial 54 finished with value: 0.8487938596491228 and parameters: {'n_estimators': 126, 'max_depth': 10, 'learning_rate': 0.11702761961432409, 'subsample': 0.8680021050342562, 'colsample_bytree': 0.8678974968714055, 'gamma': 0.5079828318216955, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:33,800] Trial 55 finished with value: 0.8446052631578947 and parameters: {'n_estimators': 105, 'max_depth': 10, 'learning_rate': 0.0956917016989847, 'subsample': 0.8969112349100035, 'colsample_bytree': 0.840388447084909, 'gamma': 0.28381748652434735, 'min_child_weight': 3}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:34,362] Trial 56 finished with value: 0.8529166666666667 and parameters: {'n_estimators': 117, 'max_depth': 9, 'learning_rate': 0.17228761358796657, 'subsample': 0.8816350991758439, 'colsample_bytree': 0.7496402929618392, 'gamma': 1.843289672683221, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:35,095] Trial 57 finished with value: 0.8697368421052631 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.184780494744929, 'subsample': 0.9097256123889009, 'colsample_bytree': 0.8905103185275807, 'gamma': 0.09191819490109188, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:35,836] Trial 58 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 129, 'max_depth': 4, 'learning_rate': 0.18564052566519915, 'subsample': 0.9218647992041259, 'colsample_bytree': 0.8885071784712324, 'gamma': 0.09936128421751034, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:36,706] Trial 59 finished with value: 0.8551315789473684 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.20503043333172405, 'subsample': 0.9054588152204305, 'colsample_bytree': 0.8538929689583808, 'gamma': 0.017435893202418992, 'min_child_weight': 4}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:37,744] Trial 60 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 205, 'max_depth': 10, 'learning_rate': 0.23115840409378857, 'subsample': 0.9361302694998641, 'colsample_bytree': 0.7948297533946587, 'gamma': 0.5760240234382561, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:38,561] Trial 61 finished with value: 0.8697368421052631 and parameters: {'n_estimators': 120, 'max_depth': 9, 'learning_rate': 0.13502151313081948, 'subsample': 0.8624615685291795, 'colsample_bytree': 0.9633970184214999, 'gamma': 1.0875174536324774, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:39,269] Trial 62 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 123, 'max_depth': 10, 'learning_rate': 0.1744279214530624, 'subsample': 0.9112761133541847, 'colsample_bytree': 0.9635584725556676, 'gamma': 0.6460463926847597, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:39,925] Trial 63 finished with value: 0.8571491228070176 and parameters: {'n_estimators': 120, 'max_depth': 10, 'learning_rate': 0.18075200939854638, 'subsample': 0.9654747726945787, 'colsample_bytree': 0.9645274151300604, 'gamma': 1.0455863933357548, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:40,696] Trial 64 finished with value: 0.8655701754385966 and parameters: {'n_estimators': 142, 'max_depth': 9, 'learning_rate': 0.21786999586042863, 'subsample': 0.9138003948121965, 'colsample_bytree': 0.9518891824563257, 'gamma': 0.6759191426055042, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:41,227] Trial 65 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 112, 'max_depth': 10, 'learning_rate': 0.1552615849625625, 'subsample': 0.9576559148593701, 'colsample_bytree': 0.9838729906638688, 'gamma': 2.8916510733694105, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:42,156] Trial 66 finished with value: 0.8655921052631579 and parameters: {'n_estimators': 169, 'max_depth': 9, 'learning_rate': 0.17285623024803148, 'subsample': 0.9320711326577981, 'colsample_bytree': 0.9737450742471978, 'gamma': 0.29964076993689526, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:42,864] Trial 67 finished with value: 0.8655043859649123 and parameters: {'n_estimators': 131, 'max_depth': 10, 'learning_rate': 0.1911957714257698, 'subsample': 0.8680668237986202, 'colsample_bytree': 0.9965102098768615, 'gamma': 0.8104340357005415, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:43,718] Trial 68 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 116, 'max_depth': 8, 'learning_rate': 0.15907624050037752, 'subsample': 0.8874138552028485, 'colsample_bytree': 0.9399023147779442, 'gamma': 1.709048861293888, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:44,400] Trial 69 finished with value: 0.8509210526315789 and parameters: {'n_estimators': 114, 'max_depth': 7, 'learning_rate': 0.1579515025803169, 'subsample': 0.9181195882895519, 'colsample_bytree': 0.7750587951259595, 'gamma': 1.736811298636407, 'min_child_weight': 3}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:45,230] Trial 70 finished with value: 0.8550219298245614 and parameters: {'n_estimators': 103, 'max_depth': 8, 'learning_rate': 0.16900485537333146, 'subsample': 0.8888021480993193, 'colsample_bytree': 0.9185750434283432, 'gamma': 1.964674694157213, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:45,810] Trial 71 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 120, 'max_depth': 8, 'learning_rate': 0.2038473983357572, 'subsample': 0.9441921514700057, 'colsample_bytree': 0.900335160048817, 'gamma': 1.4408921624658633, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:46,539] Trial 72 finished with value: 0.8717982456140352 and parameters: {'n_estimators': 141, 'max_depth': 9, 'learning_rate': 0.1812833496328357, 'subsample': 0.8952243565630738, 'colsample_bytree': 0.9485946945258672, 'gamma': 1.116423255763554, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:47,341] Trial 73 finished with value: 0.8613157894736843 and parameters: {'n_estimators': 137, 'max_depth': 8, 'learning_rate': 0.1839233202119287, 'subsample': 0.9040755416801171, 'colsample_bytree': 0.945584107236066, 'gamma': 1.3451765579385957, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:48,341] Trial 74 finished with value: 0.8635307017543858 and parameters: {'n_estimators': 247, 'max_depth': 10, 'learning_rate': 0.19496569162765776, 'subsample': 0.9277760975120468, 'colsample_bytree': 0.8139380825275201, 'gamma': 1.1060120630976773, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:49,070] Trial 75 finished with value: 0.857171052631579 and parameters: {'n_estimators': 131, 'max_depth': 9, 'learning_rate': 0.1792844424152732, 'subsample': 0.9773638221454899, 'colsample_bytree': 0.9359503575545776, 'gamma': 0.6585769046390553, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:49,884] Trial 76 finished with value: 0.8718421052631579 and parameters: {'n_estimators': 185, 'max_depth': 8, 'learning_rate': 0.15934980138995938, 'subsample': 0.8828908473021021, 'colsample_bytree': 0.9365168905108557, 'gamma': 0.7946325786464172, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:50,784] Trial 77 finished with value: 0.8634210526315791 and parameters: {'n_estimators': 194, 'max_depth': 8, 'learning_rate': 0.15035391840207793, 'subsample': 0.8802516217729743, 'colsample_bytree': 0.9341122852821044, 'gamma': 0.8754593923327761, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:51,883] Trial 78 finished with value: 0.8487280701754386 and parameters: {'n_estimators': 184, 'max_depth': 7, 'learning_rate': 0.16047789503537718, 'subsample': 0.7567138547783295, 'colsample_bytree': 0.9530162365904167, 'gamma': 2.16929406457346, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:52,892] Trial 79 finished with value: 0.8613377192982457 and parameters: {'n_estimators': 215, 'max_depth': 8, 'learning_rate': 0.14342787780989213, 'subsample': 0.8355471577916588, 'colsample_bytree': 0.9804589158469978, 'gamma': 1.5766462950369715, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:53,599] Trial 80 finished with value: 0.8320175438596491 and parameters: {'n_estimators': 157, 'max_depth': 7, 'learning_rate': 0.22468803022857803, 'subsample': 0.9522778320176393, 'colsample_bytree': 0.758685011198084, 'gamma': 4.261051550352281, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:54,377] Trial 81 finished with value: 0.8717982456140352 and parameters: {'n_estimators': 145, 'max_depth': 9, 'learning_rate': 0.1738559425391398, 'subsample': 0.8899825729706311, 'colsample_bytree': 0.917245210866915, 'gamma': 0.9525018423840226, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:55,236] Trial 82 finished with value: 0.8614254385964912 and parameters: {'n_estimators': 173, 'max_depth': 9, 'learning_rate': 0.17053753740836303, 'subsample': 0.8862405037179094, 'colsample_bytree': 0.9175116521200265, 'gamma': 0.9510889444420352, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:55,858] Trial 83 finished with value: 0.8550877192982457 and parameters: {'n_estimators': 146, 'max_depth': 8, 'learning_rate': 0.20818327341502146, 'subsample': 0.8963093010496168, 'colsample_bytree': 0.9429236407991396, 'gamma': 1.1235680113389415, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:56,641] Trial 84 finished with value: 0.8698245614035087 and parameters: {'n_estimators': 182, 'max_depth': 8, 'learning_rate': 0.15953244595161692, 'subsample': 0.9392985714365429, 'colsample_bytree': 0.7861283420703651, 'gamma': 0.772610019898432, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:57,350] Trial 85 finished with value: 0.8319298245614035 and parameters: {'n_estimators': 198, 'max_depth': 9, 'learning_rate': 0.19848166173699794, 'subsample': 0.9647952105555041, 'colsample_bytree': 0.7840615473104143, 'gamma': 0.8186228216226975, 'min_child_weight': 7}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:58,734] Trial 86 finished with value: 0.8740131578947368 and parameters: {'n_estimators': 180, 'max_depth': 9, 'learning_rate': 0.14301814038434763, 'subsample': 0.940761302746999, 'colsample_bytree': 0.7916620179372285, 'gamma': 0.7550337781253431, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:31:59,658] Trial 87 finished with value: 0.8697368421052631 and parameters: {'n_estimators': 167, 'max_depth': 8, 'learning_rate': 0.14253366643289006, 'subsample': 0.9395119662620358, 'colsample_bytree': 0.7345148313679727, 'gamma': 0.40462658550460373, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:31:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:00,564] Trial 88 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 183, 'max_depth': 9, 'learning_rate': 0.12628680480856241, 'subsample': 0.9853238929897717, 'colsample_bytree': 0.7957858101496941, 'gamma': 1.3027990954355648, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:01,191] Trial 89 finished with value: 0.8278070175438597 and parameters: {'n_estimators': 191, 'max_depth': 9, 'learning_rate': 0.152626072049147, 'subsample': 0.9708160188852314, 'colsample_bytree': 0.7494643697267814, 'gamma': 0.9987450724432999, 'min_child_weight': 8}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:02,092] Trial 90 finished with value: 0.8446271929824561 and parameters: {'n_estimators': 203, 'max_depth': 8, 'learning_rate': 0.1632117307801071, 'subsample': 0.6084735296791733, 'colsample_bytree': 0.7652739564928103, 'gamma': 0.7862081656587729, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:02,879] Trial 91 finished with value: 0.869780701754386 and parameters: {'n_estimators': 180, 'max_depth': 9, 'learning_rate': 0.17177139158597093, 'subsample': 0.9523616999323501, 'colsample_bytree': 0.7868805043428155, 'gamma': 0.6570782940503165, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:03,773] Trial 92 finished with value: 0.8655701754385966 and parameters: {'n_estimators': 184, 'max_depth': 9, 'learning_rate': 0.17815630629371745, 'subsample': 0.9533906395027519, 'colsample_bytree': 0.7865763104936815, 'gamma': 1.178476234088857, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:04,827] Trial 93 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 154, 'max_depth': 8, 'learning_rate': 0.1892922532173706, 'subsample': 0.9238892229928187, 'colsample_bytree': 0.825434697756135, 'gamma': 0.5455547431593273, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:05,675] Trial 94 finished with value: 0.8697368421052631 and parameters: {'n_estimators': 179, 'max_depth': 9, 'learning_rate': 0.12027326172035163, 'subsample': 0.9354912940895914, 'colsample_bytree': 0.8079060304443633, 'gamma': 0.7379178148482408, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:06,306] Trial 95 finished with value: 0.8592324561403508 and parameters: {'n_estimators': 162, 'max_depth': 9, 'learning_rate': 0.16610459830634516, 'subsample': 0.9599225103361447, 'colsample_bytree': 0.7776162226498615, 'gamma': 0.9085914810882255, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:07,541] Trial 96 finished with value: 0.8760307017543859 and parameters: {'n_estimators': 209, 'max_depth': 8, 'learning_rate': 0.12934025317879536, 'subsample': 0.9447281884481931, 'colsample_bytree': 0.7177482639314924, 'gamma': 0.34717277039170985, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:08,493] Trial 97 finished with value: 0.857171052631579 and parameters: {'n_estimators': 209, 'max_depth': 7, 'learning_rate': 0.1465206903037706, 'subsample': 0.9427506624911244, 'colsample_bytree': 0.6779350871897606, 'gamma': 0.36563879219648415, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:09,572] Trial 98 finished with value: 0.8676754385964912 and parameters: {'n_estimators': 171, 'max_depth': 8, 'learning_rate': 0.10628097307905067, 'subsample': 0.9800243529888603, 'colsample_bytree': 0.7150581635519215, 'gamma': 0.5106104731212894, 'min_child_weight': 1}. Best is trial 22 with value: 0.8760964912280702.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:32:10,397] Trial 99 finished with value: 0.8488157894736842 and parameters: {'n_estimators': 231, 'max_depth': 7, 'learning_rate': 0.14070708963023296, 'subsample': 0.928166099413663, 'colsample_bytree': 0.6869305218217027, 'gamma': 1.4403434396315058, 'min_child_weight': 2}. Best is trial 22 with value: 0.8760964912280702.
✅ Best XGBoost Parameters: {'n_estimators': 117, 'max_depth': 8, 'learning_rate': 0.16334865624436007, 'subsample': 0.9239199089668532, 'colsample_bytree': 0.763399102069695, 'gamma': 1.2878301473077314, 'min_child_weight': 1}
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:32:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
✅ Best model selected based on F1: RandomForest
01-05-2025 23:32:11 Retraining best model on original full dataset...
01-05-2025 23:32:11 ✅ Exported and logged final model to W&B as artifact: final_model_randomforest.pkl
Requirement already satisfied: wandb in c:\users\x-hp\anaconda3\lib\site-packages (0.19.8)
Requirement already satisfied: click!=8.0.0,>=7.1 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (8.1.7)
Requirement already satisfied: docker-pycreds>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (0.4.0)
Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.1.43)
Requirement already satisfied: platformdirs in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (3.10.0)
Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (4.25.3)
Requirement already satisfied: psutil>=5.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (5.9.0)
Requirement already satisfied: pydantic<3,>=2.6 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.8.2)
Requirement already satisfied: pyyaml in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (6.0.1)
Requirement already satisfied: requests<3,>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.32.3)
Requirement already satisfied: sentry-sdk>=2.0.0 in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (2.18.0)
Requirement already satisfied: setproctitle in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (1.3.5)
Requirement already satisfied: setuptools in c:\users\x-hp\anaconda3\lib\site-packages (from wandb) (75.1.0)
Requirement already satisfied: colorama in c:\users\x-hp\anaconda3\lib\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)
Requirement already satisfied: six>=1.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)
Requirement already satisfied: gitdb<5,>=4.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)
Requirement already satisfied: annotated-types>=0.4.0 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (0.6.0)
Requirement already satisfied: pydantic-core==2.20.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (2.20.1)
Requirement already satisfied: typing-extensions>=4.6.1 in c:\users\x-hp\anaconda3\lib\site-packages (from pydantic<3,>=2.6->wandb) (4.11.0)
Requirement already satisfied: charset-normalizer<4,>=2 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)
Requirement already satisfied: idna<4,>=2.5 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (3.7)
Requirement already satisfied: urllib3<3,>=1.21.1 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)
Requirement already satisfied: certifi>=2017.4.17 in c:\users\x-hp\anaconda3\lib\site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)
Requirement already satisfied: smmap<5,>=3.0.1 in c:\users\x-hp\anaconda3\lib\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)
[34m[1mwandb[0m: [33mWARNING[0m Calling wandb.login() after wandb.init() has no effect.
01-05-2025 23:36:10 Downloading and reading train artifact
[34m[1mwandb[0m:   1 of 1 files downloaded.
01-05-2025 23:36:12 Spliting data into train/val
01-05-2025 23:36:12 x train: (375, 28)
01-05-2025 23:36:12 y train: (375,)
01-05-2025 23:36:12 x val: (162, 28)
01-05-2025 23:36:12 y val: (162,)
01-05-2025 23:36:12 Outlier Removal
01-05-2025 23:36:12 x_train shape [original]: (375, 28)
01-05-2025 23:36:12 x_train shape [outlier removal]: (357, 28)
01-05-2025 23:36:13 Data after SMOTE: (476, 28)
01-05-2025 23:36:14 [Logistic] Accuracy: 0.7469135802469136
01-05-2025 23:36:15 [Random Forest] Accuracy: 0.7222222222222222
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:36:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
01-05-2025 23:36:17 [XGBoost] Accuracy: 0.7222222222222222
                 Model  Accuracy
0  Logistic Regression  0.746914
1        Random Forest  0.722222
2              XGBoost  0.722222
[I 2025-05-01 23:36:20,816] A new study created in memory with name: no-name-4dc1d3ff-4cce-4f6a-8210-b6a60cbbb96d
[I 2025-05-01 23:36:24,483] Trial 0 finished with value: 0.8382675438596492 and parameters: {'n_estimators': 73, 'max_depth': 6, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8382675438596492.
[I 2025-05-01 23:36:26,410] Trial 1 finished with value: 0.7962280701754387 and parameters: {'n_estimators': 63, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.8382675438596492.
[I 2025-05-01 23:36:28,453] Trial 2 finished with value: 0.7899122807017542 and parameters: {'n_estimators': 119, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8382675438596492.
[I 2025-05-01 23:36:32,460] Trial 3 finished with value: 0.7962061403508772 and parameters: {'n_estimators': 99, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 0.8382675438596492.
[I 2025-05-01 23:36:37,992] Trial 4 finished with value: 0.8550219298245614 and parameters: {'n_estimators': 155, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 4 with value: 0.8550219298245614.
[I 2025-05-01 23:36:41,476] Trial 5 finished with value: 0.8193421052631578 and parameters: {'n_estimators': 154, 'max_depth': 4, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 4 with value: 0.8550219298245614.
[I 2025-05-01 23:36:43,373] Trial 6 finished with value: 0.8550657894736842 and parameters: {'n_estimators': 94, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:36:44,453] Trial 7 finished with value: 0.8277412280701754 and parameters: {'n_estimators': 50, 'max_depth': 4, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:36:48,232] Trial 8 finished with value: 0.8403289473684209 and parameters: {'n_estimators': 173, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 5}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:36:51,841] Trial 9 finished with value: 0.7836184210526316 and parameters: {'n_estimators': 200, 'max_depth': 2, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:36:53,839] Trial 10 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 90, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:36:56,878] Trial 11 finished with value: 0.8529605263157893 and parameters: {'n_estimators': 137, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:36:59,087] Trial 12 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 117, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:37:04,442] Trial 13 finished with value: 0.8550657894736842 and parameters: {'n_estimators': 160, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:37:08,976] Trial 14 finished with value: 0.8424561403508772 and parameters: {'n_estimators': 184, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:37:12,200] Trial 15 finished with value: 0.8424780701754386 and parameters: {'n_estimators': 140, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:37:14,368] Trial 16 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 98, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:37:17,812] Trial 17 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 166, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 3}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:37:21,006] Trial 18 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 133, 'max_depth': 8, 'min_samples_split': 2, 'min_samples_leaf': 3}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:37:22,765] Trial 19 finished with value: 0.8445614035087718 and parameters: {'n_estimators': 81, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:37:25,108] Trial 20 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 112, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 6 with value: 0.8550657894736842.
[I 2025-05-01 23:37:27,339] Trial 21 finished with value: 0.8613377192982457 and parameters: {'n_estimators': 100, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:29,170] Trial 22 finished with value: 0.8613157894736843 and parameters: {'n_estimators': 104, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:31,111] Trial 23 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 98, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:33,144] Trial 24 finished with value: 0.8508333333333333 and parameters: {'n_estimators': 105, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:34,766] Trial 25 finished with value: 0.8424122807017543 and parameters: {'n_estimators': 84, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:36,099] Trial 26 finished with value: 0.8424342105263157 and parameters: {'n_estimators': 71, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:38,792] Trial 27 finished with value: 0.8466008771929825 and parameters: {'n_estimators': 129, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:40,610] Trial 28 finished with value: 0.8424999999999999 and parameters: {'n_estimators': 108, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:41,951] Trial 29 finished with value: 0.831951754385965 and parameters: {'n_estimators': 71, 'max_depth': 5, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:43,605] Trial 30 finished with value: 0.8445394736842105 and parameters: {'n_estimators': 90, 'max_depth': 9, 'min_samples_split': 3, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:45,730] Trial 31 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 123, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:47,837] Trial 32 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 121, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:49,987] Trial 33 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 120, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:52,107] Trial 34 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 122, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:54,484] Trial 35 finished with value: 0.8382456140350877 and parameters: {'n_estimators': 141, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:56,995] Trial 36 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 145, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:37:59,393] Trial 37 finished with value: 0.8487280701754385 and parameters: {'n_estimators': 146, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:02,218] Trial 38 finished with value: 0.8445614035087718 and parameters: {'n_estimators': 147, 'max_depth': 5, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:04,542] Trial 39 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 128, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:06,354] Trial 40 finished with value: 0.8571491228070174 and parameters: {'n_estimators': 105, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:08,573] Trial 41 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 117, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:11,128] Trial 42 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 128, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:13,052] Trial 43 finished with value: 0.8592324561403508 and parameters: {'n_estimators': 112, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:15,157] Trial 44 finished with value: 0.8550219298245614 and parameters: {'n_estimators': 111, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:16,238] Trial 45 finished with value: 0.8424780701754386 and parameters: {'n_estimators': 55, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:17,927] Trial 46 finished with value: 0.8466008771929824 and parameters: {'n_estimators': 87, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:19,683] Trial 47 finished with value: 0.8319298245614034 and parameters: {'n_estimators': 102, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 5}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:22,792] Trial 48 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 153, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:24,906] Trial 49 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 113, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:26,197] Trial 50 finished with value: 0.7815350877192981 and parameters: {'n_estimators': 80, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:28,442] Trial 51 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 124, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:30,189] Trial 52 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 93, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:33,046] Trial 53 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 135, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:34,951] Trial 54 finished with value: 0.7983114035087719 and parameters: {'n_estimators': 113, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:37,533] Trial 55 finished with value: 0.8445833333333332 and parameters: {'n_estimators': 118, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:39,290] Trial 56 finished with value: 0.8445833333333332 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:41,632] Trial 57 finished with value: 0.8550657894736842 and parameters: {'n_estimators': 133, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:44,998] Trial 58 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 185, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:47,208] Trial 59 finished with value: 0.8403508771929824 and parameters: {'n_estimators': 96, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:48,993] Trial 60 finished with value: 0.8424342105263157 and parameters: {'n_estimators': 107, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:51,061] Trial 61 finished with value: 0.8550877192982457 and parameters: {'n_estimators': 116, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:53,360] Trial 62 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 125, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:55,440] Trial 63 finished with value: 0.8550877192982457 and parameters: {'n_estimators': 118, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:38:58,574] Trial 64 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 143, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:01,725] Trial 65 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 146, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:04,199] Trial 66 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 140, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:07,074] Trial 67 finished with value: 0.8529605263157893 and parameters: {'n_estimators': 154, 'max_depth': 9, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:09,891] Trial 68 finished with value: 0.8487938596491228 and parameters: {'n_estimators': 164, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:12,273] Trial 69 finished with value: 0.8445833333333332 and parameters: {'n_estimators': 131, 'max_depth': 5, 'min_samples_split': 3, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:14,160] Trial 70 finished with value: 0.8529166666666667 and parameters: {'n_estimators': 104, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:16,133] Trial 71 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 110, 'max_depth': 10, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:18,266] Trial 72 finished with value: 0.8529385964912282 and parameters: {'n_estimators': 123, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:20,682] Trial 73 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 137, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:22,614] Trial 74 finished with value: 0.8571271929824562 and parameters: {'n_estimators': 115, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:25,239] Trial 75 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 149, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:27,889] Trial 76 finished with value: 0.8487938596491228 and parameters: {'n_estimators': 158, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:30,545] Trial 77 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 142, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:32,672] Trial 78 finished with value: 0.8466447368421053 and parameters: {'n_estimators': 126, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:34,775] Trial 79 finished with value: 0.8487500000000001 and parameters: {'n_estimators': 121, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:36,658] Trial 80 finished with value: 0.8592324561403508 and parameters: {'n_estimators': 92, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:38,238] Trial 81 finished with value: 0.8592324561403508 and parameters: {'n_estimators': 90, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:39,909] Trial 82 finished with value: 0.8571271929824562 and parameters: {'n_estimators': 91, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 21 with value: 0.8613377192982457.
[I 2025-05-01 23:39:41,213] Trial 83 finished with value: 0.8634429824561403 and parameters: {'n_estimators': 76, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:42,573] Trial 84 finished with value: 0.8487719298245613 and parameters: {'n_estimators': 77, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:44,052] Trial 85 finished with value: 0.8613377192982457 and parameters: {'n_estimators': 87, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:45,287] Trial 86 finished with value: 0.8445833333333332 and parameters: {'n_estimators': 68, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:46,857] Trial 87 finished with value: 0.8613377192982457 and parameters: {'n_estimators': 86, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:48,311] Trial 88 finished with value: 0.8424561403508772 and parameters: {'n_estimators': 84, 'max_depth': 9, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:49,709] Trial 89 finished with value: 0.8424122807017543 and parameters: {'n_estimators': 63, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:51,011] Trial 90 finished with value: 0.8571491228070174 and parameters: {'n_estimators': 74, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:52,563] Trial 91 finished with value: 0.8613377192982457 and parameters: {'n_estimators': 87, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:54,127] Trial 92 finished with value: 0.8613377192982457 and parameters: {'n_estimators': 87, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:55,938] Trial 93 finished with value: 0.8424342105263157 and parameters: {'n_estimators': 87, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 4}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:57,434] Trial 94 finished with value: 0.861359649122807 and parameters: {'n_estimators': 82, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:39:58,865] Trial 95 finished with value: 0.8613596491228069 and parameters: {'n_estimators': 81, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:40:00,453] Trial 96 finished with value: 0.8613596491228069 and parameters: {'n_estimators': 81, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:40:02,086] Trial 97 finished with value: 0.8487500000000001 and parameters: {'n_estimators': 79, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 83 with value: 0.8634429824561403.
[I 2025-05-01 23:40:03,806] Trial 98 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 83, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 98 with value: 0.8634649122807018.
[I 2025-05-01 23:40:05,607] Trial 99 finished with value: 0.8487280701754386 and parameters: {'n_estimators': 85, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 98 with value: 0.8634649122807018.
✅ Best Random Forest Parameters: {'n_estimators': 83, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}
[I 2025-05-01 23:40:05,608] A new study created in memory with name: no-name-c844028f-5021-4b08-9ae7-e416ac025175
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:06,267] Trial 0 finished with value: 0.8361184210526315 and parameters: {'n_estimators': 117, 'max_depth': 8, 'learning_rate': 0.141781349338635, 'subsample': 0.7564905103397797, 'colsample_bytree': 0.8276723890747341, 'gamma': 3.798656373873024, 'min_child_weight': 6}. Best is trial 0 with value: 0.8361184210526315.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:07,587] Trial 1 finished with value: 0.8340570175438596 and parameters: {'n_estimators': 240, 'max_depth': 10, 'learning_rate': 0.0281561469675563, 'subsample': 0.6837665753607125, 'colsample_bytree': 0.6632564102842993, 'gamma': 1.3595903061752024, 'min_child_weight': 6}. Best is trial 0 with value: 0.8361184210526315.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:08,572] Trial 2 finished with value: 0.8340789473684211 and parameters: {'n_estimators': 244, 'max_depth': 6, 'learning_rate': 0.09707034924965846, 'subsample': 0.9551972272587449, 'colsample_bytree': 0.6575002542402707, 'gamma': 1.1996068926574717, 'min_child_weight': 8}. Best is trial 0 with value: 0.8361184210526315.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:09,123] Trial 3 finished with value: 0.8361842105263158 and parameters: {'n_estimators': 116, 'max_depth': 3, 'learning_rate': 0.2679873592481082, 'subsample': 0.773705457405979, 'colsample_bytree': 0.83686968544256, 'gamma': 2.2878257701740017, 'min_child_weight': 5}. Best is trial 3 with value: 0.8361842105263158.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:10,115] Trial 4 finished with value: 0.8319956140350877 and parameters: {'n_estimators': 261, 'max_depth': 9, 'learning_rate': 0.058303669622694426, 'subsample': 0.9755442187359542, 'colsample_bytree': 0.9353137016645482, 'gamma': 1.93916151117293, 'min_child_weight': 8}. Best is trial 3 with value: 0.8361842105263158.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:10,814] Trial 5 finished with value: 0.8257017543859648 and parameters: {'n_estimators': 158, 'max_depth': 7, 'learning_rate': 0.12534327373598597, 'subsample': 0.9490123770427678, 'colsample_bytree': 0.7675148265845794, 'gamma': 0.9609059253164154, 'min_child_weight': 8}. Best is trial 3 with value: 0.8361842105263158.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:11,463] Trial 6 finished with value: 0.8445614035087718 and parameters: {'n_estimators': 137, 'max_depth': 9, 'learning_rate': 0.09880786612862293, 'subsample': 0.7358355259723147, 'colsample_bytree': 0.8722424613054198, 'gamma': 3.428084331152788, 'min_child_weight': 3}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:13,050] Trial 7 finished with value: 0.8193859649122807 and parameters: {'n_estimators': 253, 'max_depth': 9, 'learning_rate': 0.15995694047146897, 'subsample': 0.6513997383773612, 'colsample_bytree': 0.8378689789594289, 'gamma': 0.2244230914906753, 'min_child_weight': 4}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:13,924] Trial 8 finished with value: 0.8277850877192983 and parameters: {'n_estimators': 114, 'max_depth': 10, 'learning_rate': 0.0655274076977052, 'subsample': 0.9578724910396911, 'colsample_bytree': 0.7414116429833022, 'gamma': 0.47708566493886106, 'min_child_weight': 7}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:14,845] Trial 9 finished with value: 0.8194078947368422 and parameters: {'n_estimators': 167, 'max_depth': 10, 'learning_rate': 0.273565079549175, 'subsample': 0.6309833852371997, 'colsample_bytree': 0.8508572468988718, 'gamma': 1.0693377690251604, 'min_child_weight': 5}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:15,649] Trial 10 finished with value: 0.8361622807017544 and parameters: {'n_estimators': 195, 'max_depth': 5, 'learning_rate': 0.20043607236012315, 'subsample': 0.861642950998063, 'colsample_bytree': 0.9851104912439217, 'gamma': 4.887913482806245, 'min_child_weight': 1}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:16,494] Trial 11 finished with value: 0.8362280701754387 and parameters: {'n_estimators': 144, 'max_depth': 3, 'learning_rate': 0.26809192885114475, 'subsample': 0.7592122569821838, 'colsample_bytree': 0.9047229732989306, 'gamma': 3.163457118753792, 'min_child_weight': 3}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:17,131] Trial 12 finished with value: 0.8424342105263157 and parameters: {'n_estimators': 153, 'max_depth': 4, 'learning_rate': 0.2062540791287934, 'subsample': 0.717366389679928, 'colsample_bytree': 0.9117724323701912, 'gamma': 3.2794962200898383, 'min_child_weight': 2}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:18,257] Trial 13 finished with value: 0.8277192982456139 and parameters: {'n_estimators': 197, 'max_depth': 5, 'learning_rate': 0.20199198788406375, 'subsample': 0.6978411436620092, 'colsample_bytree': 0.897064348540169, 'gamma': 3.486500902469317, 'min_child_weight': 1}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:18,961] Trial 14 finished with value: 0.8319298245614035 and parameters: {'n_estimators': 148, 'max_depth': 4, 'learning_rate': 0.2094861998020991, 'subsample': 0.8363995585305862, 'colsample_bytree': 0.9853483417827364, 'gamma': 4.326458403257886, 'min_child_weight': 3}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:19,885] Trial 15 finished with value: 0.8067763157894736 and parameters: {'n_estimators': 297, 'max_depth': 7, 'learning_rate': 0.17855162565036092, 'subsample': 0.7137022428347995, 'colsample_bytree': 0.914326232014415, 'gamma': 2.8607642966436644, 'min_child_weight': 10}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:20,626] Trial 16 finished with value: 0.8319298245614035 and parameters: {'n_estimators': 175, 'max_depth': 6, 'learning_rate': 0.11127127642151557, 'subsample': 0.8248253976404485, 'colsample_bytree': 0.7278869512844974, 'gamma': 3.9747922130667974, 'min_child_weight': 2}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:21,251] Trial 17 finished with value: 0.825701754385965 and parameters: {'n_estimators': 138, 'max_depth': 8, 'learning_rate': 0.23924794323088752, 'subsample': 0.6068010386744334, 'colsample_bytree': 0.601647905621697, 'gamma': 2.6897905751620073, 'min_child_weight': 3}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:22,518] Trial 18 finished with value: 0.8298684210526315 and parameters: {'n_estimators': 103, 'max_depth': 4, 'learning_rate': 0.08595536737421688, 'subsample': 0.8741509794611294, 'colsample_bytree': 0.944962346466558, 'gamma': 4.721021629328046, 'min_child_weight': 2}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:23,488] Trial 19 finished with value: 0.8298684210526316 and parameters: {'n_estimators': 182, 'max_depth': 5, 'learning_rate': 0.29958591686139663, 'subsample': 0.7093034570587237, 'colsample_bytree': 0.7935428991674465, 'gamma': 2.143550212653118, 'min_child_weight': 4}. Best is trial 6 with value: 0.8445614035087718.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:24,653] Trial 20 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 218, 'max_depth': 8, 'learning_rate': 0.025533317355579788, 'subsample': 0.7965138584666798, 'colsample_bytree': 0.8715632785975445, 'gamma': 3.266564380849029, 'min_child_weight': 2}. Best is trial 20 with value: 0.8466666666666667.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:26,669] Trial 21 finished with value: 0.846688596491228 and parameters: {'n_estimators': 221, 'max_depth': 8, 'learning_rate': 0.01577038985479701, 'subsample': 0.7881232917861279, 'colsample_bytree': 0.8625184060267292, 'gamma': 3.36235607778806, 'min_child_weight': 2}. Best is trial 21 with value: 0.846688596491228.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:27,875] Trial 22 finished with value: 0.8529166666666667 and parameters: {'n_estimators': 224, 'max_depth': 8, 'learning_rate': 0.02417963687278346, 'subsample': 0.7902929050159565, 'colsample_bytree': 0.8710385799089797, 'gamma': 3.6676577421525876, 'min_child_weight': 1}. Best is trial 22 with value: 0.8529166666666667.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:29,405] Trial 23 finished with value: 0.8424342105263157 and parameters: {'n_estimators': 224, 'max_depth': 8, 'learning_rate': 0.014397460472979325, 'subsample': 0.8093318843748002, 'colsample_bytree': 0.8674848031597586, 'gamma': 4.175222882582386, 'min_child_weight': 1}. Best is trial 22 with value: 0.8529166666666667.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:30,503] Trial 24 finished with value: 0.8466447368421053 and parameters: {'n_estimators': 218, 'max_depth': 8, 'learning_rate': 0.04176246979246844, 'subsample': 0.8980547939086029, 'colsample_bytree': 0.7997662440124413, 'gamma': 2.8539526808875344, 'min_child_weight': 1}. Best is trial 22 with value: 0.8529166666666667.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:31,912] Trial 25 finished with value: 0.8361842105263158 and parameters: {'n_estimators': 216, 'max_depth': 7, 'learning_rate': 0.015948649925264166, 'subsample': 0.788377863766536, 'colsample_bytree': 0.8736653815051876, 'gamma': 3.689775825546044, 'min_child_weight': 2}. Best is trial 22 with value: 0.8529166666666667.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:32,999] Trial 26 finished with value: 0.8277412280701754 and parameters: {'n_estimators': 279, 'max_depth': 9, 'learning_rate': 0.048994773649213716, 'subsample': 0.9168101947811474, 'colsample_bytree': 0.957541441983012, 'gamma': 4.484928588753906, 'min_child_weight': 4}. Best is trial 22 with value: 0.8529166666666667.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:34,212] Trial 27 finished with value: 0.8403508771929825 and parameters: {'n_estimators': 209, 'max_depth': 8, 'learning_rate': 0.07450038437706381, 'subsample': 0.7978834724103132, 'colsample_bytree': 0.8148640104436136, 'gamma': 3.0022605668838844, 'min_child_weight': 2}. Best is trial 22 with value: 0.8529166666666667.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:35,882] Trial 28 finished with value: 0.8529824561403508 and parameters: {'n_estimators': 228, 'max_depth': 7, 'learning_rate': 0.033336138249255146, 'subsample': 0.8510450825875701, 'colsample_bytree': 0.7717113680238774, 'gamma': 1.8046694434690473, 'min_child_weight': 1}. Best is trial 28 with value: 0.8529824561403508.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:37,235] Trial 29 finished with value: 0.8613815789473683 and parameters: {'n_estimators': 229, 'max_depth': 7, 'learning_rate': 0.0414339715595412, 'subsample': 0.8396949830469648, 'colsample_bytree': 0.7636256923301286, 'gamma': 1.5494242777546705, 'min_child_weight': 1}. Best is trial 29 with value: 0.8613815789473683.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:38,221] Trial 30 finished with value: 0.8466008771929824 and parameters: {'n_estimators': 232, 'max_depth': 6, 'learning_rate': 0.13444428758073187, 'subsample': 0.8472934273531735, 'colsample_bytree': 0.7165177976076594, 'gamma': 1.6072423483066665, 'min_child_weight': 1}. Best is trial 29 with value: 0.8613815789473683.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:39,626] Trial 31 finished with value: 0.8613815789473683 and parameters: {'n_estimators': 267, 'max_depth': 7, 'learning_rate': 0.04062093284421958, 'subsample': 0.872485082485032, 'colsample_bytree': 0.7419802381398383, 'gamma': 1.6685712773393013, 'min_child_weight': 1}. Best is trial 29 with value: 0.8613815789473683.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:41,128] Trial 32 finished with value: 0.8571491228070176 and parameters: {'n_estimators': 272, 'max_depth': 7, 'learning_rate': 0.04016826695228869, 'subsample': 0.8929486051449196, 'colsample_bytree': 0.690535358847615, 'gamma': 1.6468809340017763, 'min_child_weight': 1}. Best is trial 29 with value: 0.8613815789473683.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:43,032] Trial 33 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 269, 'max_depth': 7, 'learning_rate': 0.04184674843654311, 'subsample': 0.893990826855748, 'colsample_bytree': 0.7004515902141796, 'gamma': 1.5727281894375404, 'min_child_weight': 1}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:44,076] Trial 34 finished with value: 0.8319517543859648 and parameters: {'n_estimators': 274, 'max_depth': 6, 'learning_rate': 0.07118616494882245, 'subsample': 0.8950967392727174, 'colsample_bytree': 0.6885887297444329, 'gamma': 1.411096870702663, 'min_child_weight': 10}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:45,244] Trial 35 finished with value: 0.8445833333333332 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.04365397187070251, 'subsample': 0.9239814990132029, 'colsample_bytree': 0.6552227273359996, 'gamma': 2.404970007408972, 'min_child_weight': 3}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:46,736] Trial 36 finished with value: 0.8235745614035089 and parameters: {'n_estimators': 269, 'max_depth': 6, 'learning_rate': 0.05231847054382933, 'subsample': 0.8867141243823117, 'colsample_bytree': 0.6905570231952208, 'gamma': 0.7402999985627878, 'min_child_weight': 6}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:47,949] Trial 37 finished with value: 0.8550438596491228 and parameters: {'n_estimators': 248, 'max_depth': 7, 'learning_rate': 0.08900313781423363, 'subsample': 0.9262663863280417, 'colsample_bytree': 0.6344907097605166, 'gamma': 1.4432487182482652, 'min_child_weight': 1}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:48,991] Trial 38 finished with value: 0.8277412280701755 and parameters: {'n_estimators': 286, 'max_depth': 6, 'learning_rate': 0.11253835085219474, 'subsample': 0.8708175556670718, 'colsample_bytree': 0.7516897590311624, 'gamma': 1.8927571862746082, 'min_child_weight': 9}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:51,202] Trial 39 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 260, 'max_depth': 7, 'learning_rate': 0.06100862693107778, 'subsample': 0.9899981406602664, 'colsample_bytree': 0.6997851626870688, 'gamma': 0.7931472933736334, 'min_child_weight': 2}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:52,456] Trial 40 finished with value: 0.827763157894737 and parameters: {'n_estimators': 258, 'max_depth': 7, 'learning_rate': 0.06663025079250393, 'subsample': 0.9805870033805759, 'colsample_bytree': 0.7133360391118291, 'gamma': 0.6698761465604617, 'min_child_weight': 5}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:54,055] Trial 41 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 239, 'max_depth': 7, 'learning_rate': 0.043052775419415136, 'subsample': 0.9508378403187498, 'colsample_bytree': 0.684106964950268, 'gamma': 1.1855713836098007, 'min_child_weight': 1}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:55,823] Trial 42 finished with value: 0.8593201754385964 and parameters: {'n_estimators': 239, 'max_depth': 6, 'learning_rate': 0.05871702631991643, 'subsample': 0.9964751993326695, 'colsample_bytree': 0.6729148211864403, 'gamma': 0.024742260862611776, 'min_child_weight': 2}. Best is trial 33 with value: 0.8634649122807018.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:57,664] Trial 43 finished with value: 0.8676535087719298 and parameters: {'n_estimators': 238, 'max_depth': 6, 'learning_rate': 0.08140753515728112, 'subsample': 0.9634698656612128, 'colsample_bytree': 0.6640801123943433, 'gamma': 0.10368591625617196, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:40:58,688] Trial 44 finished with value: 0.8256578947368421 and parameters: {'n_estimators': 240, 'max_depth': 5, 'learning_rate': 0.08277305833468326, 'subsample': 0.9625794736968502, 'colsample_bytree': 0.6418544915020166, 'gamma': 1.2308792147990681, 'min_child_weight': 7}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:40:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:00,052] Trial 45 finished with value: 0.8592763157894737 and parameters: {'n_estimators': 252, 'max_depth': 6, 'learning_rate': 0.10708089363343443, 'subsample': 0.955327273376124, 'colsample_bytree': 0.7670382491311889, 'gamma': 2.1125589360109274, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:02,477] Trial 46 finished with value: 0.8466666666666667 and parameters: {'n_estimators': 286, 'max_depth': 9, 'learning_rate': 0.03893786191914955, 'subsample': 0.9392290878441871, 'colsample_bytree': 0.7393474013845736, 'gamma': 0.30878278707711637, 'min_child_weight': 3}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:03,861] Trial 47 finished with value: 0.8403947368421054 and parameters: {'n_estimators': 205, 'max_depth': 7, 'learning_rate': 0.030540716453627832, 'subsample': 0.9129367544808534, 'colsample_bytree': 0.6134879723033398, 'gamma': 0.9357357299345253, 'min_child_weight': 4}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:04,960] Trial 48 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 236, 'max_depth': 5, 'learning_rate': 0.0742700372077848, 'subsample': 0.9380316844055498, 'colsample_bytree': 0.6694120578852073, 'gamma': 1.1822147887285939, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:06,409] Trial 49 finished with value: 0.8362061403508771 and parameters: {'n_estimators': 265, 'max_depth': 6, 'learning_rate': 0.15903846399436564, 'subsample': 0.8297279610158053, 'colsample_bytree': 0.7840839755710393, 'gamma': 0.5361235907335166, 'min_child_weight': 3}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:07,639] Trial 50 finished with value: 0.8571929824561403 and parameters: {'n_estimators': 246, 'max_depth': 7, 'learning_rate': 0.09908667341365536, 'subsample': 0.9742261027446042, 'colsample_bytree': 0.7523004860580981, 'gamma': 1.6075457356765073, 'min_child_weight': 2}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:10,015] Trial 51 finished with value: 0.8592105263157895 and parameters: {'n_estimators': 241, 'max_depth': 6, 'learning_rate': 0.05915329185467213, 'subsample': 0.9705748691103797, 'colsample_bytree': 0.6723945825118447, 'gamma': 0.12471918946857041, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:11,956] Trial 52 finished with value: 0.8530263157894737 and parameters: {'n_estimators': 256, 'max_depth': 6, 'learning_rate': 0.05358597023518075, 'subsample': 0.9840785006708318, 'colsample_bytree': 0.7061711637825029, 'gamma': 0.020933181911615986, 'min_child_weight': 2}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:14,662] Trial 53 finished with value: 0.8613815789473683 and parameters: {'n_estimators': 233, 'max_depth': 7, 'learning_rate': 0.010389575229780873, 'subsample': 0.9929647160777677, 'colsample_bytree': 0.729335985678042, 'gamma': 0.37076580589665165, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:17,029] Trial 54 finished with value: 0.8592324561403508 and parameters: {'n_estimators': 194, 'max_depth': 7, 'learning_rate': 0.011146416368422618, 'subsample': 0.9084148498907372, 'colsample_bytree': 0.6509009200813564, 'gamma': 0.9851490092829334, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:19,343] Trial 55 finished with value: 0.8634649122807018 and parameters: {'n_estimators': 232, 'max_depth': 7, 'learning_rate': 0.025588314816709992, 'subsample': 0.9437875635427149, 'colsample_bytree': 0.734237514629999, 'gamma': 0.37677004500554734, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:21,807] Trial 56 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 286, 'max_depth': 8, 'learning_rate': 0.024090014288485957, 'subsample': 0.9423214775758731, 'colsample_bytree': 0.7258075285674069, 'gamma': 0.5173663372373198, 'min_child_weight': 2}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:23,268] Trial 57 finished with value: 0.8445614035087718 and parameters: {'n_estimators': 208, 'max_depth': 7, 'learning_rate': 0.03577398985249214, 'subsample': 0.878647207262654, 'colsample_bytree': 0.6277404242585637, 'gamma': 2.0500355768613057, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:24,141] Trial 58 finished with value: 0.8571052631578947 and parameters: {'n_estimators': 248, 'max_depth': 8, 'learning_rate': 0.17501907700075903, 'subsample': 0.8524555605497974, 'colsample_bytree': 0.7550908552019835, 'gamma': 1.1530459421959496, 'min_child_weight': 2}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:25,504] Trial 59 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 264, 'max_depth': 5, 'learning_rate': 0.028069075313260555, 'subsample': 0.818365419874122, 'colsample_bytree': 0.8187828239152132, 'gamma': 2.5252083122261184, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:26,729] Trial 60 finished with value: 0.8571271929824562 and parameters: {'n_estimators': 185, 'max_depth': 9, 'learning_rate': 0.048442584740369006, 'subsample': 0.7652894506920507, 'colsample_bytree': 0.7771821383522803, 'gamma': 1.3307978926206934, 'min_child_weight': 2}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:28,899] Trial 61 finished with value: 0.8655921052631579 and parameters: {'n_estimators': 233, 'max_depth': 7, 'learning_rate': 0.024914284937229553, 'subsample': 0.9541121653943269, 'colsample_bytree': 0.7369793758045317, 'gamma': 0.31839934007888737, 'min_child_weight': 1}. Best is trial 43 with value: 0.8676535087719298.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:30,844] Trial 62 finished with value: 0.8676973684210525 and parameters: {'n_estimators': 229, 'max_depth': 7, 'learning_rate': 0.02097998420184006, 'subsample': 0.9547148965445703, 'colsample_bytree': 0.7399869583751515, 'gamma': 0.4261060858563683, 'min_child_weight': 1}. Best is trial 62 with value: 0.8676973684210525.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:33,106] Trial 63 finished with value: 0.8655921052631579 and parameters: {'n_estimators': 227, 'max_depth': 7, 'learning_rate': 0.021122908920478606, 'subsample': 0.9549639099224954, 'colsample_bytree': 0.6817423844237841, 'gamma': 0.2320754861854466, 'min_child_weight': 1}. Best is trial 62 with value: 0.8676973684210525.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:35,016] Trial 64 finished with value: 0.8551315789473684 and parameters: {'n_estimators': 213, 'max_depth': 8, 'learning_rate': 0.019090776967432124, 'subsample': 0.9529640979925512, 'colsample_bytree': 0.6821628410124223, 'gamma': 0.36365788459996873, 'min_child_weight': 3}. Best is trial 62 with value: 0.8676973684210525.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:36,650] Trial 65 finished with value: 0.8550877192982457 and parameters: {'n_estimators': 203, 'max_depth': 7, 'learning_rate': 0.026334030764598317, 'subsample': 0.9672446021295058, 'colsample_bytree': 0.7168298596464093, 'gamma': 0.23693735012888484, 'min_child_weight': 2}. Best is trial 62 with value: 0.8676973684210525.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:38,562] Trial 66 finished with value: 0.8655921052631579 and parameters: {'n_estimators': 224, 'max_depth': 8, 'learning_rate': 0.02088915708498188, 'subsample': 0.9298003974789233, 'colsample_bytree': 0.7026616374972338, 'gamma': 0.5653963328346907, 'min_child_weight': 1}. Best is trial 62 with value: 0.8676973684210525.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:40,257] Trial 67 finished with value: 0.8634868421052632 and parameters: {'n_estimators': 220, 'max_depth': 8, 'learning_rate': 0.0325163285003854, 'subsample': 0.9296654388523269, 'colsample_bytree': 0.6994854800238981, 'gamma': 0.5951004877526984, 'min_child_weight': 1}. Best is trial 62 with value: 0.8676973684210525.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:41,782] Trial 68 finished with value: 0.8572149122807018 and parameters: {'n_estimators': 221, 'max_depth': 10, 'learning_rate': 0.032878417874415776, 'subsample': 0.927303940699092, 'colsample_bytree': 0.7018728956587219, 'gamma': 0.6107815450154654, 'min_child_weight': 2}. Best is trial 62 with value: 0.8676973684210525.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:43,513] Trial 69 finished with value: 0.8697587719298246 and parameters: {'n_estimators': 226, 'max_depth': 8, 'learning_rate': 0.020110071638849703, 'subsample': 0.9366060165372285, 'colsample_bytree': 0.666142372860967, 'gamma': 0.8717849507680567, 'min_child_weight': 1}. Best is trial 69 with value: 0.8697587719298246.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:45,753] Trial 70 finished with value: 0.869780701754386 and parameters: {'n_estimators': 214, 'max_depth': 9, 'learning_rate': 0.018487411543262375, 'subsample': 0.9323216602840063, 'colsample_bytree': 0.6570586281650869, 'gamma': 0.8613461836627363, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:47,565] Trial 71 finished with value: 0.8655701754385966 and parameters: {'n_estimators': 225, 'max_depth': 9, 'learning_rate': 0.019490125702658845, 'subsample': 0.933465344287059, 'colsample_bytree': 0.6625275006374621, 'gamma': 0.8625380092249131, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:49,519] Trial 72 finished with value: 0.8613596491228069 and parameters: {'n_estimators': 226, 'max_depth': 9, 'learning_rate': 0.020062632286054483, 'subsample': 0.9626799036697491, 'colsample_bytree': 0.6572801835104923, 'gamma': 0.8390958765524859, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:51,271] Trial 73 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 214, 'max_depth': 9, 'learning_rate': 0.010304463766306752, 'subsample': 0.9049396157390212, 'colsample_bytree': 0.616812627575256, 'gamma': 0.23379624600359117, 'min_child_weight': 2}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:53,262] Trial 74 finished with value: 0.8655921052631579 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.019082017665805227, 'subsample': 0.9163621014935158, 'colsample_bytree': 0.6482251497702106, 'gamma': 0.4486730935345358, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:55,709] Trial 75 finished with value: 0.8550877192982457 and parameters: {'n_estimators': 196, 'max_depth': 9, 'learning_rate': 0.01866201683904392, 'subsample': 0.9162962293292093, 'colsample_bytree': 0.64587532871695, 'gamma': 0.13245009973890648, 'min_child_weight': 2}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:57,762] Trial 76 finished with value: 0.8655701754385964 and parameters: {'n_estimators': 211, 'max_depth': 10, 'learning_rate': 0.23360214079428215, 'subsample': 0.9755653409557863, 'colsample_bytree': 0.6299840072260953, 'gamma': 0.47171654942622804, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:41:58,833] Trial 77 finished with value: 0.8088815789473685 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.14151974555560076, 'subsample': 0.6762934297939847, 'colsample_bytree': 0.6742839256934864, 'gamma': 1.0353188470731065, 'min_child_weight': 7}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:41:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:03,245] Trial 78 finished with value: 0.857171052631579 and parameters: {'n_estimators': 191, 'max_depth': 10, 'learning_rate': 0.05106794228324986, 'subsample': 0.9570827550371909, 'colsample_bytree': 0.6052258212960745, 'gamma': 0.7066707112238035, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:05,629] Trial 79 finished with value: 0.8550877192982455 and parameters: {'n_estimators': 230, 'max_depth': 8, 'learning_rate': 0.12115850600783003, 'subsample': 0.91984727495944, 'colsample_bytree': 0.6405640581806046, 'gamma': 0.15916425195818218, 'min_child_weight': 2}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:08,626] Trial 80 finished with value: 0.8697807017543859 and parameters: {'n_estimators': 173, 'max_depth': 10, 'learning_rate': 0.03002815870317384, 'subsample': 0.9498186165987929, 'colsample_bytree': 0.659983413422197, 'gamma': 0.4363339006918521, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:11,713] Trial 81 finished with value: 0.8634868421052632 and parameters: {'n_estimators': 126, 'max_depth': 10, 'learning_rate': 0.032879638885078506, 'subsample': 0.9469331292567732, 'colsample_bytree': 0.6794980269386633, 'gamma': 0.4408100537533762, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:14,718] Trial 82 finished with value: 0.8613815789473683 and parameters: {'n_estimators': 162, 'max_depth': 10, 'learning_rate': 0.017655346926369365, 'subsample': 0.9338332346961687, 'colsample_bytree': 0.6627165691192735, 'gamma': 0.669072708773421, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:17,214] Trial 83 finished with value: 0.8571710526315789 and parameters: {'n_estimators': 180, 'max_depth': 9, 'learning_rate': 0.04801594968860028, 'subsample': 0.9647227531015967, 'colsample_bytree': 0.6208640164483391, 'gamma': 0.27594328894002923, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:19,319] Trial 84 finished with value: 0.857171052631579 and parameters: {'n_estimators': 217, 'max_depth': 10, 'learning_rate': 0.025684475050864724, 'subsample': 0.9045269946952874, 'colsample_bytree': 0.6512569075560021, 'gamma': 0.017282660965155516, 'min_child_weight': 2}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:20,902] Trial 85 finished with value: 0.8655701754385966 and parameters: {'n_estimators': 166, 'max_depth': 8, 'learning_rate': 0.03582834593083253, 'subsample': 0.9857495764847075, 'colsample_bytree': 0.6936195059855834, 'gamma': 0.5227163543917098, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:24,660] Trial 86 finished with value: 0.8592543859649122 and parameters: {'n_estimators': 188, 'max_depth': 9, 'learning_rate': 0.010720377176432877, 'subsample': 0.950370431286284, 'colsample_bytree': 0.7145326125649912, 'gamma': 0.7966763822512317, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:25,817] Trial 87 finished with value: 0.8508552631578947 and parameters: {'n_estimators': 154, 'max_depth': 3, 'learning_rate': 0.06525870886436463, 'subsample': 0.9758007677042787, 'colsample_bytree': 0.6375283233800813, 'gamma': 0.12732376317243121, 'min_child_weight': 2}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:27,718] Trial 88 finished with value: 0.8697807017543859 and parameters: {'n_estimators': 177, 'max_depth': 10, 'learning_rate': 0.021538615689649567, 'subsample': 0.9980199302557838, 'colsample_bytree': 0.6680187673983093, 'gamma': 0.921403967506613, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:28,803] Trial 89 finished with value: 0.8508771929824561 and parameters: {'n_estimators': 176, 'max_depth': 8, 'learning_rate': 0.08204028810467813, 'subsample': 0.7480071218970787, 'colsample_bytree': 0.6679807443410116, 'gamma': 0.9974078214423222, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:30,393] Trial 90 finished with value: 0.8655482456140351 and parameters: {'n_estimators': 235, 'max_depth': 9, 'learning_rate': 0.04484921586948645, 'subsample': 0.9964545882072973, 'colsample_bytree': 0.6810126736614192, 'gamma': 0.9127330972114036, 'min_child_weight': 1}. Best is trial 70 with value: 0.869780701754386.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:32,642] Trial 91 finished with value: 0.8718640350877193 and parameters: {'n_estimators': 207, 'max_depth': 10, 'learning_rate': 0.01984393003584142, 'subsample': 0.9993738633955225, 'colsample_bytree': 0.658973270280266, 'gamma': 0.4136771466451134, 'min_child_weight': 1}. Best is trial 91 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:34,897] Trial 92 finished with value: 0.8676754385964912 and parameters: {'n_estimators': 244, 'max_depth': 10, 'learning_rate': 0.028994748062939165, 'subsample': 0.9858624494047927, 'colsample_bytree': 0.663122342675414, 'gamma': 0.7223431933060664, 'min_child_weight': 1}. Best is trial 91 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:36,188] Trial 93 finished with value: 0.8655921052631579 and parameters: {'n_estimators': 252, 'max_depth': 10, 'learning_rate': 0.055329753355921385, 'subsample': 0.9850865171364663, 'colsample_bytree': 0.6562444471366597, 'gamma': 0.7323223934758003, 'min_child_weight': 1}. Best is trial 91 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:38,254] Trial 94 finished with value: 0.861359649122807 and parameters: {'n_estimators': 242, 'max_depth': 10, 'learning_rate': 0.02918915458664316, 'subsample': 0.9792293888706978, 'colsample_bytree': 0.6296451807792544, 'gamma': 0.2789937453529579, 'min_child_weight': 2}. Best is trial 91 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:40,089] Trial 95 finished with value: 0.8655921052631579 and parameters: {'n_estimators': 169, 'max_depth': 10, 'learning_rate': 0.03802678844977183, 'subsample': 0.9990463760416667, 'colsample_bytree': 0.6894145641435583, 'gamma': 0.36447764707847763, 'min_child_weight': 1}. Best is trial 91 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:41,380] Trial 96 finished with value: 0.8466885964912281 and parameters: {'n_estimators': 208, 'max_depth': 10, 'learning_rate': 0.029149612611658632, 'subsample': 0.9681549640592273, 'colsample_bytree': 0.6652849229300704, 'gamma': 0.6367014763967096, 'min_child_weight': 5}. Best is trial 91 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:43,077] Trial 97 finished with value: 0.8634868421052632 and parameters: {'n_estimators': 238, 'max_depth': 10, 'learning_rate': 0.024373471272242438, 'subsample': 0.9889873215411056, 'colsample_bytree': 0.6743272874234633, 'gamma': 1.293506719752679, 'min_child_weight': 1}. Best is trial 91 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:45,534] Trial 98 finished with value: 0.861359649122807 and parameters: {'n_estimators': 246, 'max_depth': 10, 'learning_rate': 0.014537602242096572, 'subsample': 0.9999788519451228, 'colsample_bytree': 0.6439780718549563, 'gamma': 1.1175474447212197, 'min_child_weight': 1}. Best is trial 91 with value: 0.8718640350877193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:42:47,351] Trial 99 finished with value: 0.8550877192982457 and parameters: {'n_estimators': 229, 'max_depth': 9, 'learning_rate': 0.04544518469372745, 'subsample': 0.9582534771761431, 'colsample_bytree': 0.6586855472884099, 'gamma': 0.1732656887781961, 'min_child_weight': 2}. Best is trial 91 with value: 0.8718640350877193.
✅ Best XGBoost Parameters: {'n_estimators': 207, 'max_depth': 10, 'learning_rate': 0.01984393003584142, 'subsample': 0.9993738633955225, 'colsample_bytree': 0.658973270280266, 'gamma': 0.4136771466451134, 'min_child_weight': 1}
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:42:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
✅ Best model selected based on F1: RandomForest
01-05-2025 23:42:48 Retraining best model on original full dataset...
✅ Best model selected based on F1: RandomForest
[I 2025-05-01 23:51:12,559] A new study created in memory with name: no-name-9ad7f9b0-d55c-4282-9980-a8b86c6f1570
[I 2025-05-01 23:51:14,736] Trial 0 finished with value: 0.7816901408450704 and parameters: {'n_estimators': 62, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 0 with value: 0.7816901408450704.
[I 2025-05-01 23:51:18,477] Trial 1 finished with value: 0.7731611893583723 and parameters: {'n_estimators': 103, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 5}. Best is trial 0 with value: 0.7816901408450704.
[I 2025-05-01 23:51:20,856] Trial 2 finished with value: 0.7732394366197182 and parameters: {'n_estimators': 120, 'max_depth': 2, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 0 with value: 0.7816901408450704.
[I 2025-05-01 23:51:23,228] Trial 3 finished with value: 0.7872456964006259 and parameters: {'n_estimators': 76, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.7872456964006259.
[I 2025-05-01 23:51:25,130] Trial 4 finished with value: 0.7900625978090767 and parameters: {'n_estimators': 104, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 4 with value: 0.7900625978090767.
[I 2025-05-01 23:51:28,279] Trial 5 finished with value: 0.7900234741784038 and parameters: {'n_estimators': 175, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 4 with value: 0.7900625978090767.
[I 2025-05-01 23:51:29,503] Trial 6 finished with value: 0.7704616588419405 and parameters: {'n_estimators': 75, 'max_depth': 4, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 4 with value: 0.7900625978090767.
[I 2025-05-01 23:51:32,753] Trial 7 finished with value: 0.7844679186228483 and parameters: {'n_estimators': 179, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 4 with value: 0.7900625978090767.
[I 2025-05-01 23:51:35,188] Trial 8 finished with value: 0.776056338028169 and parameters: {'n_estimators': 173, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 4 with value: 0.7900625978090767.
[I 2025-05-01 23:51:37,289] Trial 9 finished with value: 0.7816510172143974 and parameters: {'n_estimators': 144, 'max_depth': 3, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.7900625978090767.
[I 2025-05-01 23:51:38,751] Trial 10 finished with value: 0.7704225352112676 and parameters: {'n_estimators': 99, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 4 with value: 0.7900625978090767.
[I 2025-05-01 23:51:42,429] Trial 11 finished with value: 0.8012910798122064 and parameters: {'n_estimators': 200, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.8012910798122064.
[I 2025-05-01 23:51:45,199] Trial 12 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 143, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 11 with value: 0.8012910798122064.
[I 2025-05-01 23:51:48,507] Trial 13 finished with value: 0.8041079812206572 and parameters: {'n_estimators': 197, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:51:51,634] Trial 14 finished with value: 0.7844287949921753 and parameters: {'n_estimators': 200, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:51:54,272] Trial 15 finished with value: 0.7788341158059466 and parameters: {'n_estimators': 199, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:51:56,707] Trial 16 finished with value: 0.7900625978090765 and parameters: {'n_estimators': 153, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:51:58,969] Trial 17 finished with value: 0.7788341158059469 and parameters: {'n_estimators': 161, 'max_depth': 6, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:02,296] Trial 18 finished with value: 0.7957355242566511 and parameters: {'n_estimators': 188, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:04,498] Trial 19 finished with value: 0.7816118935837245 and parameters: {'n_estimators': 127, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:07,450] Trial 20 finished with value: 0.7816118935837245 and parameters: {'n_estimators': 162, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:10,492] Trial 21 finished with value: 0.7957355242566511 and parameters: {'n_estimators': 188, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:13,277] Trial 22 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 188, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:16,196] Trial 23 finished with value: 0.7873239436619718 and parameters: {'n_estimators': 188, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:19,838] Trial 24 finished with value: 0.7788732394366196 and parameters: {'n_estimators': 197, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:23,401] Trial 25 finished with value: 0.7872848200312988 and parameters: {'n_estimators': 171, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:26,546] Trial 26 finished with value: 0.7844287949921753 and parameters: {'n_estimators': 185, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:29,012] Trial 27 finished with value: 0.7900625978090767 and parameters: {'n_estimators': 164, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:31,240] Trial 28 finished with value: 0.7901017214397497 and parameters: {'n_estimators': 143, 'max_depth': 10, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:34,443] Trial 29 finished with value: 0.7873239436619718 and parameters: {'n_estimators': 193, 'max_depth': 5, 'min_samples_split': 4, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:38,053] Trial 30 finished with value: 0.7844679186228483 and parameters: {'n_estimators': 179, 'max_depth': 6, 'min_samples_split': 4, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:41,292] Trial 31 finished with value: 0.7957355242566511 and parameters: {'n_estimators': 187, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:44,826] Trial 32 finished with value: 0.7984741784037558 and parameters: {'n_estimators': 200, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:47,641] Trial 33 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 199, 'max_depth': 10, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:50,955] Trial 34 finished with value: 0.7844679186228483 and parameters: {'n_estimators': 181, 'max_depth': 8, 'min_samples_split': 3, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:54,255] Trial 35 finished with value: 0.7844679186228483 and parameters: {'n_estimators': 171, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:57,662] Trial 36 finished with value: 0.8041079812206572 and parameters: {'n_estimators': 192, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:52:58,685] Trial 37 finished with value: 0.7900625978090765 and parameters: {'n_estimators': 57, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:01,632] Trial 38 finished with value: 0.7872456964006259 and parameters: {'n_estimators': 194, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:03,880] Trial 39 finished with value: 0.7900234741784038 and parameters: {'n_estimators': 115, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:04,931] Trial 40 finished with value: 0.784467918622848 and parameters: {'n_estimators': 71, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:07,524] Trial 41 finished with value: 0.7956572769953052 and parameters: {'n_estimators': 192, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:10,572] Trial 42 finished with value: 0.7956964006259781 and parameters: {'n_estimators': 180, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:12,213] Trial 43 finished with value: 0.7929186228482002 and parameters: {'n_estimators': 88, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:15,121] Trial 44 finished with value: 0.795618153364632 and parameters: {'n_estimators': 174, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:18,037] Trial 45 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:21,260] Trial 46 finished with value: 0.7844679186228483 and parameters: {'n_estimators': 167, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 5}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:23,364] Trial 47 finished with value: 0.7704616588419405 and parameters: {'n_estimators': 151, 'max_depth': 2, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:26,379] Trial 48 finished with value: 0.7900234741784036 and parameters: {'n_estimators': 181, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:29,130] Trial 49 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 156, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:31,873] Trial 50 finished with value: 0.7957355242566511 and parameters: {'n_estimators': 192, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:34,790] Trial 51 finished with value: 0.7957355242566511 and parameters: {'n_estimators': 186, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:38,449] Trial 52 finished with value: 0.7956964006259779 and parameters: {'n_estimators': 192, 'max_depth': 9, 'min_samples_split': 6, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:40,476] Trial 53 finished with value: 0.78716744913928 and parameters: {'n_estimators': 128, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:43,629] Trial 54 finished with value: 0.7901017214397497 and parameters: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:48,783] Trial 55 finished with value: 0.7900234741784038 and parameters: {'n_estimators': 184, 'max_depth': 8, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:53:54,789] Trial 56 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 175, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:01,266] Trial 57 finished with value: 0.7788732394366196 and parameters: {'n_estimators': 178, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:06,103] Trial 58 finished with value: 0.7844679186228483 and parameters: {'n_estimators': 193, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:08,521] Trial 59 finished with value: 0.7928403755868544 and parameters: {'n_estimators': 168, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:11,599] Trial 60 finished with value: 0.7872456964006259 and parameters: {'n_estimators': 134, 'max_depth': 7, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:15,840] Trial 61 finished with value: 0.7957355242566511 and parameters: {'n_estimators': 189, 'max_depth': 9, 'min_samples_split': 7, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:19,403] Trial 62 finished with value: 0.8012128325508607 and parameters: {'n_estimators': 175, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:22,111] Trial 63 finished with value: 0.7956964006259781 and parameters: {'n_estimators': 174, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:24,357] Trial 64 finished with value: 0.8012128325508607 and parameters: {'n_estimators': 196, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:26,266] Trial 65 finished with value: 0.8013302034428795 and parameters: {'n_estimators': 196, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:28,860] Trial 66 finished with value: 0.784467918622848 and parameters: {'n_estimators': 194, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:30,798] Trial 67 finished with value: 0.7928403755868544 and parameters: {'n_estimators': 158, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:33,197] Trial 68 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 177, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:35,030] Trial 69 finished with value: 0.7872848200312988 and parameters: {'n_estimators': 182, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 4}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:37,055] Trial 70 finished with value: 0.7901017214397497 and parameters: {'n_estimators': 196, 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:39,130] Trial 71 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 176, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:41,026] Trial 72 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 188, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:42,973] Trial 73 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 168, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:45,120] Trial 74 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 183, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:47,148] Trial 75 finished with value: 0.7900234741784038 and parameters: {'n_estimators': 196, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:48,954] Trial 76 finished with value: 0.7956964006259779 and parameters: {'n_estimators': 150, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:50,854] Trial 77 finished with value: 0.7901408450704224 and parameters: {'n_estimators': 189, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:52,864] Trial 78 finished with value: 0.7956964006259779 and parameters: {'n_estimators': 177, 'max_depth': 8, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:55,223] Trial 79 finished with value: 0.7984741784037558 and parameters: {'n_estimators': 197, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:56,906] Trial 80 finished with value: 0.7928403755868544 and parameters: {'n_estimators': 163, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:54:58,965] Trial 81 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 176, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:00,749] Trial 82 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 171, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:03,358] Trial 83 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 184, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:05,471] Trial 84 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 190, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:07,705] Trial 85 finished with value: 0.7900625978090767 and parameters: {'n_estimators': 196, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:09,841] Trial 86 finished with value: 0.7984741784037558 and parameters: {'n_estimators': 185, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:10,960] Trial 87 finished with value: 0.7872456964006259 and parameters: {'n_estimators': 111, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:12,903] Trial 88 finished with value: 0.7900625978090765 and parameters: {'n_estimators': 179, 'max_depth': 9, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:15,260] Trial 89 finished with value: 0.7872456964006259 and parameters: {'n_estimators': 171, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:16,999] Trial 90 finished with value: 0.7901017214397494 and parameters: {'n_estimators': 165, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:19,201] Trial 91 finished with value: 0.8013302034428795 and parameters: {'n_estimators': 189, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:21,495] Trial 92 finished with value: 0.7984350547730829 and parameters: {'n_estimators': 190, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:23,756] Trial 93 finished with value: 0.7872848200312988 and parameters: {'n_estimators': 196, 'max_depth': 9, 'min_samples_split': 10, 'min_samples_leaf': 5}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:25,756] Trial 94 finished with value: 0.8012910798122064 and parameters: {'n_estimators': 200, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:27,761] Trial 95 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 200, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:30,495] Trial 96 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 192, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:32,428] Trial 97 finished with value: 0.7984741784037558 and parameters: {'n_estimators': 185, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:34,660] Trial 98 finished with value: 0.8041079812206572 and parameters: {'n_estimators': 197, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
[I 2025-05-01 23:55:36,645] Trial 99 finished with value: 0.7900234741784036 and parameters: {'n_estimators': 198, 'max_depth': 7, 'min_samples_split': 10, 'min_samples_leaf': 2}. Best is trial 13 with value: 0.8041079812206572.
✅ Best Random Forest Parameters: {'n_estimators': 197, 'max_depth': 8, 'min_samples_split': 10, 'min_samples_leaf': 2}
[I 2025-05-01 23:55:36,648] A new study created in memory with name: no-name-4eacb117-89e1-4566-b559-04527184779d
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:37,589] Trial 0 finished with value: 0.7591549295774647 and parameters: {'n_estimators': 157, 'max_depth': 5, 'learning_rate': 0.07937120394776405, 'subsample': 0.7503831010144125, 'colsample_bytree': 0.7992476960336733, 'gamma': 1.6398272265671898, 'min_child_weight': 8}. Best is trial 0 with value: 0.7591549295774647.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:38,450] Trial 1 finished with value: 0.7816510172143974 and parameters: {'n_estimators': 268, 'max_depth': 5, 'learning_rate': 0.21628750800692678, 'subsample': 0.9357013651043353, 'colsample_bytree': 0.8840003265654393, 'gamma': 4.670409070569631, 'min_child_weight': 7}. Best is trial 1 with value: 0.7816510172143974.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:39,007] Trial 2 finished with value: 0.7955790297339593 and parameters: {'n_estimators': 117, 'max_depth': 8, 'learning_rate': 0.10522086457220367, 'subsample': 0.8232593695367956, 'colsample_bytree': 0.801375405176611, 'gamma': 3.051892420014438, 'min_child_weight': 4}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:39,538] Trial 3 finished with value: 0.7732003129890452 and parameters: {'n_estimators': 145, 'max_depth': 6, 'learning_rate': 0.22267159151453017, 'subsample': 0.8272825495651526, 'colsample_bytree': 0.8201714115201486, 'gamma': 3.2249119130624564, 'min_child_weight': 8}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:40,277] Trial 4 finished with value: 0.776056338028169 and parameters: {'n_estimators': 263, 'max_depth': 7, 'learning_rate': 0.07417785672549965, 'subsample': 0.7758222221634994, 'colsample_bytree': 0.643246821416203, 'gamma': 3.195570933732111, 'min_child_weight': 4}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:41,179] Trial 5 finished with value: 0.7787949921752739 and parameters: {'n_estimators': 297, 'max_depth': 6, 'learning_rate': 0.24228856971652232, 'subsample': 0.8976350789066382, 'colsample_bytree': 0.7177548158370876, 'gamma': 0.967370819894432, 'min_child_weight': 3}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:41,757] Trial 6 finished with value: 0.7647496087636932 and parameters: {'n_estimators': 181, 'max_depth': 10, 'learning_rate': 0.1629713630908131, 'subsample': 0.609659530627996, 'colsample_bytree': 0.8579871783387422, 'gamma': 2.9439173690265115, 'min_child_weight': 7}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:41] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:42,794] Trial 7 finished with value: 0.7814553990610328 and parameters: {'n_estimators': 238, 'max_depth': 6, 'learning_rate': 0.15478007169601665, 'subsample': 0.9743245883933829, 'colsample_bytree': 0.7209653181038529, 'gamma': 1.256420629492125, 'min_child_weight': 1}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:43,538] Trial 8 finished with value: 0.7704225352112676 and parameters: {'n_estimators': 169, 'max_depth': 3, 'learning_rate': 0.018146239478544143, 'subsample': 0.7859564150476944, 'colsample_bytree': 0.9472430439947926, 'gamma': 2.8717733324182166, 'min_child_weight': 7}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:44,360] Trial 9 finished with value: 0.7310250391236306 and parameters: {'n_estimators': 221, 'max_depth': 7, 'learning_rate': 0.2250339609752743, 'subsample': 0.7723314613040538, 'colsample_bytree': 0.731520835586104, 'gamma': 0.4444078294098769, 'min_child_weight': 9}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:44,902] Trial 10 finished with value: 0.776017214397496 and parameters: {'n_estimators': 119, 'max_depth': 10, 'learning_rate': 0.2976528246592137, 'subsample': 0.6702385960399768, 'colsample_bytree': 0.9670473257580436, 'gamma': 4.5331096226925105, 'min_child_weight': 4}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:45,396] Trial 11 finished with value: 0.776056338028169 and parameters: {'n_estimators': 107, 'max_depth': 8, 'learning_rate': 0.14886612429214302, 'subsample': 0.9171030365337995, 'colsample_bytree': 0.8931981878508692, 'gamma': 4.99249395581782, 'min_child_weight': 5}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:46,183] Trial 12 finished with value: 0.7844679186228483 and parameters: {'n_estimators': 296, 'max_depth': 4, 'learning_rate': 0.10428368549534424, 'subsample': 0.8655736494312307, 'colsample_bytree': 0.9006704332550963, 'gamma': 4.021537425058774, 'min_child_weight': 10}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:46,867] Trial 13 finished with value: 0.7789514866979654 and parameters: {'n_estimators': 211, 'max_depth': 3, 'learning_rate': 0.1029118210445013, 'subsample': 0.8683476662826359, 'colsample_bytree': 0.7872803808952256, 'gamma': 4.117750042508886, 'min_child_weight': 10}. Best is trial 2 with value: 0.7955790297339593.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:47] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:48,381] Trial 14 finished with value: 0.8012910798122064 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.04198450514768215, 'subsample': 0.8482462273210952, 'colsample_bytree': 0.9978099508021514, 'gamma': 3.8197753298124364, 'min_child_weight': 1}. Best is trial 14 with value: 0.8012910798122064.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:49,355] Trial 15 finished with value: 0.7675665101721438 and parameters: {'n_estimators': 134, 'max_depth': 9, 'learning_rate': 0.010587024657744068, 'subsample': 0.7177098773828184, 'colsample_bytree': 0.6127654875606835, 'gamma': 2.1105998709411495, 'min_child_weight': 1}. Best is trial 14 with value: 0.8012910798122064.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:50,077] Trial 16 finished with value: 0.7985133020344287 and parameters: {'n_estimators': 183, 'max_depth': 8, 'learning_rate': 0.053001889364595434, 'subsample': 0.8273734620017592, 'colsample_bytree': 0.9718594757415716, 'gamma': 3.8884282434185984, 'min_child_weight': 2}. Best is trial 14 with value: 0.8012910798122064.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:50,776] Trial 17 finished with value: 0.7956181533646323 and parameters: {'n_estimators': 192, 'max_depth': 9, 'learning_rate': 0.04821028993856938, 'subsample': 0.7068040673243032, 'colsample_bytree': 0.9492669684724497, 'gamma': 3.816412721824703, 'min_child_weight': 2}. Best is trial 14 with value: 0.8012910798122064.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:52,093] Trial 18 finished with value: 0.7956181533646323 and parameters: {'n_estimators': 242, 'max_depth': 8, 'learning_rate': 0.04993955848838271, 'subsample': 0.9941321542198794, 'colsample_bytree': 0.9773737667072169, 'gamma': 3.621526000368528, 'min_child_weight': 2}. Best is trial 14 with value: 0.8012910798122064.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:53,135] Trial 19 finished with value: 0.8040688575899843 and parameters: {'n_estimators': 198, 'max_depth': 9, 'learning_rate': 0.04964505825076356, 'subsample': 0.8580230629354635, 'colsample_bytree': 0.9998395205380884, 'gamma': 2.3485884861217095, 'min_child_weight': 2}. Best is trial 19 with value: 0.8040688575899843.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:53,992] Trial 20 finished with value: 0.7983959311424101 and parameters: {'n_estimators': 208, 'max_depth': 9, 'learning_rate': 0.1313616854358456, 'subsample': 0.8560532022423584, 'colsample_bytree': 0.9230147592021031, 'gamma': 2.268898286314534, 'min_child_weight': 1}. Best is trial 19 with value: 0.8040688575899843.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:54,820] Trial 21 finished with value: 0.8096635367762129 and parameters: {'n_estimators': 188, 'max_depth': 9, 'learning_rate': 0.05276175396973279, 'subsample': 0.8241749508529413, 'colsample_bytree': 0.9961894450140912, 'gamma': 2.4761118280836074, 'min_child_weight': 2}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:55,746] Trial 22 finished with value: 0.7983959311424099 and parameters: {'n_estimators': 221, 'max_depth': 10, 'learning_rate': 0.03685650402149107, 'subsample': 0.8912977093175883, 'colsample_bytree': 0.9985741385767397, 'gamma': 1.9818256320498926, 'min_child_weight': 3}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:56,722] Trial 23 finished with value: 0.8039906103286386 and parameters: {'n_estimators': 168, 'max_depth': 9, 'learning_rate': 0.07885491079912363, 'subsample': 0.9500532104473125, 'colsample_bytree': 0.9981094104249184, 'gamma': 2.459523104051875, 'min_child_weight': 3}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:57,412] Trial 24 finished with value: 0.8012128325508607 and parameters: {'n_estimators': 165, 'max_depth': 10, 'learning_rate': 0.06580933285747984, 'subsample': 0.9513356668077284, 'colsample_bytree': 0.9294418524918057, 'gamma': 2.535550373316923, 'min_child_weight': 3}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:58,139] Trial 25 finished with value: 0.787206572769953 and parameters: {'n_estimators': 198, 'max_depth': 9, 'learning_rate': 0.0871931854407911, 'subsample': 0.9568795600982478, 'colsample_bytree': 0.84971847127821, 'gamma': 2.564691301431982, 'min_child_weight': 5}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:58,889] Trial 26 finished with value: 0.7871674491392803 and parameters: {'n_estimators': 146, 'max_depth': 7, 'learning_rate': 0.1289419536479463, 'subsample': 0.8066759477584614, 'colsample_bytree': 0.9994009289733782, 'gamma': 1.6153375415677933, 'min_child_weight': 2}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:55:59,612] Trial 27 finished with value: 0.7928012519561815 and parameters: {'n_estimators': 177, 'max_depth': 8, 'learning_rate': 0.1837746151967098, 'subsample': 0.9113999378651774, 'colsample_bytree': 0.9292298782946042, 'gamma': 1.736071786201764, 'min_child_weight': 3}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:55:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:00,569] Trial 28 finished with value: 0.7647104851330203 and parameters: {'n_estimators': 193, 'max_depth': 9, 'learning_rate': 0.028154322900587777, 'subsample': 0.8835072067618615, 'colsample_bytree': 0.9597165282955605, 'gamma': 0.9499505161969197, 'min_child_weight': 6}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:01,136] Trial 29 finished with value: 0.7759389671361502 and parameters: {'n_estimators': 153, 'max_depth': 10, 'learning_rate': 0.08400038474101956, 'subsample': 0.7346798407628092, 'colsample_bytree': 0.9196212908781018, 'gamma': 2.589134667441488, 'min_child_weight': 4}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:01,842] Trial 30 finished with value: 0.7843114241001565 and parameters: {'n_estimators': 162, 'max_depth': 7, 'learning_rate': 0.06518091116041853, 'subsample': 0.9991653200930464, 'colsample_bytree': 0.8467395009991456, 'gamma': 1.6845951786895978, 'min_child_weight': 2}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:02,811] Trial 31 finished with value: 0.8096635367762127 and parameters: {'n_estimators': 221, 'max_depth': 9, 'learning_rate': 0.039121134394337335, 'subsample': 0.8561327609527802, 'colsample_bytree': 0.9988103446547566, 'gamma': 3.3707238279153966, 'min_child_weight': 1}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:04,441] Trial 32 finished with value: 0.7956181533646323 and parameters: {'n_estimators': 236, 'max_depth': 9, 'learning_rate': 0.024395471439177238, 'subsample': 0.9313154614890362, 'colsample_bytree': 0.9816613776335733, 'gamma': 3.440510373399092, 'min_child_weight': 1}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:05,346] Trial 33 finished with value: 0.8011737089201878 and parameters: {'n_estimators': 226, 'max_depth': 8, 'learning_rate': 0.08466691946884444, 'subsample': 0.809777561362426, 'colsample_bytree': 0.9481888271186218, 'gamma': 2.303657356753602, 'min_child_weight': 2}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:06,194] Trial 34 finished with value: 0.7927621283255085 and parameters: {'n_estimators': 252, 'max_depth': 10, 'learning_rate': 0.062439453131477446, 'subsample': 0.8338463665863011, 'colsample_bytree': 0.978680339391902, 'gamma': 2.8363788391184417, 'min_child_weight': 3}. Best is trial 21 with value: 0.8096635367762129.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:06,892] Trial 35 finished with value: 0.8097417840375586 and parameters: {'n_estimators': 209, 'max_depth': 8, 'learning_rate': 0.11477056357894364, 'subsample': 0.7573067566359345, 'colsample_bytree': 0.8818629730250889, 'gamma': 3.3857492745912463, 'min_child_weight': 1}. Best is trial 35 with value: 0.8097417840375586.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:07,718] Trial 36 finished with value: 0.7928012519561815 and parameters: {'n_estimators': 210, 'max_depth': 8, 'learning_rate': 0.12695809971280242, 'subsample': 0.7677385018224399, 'colsample_bytree': 0.8749939947817109, 'gamma': 4.349738745548035, 'min_child_weight': 1}. Best is trial 35 with value: 0.8097417840375586.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:08,462] Trial 37 finished with value: 0.7816510172143974 and parameters: {'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.10789727090417861, 'subsample': 0.7480429259924573, 'colsample_bytree': 0.9051410737261596, 'gamma': 3.2006311746146716, 'min_child_weight': 2}. Best is trial 35 with value: 0.8097417840375586.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:09,392] Trial 38 finished with value: 0.776056338028169 and parameters: {'n_estimators': 281, 'max_depth': 8, 'learning_rate': 0.18474101901621887, 'subsample': 0.6750703962441793, 'colsample_bytree': 0.7531159031340056, 'gamma': 3.373413793135374, 'min_child_weight': 1}. Best is trial 35 with value: 0.8097417840375586.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:10,719] Trial 39 finished with value: 0.8012128325508607 and parameters: {'n_estimators': 261, 'max_depth': 9, 'learning_rate': 0.03365540562266561, 'subsample': 0.7991833861041393, 'colsample_bytree': 0.6791140502572997, 'gamma': 2.769631136704607, 'min_child_weight': 1}. Best is trial 35 with value: 0.8097417840375586.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:11,377] Trial 40 finished with value: 0.7731220657276994 and parameters: {'n_estimators': 185, 'max_depth': 7, 'learning_rate': 0.09519640392236856, 'subsample': 0.791373154370007, 'colsample_bytree': 0.8174293524421251, 'gamma': 3.0744546180750056, 'min_child_weight': 4}. Best is trial 35 with value: 0.8097417840375586.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:12,094] Trial 41 finished with value: 0.7816118935837245 and parameters: {'n_estimators': 171, 'max_depth': 9, 'learning_rate': 0.07381486382910707, 'subsample': 0.8478806922326583, 'colsample_bytree': 0.9470934627032932, 'gamma': 2.3336178548270405, 'min_child_weight': 3}. Best is trial 35 with value: 0.8097417840375586.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:13,050] Trial 42 finished with value: 0.8067683881064163 and parameters: {'n_estimators': 227, 'max_depth': 10, 'learning_rate': 0.11240779132014708, 'subsample': 0.8787448123593422, 'colsample_bytree': 0.9999692842107861, 'gamma': 2.0045685405754208, 'min_child_weight': 2}. Best is trial 35 with value: 0.8097417840375586.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:13,817] Trial 43 finished with value: 0.8068075117370892 and parameters: {'n_estimators': 227, 'max_depth': 10, 'learning_rate': 0.12088871576494024, 'subsample': 0.8782769055440244, 'colsample_bytree': 0.9608549023774735, 'gamma': 1.9917030463751422, 'min_child_weight': 2}. Best is trial 35 with value: 0.8097417840375586.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:14,846] Trial 44 finished with value: 0.8179577464788732 and parameters: {'n_estimators': 230, 'max_depth': 10, 'learning_rate': 0.11882149781881483, 'subsample': 0.8884903159739298, 'colsample_bytree': 0.9671675846690962, 'gamma': 1.3816019312369714, 'min_child_weight': 1}. Best is trial 44 with value: 0.8179577464788732.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:15,782] Trial 45 finished with value: 0.8124021909233179 and parameters: {'n_estimators': 247, 'max_depth': 10, 'learning_rate': 0.14746716471798899, 'subsample': 0.9098603151333835, 'colsample_bytree': 0.9375895809640991, 'gamma': 1.293426657811072, 'min_child_weight': 1}. Best is trial 44 with value: 0.8179577464788732.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:16,664] Trial 46 finished with value: 0.7927230046948356 and parameters: {'n_estimators': 249, 'max_depth': 10, 'learning_rate': 0.17556686636191118, 'subsample': 0.9098991188461921, 'colsample_bytree': 0.8790229462939426, 'gamma': 0.46665820666945557, 'min_child_weight': 1}. Best is trial 44 with value: 0.8179577464788732.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:18,133] Trial 47 finished with value: 0.7843505477308292 and parameters: {'n_estimators': 275, 'max_depth': 10, 'learning_rate': 0.14378764671808658, 'subsample': 0.8358266909368015, 'colsample_bytree': 0.9141780980333504, 'gamma': 0.9926315481697157, 'min_child_weight': 1}. Best is trial 44 with value: 0.8179577464788732.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:19,142] Trial 48 finished with value: 0.7843505477308295 and parameters: {'n_estimators': 215, 'max_depth': 10, 'learning_rate': 0.1625929589399911, 'subsample': 0.6101434351558854, 'colsample_bytree': 0.9417512083522859, 'gamma': 1.1986635135200625, 'min_child_weight': 1}. Best is trial 44 with value: 0.8179577464788732.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:20,166] Trial 49 finished with value: 0.7843896713615022 and parameters: {'n_estimators': 237, 'max_depth': 8, 'learning_rate': 0.2715910926553182, 'subsample': 0.9295617955044851, 'colsample_bytree': 0.8646732291159023, 'gamma': 0.2886117147409224, 'min_child_weight': 1}. Best is trial 44 with value: 0.8179577464788732.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:20,942] Trial 50 finished with value: 0.7563771517996869 and parameters: {'n_estimators': 254, 'max_depth': 6, 'learning_rate': 0.2170216394745667, 'subsample': 0.900313617399802, 'colsample_bytree': 0.9646013090785747, 'gamma': 1.3811511023778016, 'min_child_weight': 9}. Best is trial 44 with value: 0.8179577464788732.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:21,799] Trial 51 finished with value: 0.7899452269170579 and parameters: {'n_estimators': 229, 'max_depth': 10, 'learning_rate': 0.12229729920624823, 'subsample': 0.873384808043178, 'colsample_bytree': 0.9818189096506815, 'gamma': 0.7238632215857077, 'min_child_weight': 2}. Best is trial 44 with value: 0.8179577464788732.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:22,592] Trial 52 finished with value: 0.7788732394366196 and parameters: {'n_estimators': 220, 'max_depth': 10, 'learning_rate': 0.14495498573373067, 'subsample': 0.8134366502946605, 'colsample_bytree': 0.9607185935660286, 'gamma': 1.4564749943353597, 'min_child_weight': 1}. Best is trial 44 with value: 0.8179577464788732.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:23,480] Trial 53 finished with value: 0.8180359937402193 and parameters: {'n_estimators': 245, 'max_depth': 9, 'learning_rate': 0.11721072882708138, 'subsample': 0.8957843436826788, 'colsample_bytree': 0.9311875395349593, 'gamma': 1.9686979919948926, 'min_child_weight': 2}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:24,674] Trial 54 finished with value: 0.7872456964006259 and parameters: {'n_estimators': 247, 'max_depth': 9, 'learning_rate': 0.1357502490651663, 'subsample': 0.8413976071422721, 'colsample_bytree': 0.895538959354202, 'gamma': 1.1700762616829792, 'min_child_weight': 1}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:25,618] Trial 55 finished with value: 0.8096244131455398 and parameters: {'n_estimators': 260, 'max_depth': 9, 'learning_rate': 0.10054777630370945, 'subsample': 0.9004703855273859, 'colsample_bytree': 0.9320420643553946, 'gamma': 1.742231407700501, 'min_child_weight': 2}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:25] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:26,336] Trial 56 finished with value: 0.8041471048513303 and parameters: {'n_estimators': 204, 'max_depth': 9, 'learning_rate': 0.16063365800745336, 'subsample': 0.7764931116432294, 'colsample_bytree': 0.978308637360403, 'gamma': 3.5193444309261777, 'min_child_weight': 2}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:26] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:26,973] Trial 57 finished with value: 0.7815727699530516 and parameters: {'n_estimators': 242, 'max_depth': 9, 'learning_rate': 0.17568671043100545, 'subsample': 0.8234779023588404, 'colsample_bytree': 0.9074434877011636, 'gamma': 2.7100127294936525, 'min_child_weight': 8}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:27] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:28,120] Trial 58 finished with value: 0.7788341158059469 and parameters: {'n_estimators': 269, 'max_depth': 8, 'learning_rate': 0.1160685598038691, 'subsample': 0.8643586152286872, 'colsample_bytree': 0.7805418682979458, 'gamma': 2.962092393103389, 'min_child_weight': 4}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:28] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:28,857] Trial 59 finished with value: 0.7759389671361502 and parameters: {'n_estimators': 216, 'max_depth': 10, 'learning_rate': 0.14074347840495527, 'subsample': 0.9362498006182913, 'colsample_bytree': 0.9378774346901021, 'gamma': 1.846874750952899, 'min_child_weight': 6}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:29] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:30,056] Trial 60 finished with value: 0.809507042253521 and parameters: {'n_estimators': 232, 'max_depth': 9, 'learning_rate': 0.09471552198202457, 'subsample': 0.9667535184591796, 'colsample_bytree': 0.8314506232522051, 'gamma': 1.4762599656120043, 'min_child_weight': 1}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:30] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:31,037] Trial 61 finished with value: 0.8067683881064163 and parameters: {'n_estimators': 261, 'max_depth': 9, 'learning_rate': 0.09942782952958293, 'subsample': 0.902352036157782, 'colsample_bytree': 0.9371996043188865, 'gamma': 2.1293775151068823, 'min_child_weight': 2}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:31] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:31,931] Trial 62 finished with value: 0.8095852895148671 and parameters: {'n_estimators': 283, 'max_depth': 9, 'learning_rate': 0.15543167854004208, 'subsample': 0.9214237341161033, 'colsample_bytree': 0.9546827159251272, 'gamma': 1.7852697014138075, 'min_child_weight': 2}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:32] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:33,120] Trial 63 finished with value: 0.7955790297339593 and parameters: {'n_estimators': 255, 'max_depth': 8, 'learning_rate': 0.10373987422290312, 'subsample': 0.8923724117520038, 'colsample_bytree': 0.9859109136625369, 'gamma': 1.5275681226907398, 'min_child_weight': 3}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:33] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:33,918] Trial 64 finished with value: 0.7843896713615022 and parameters: {'n_estimators': 191, 'max_depth': 10, 'learning_rate': 0.20009420667850653, 'subsample': 0.8546710527677941, 'colsample_bytree': 0.9226047324382507, 'gamma': 1.2682520391633016, 'min_child_weight': 1}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:34] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:35,848] Trial 65 finished with value: 0.7983959311424099 and parameters: {'n_estimators': 242, 'max_depth': 9, 'learning_rate': 0.016387366983656587, 'subsample': 0.8865739812157427, 'colsample_bytree': 0.8900939536504182, 'gamma': 2.16432947575525, 'min_child_weight': 2}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:35] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:36] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:37,015] Trial 66 finished with value: 0.7787558685446008 and parameters: {'n_estimators': 235, 'max_depth': 9, 'learning_rate': 0.07307235211181043, 'subsample': 0.9414806654254413, 'colsample_bytree': 0.9355735071327058, 'gamma': 0.8227662664023399, 'min_child_weight': 3}. Best is trial 53 with value: 0.8180359937402193.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:37] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:37,994] Trial 67 finished with value: 0.8208137715179967 and parameters: {'n_estimators': 268, 'max_depth': 8, 'learning_rate': 0.05635726740479233, 'subsample': 0.9195908443715842, 'colsample_bytree': 0.9680828085840469, 'gamma': 1.851432684684562, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:38] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:39,141] Trial 68 finished with value: 0.8041079812206572 and parameters: {'n_estimators': 285, 'max_depth': 6, 'learning_rate': 0.04275341694490104, 'subsample': 0.9825302294909949, 'colsample_bytree': 0.9692328767969366, 'gamma': 4.181172279869142, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:39] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:40] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:41,591] Trial 69 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 267, 'max_depth': 7, 'learning_rate': 0.022970621385948617, 'subsample': 0.9160297577502232, 'colsample_bytree': 0.9873602783355571, 'gamma': 4.903108632442258, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:42] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:42,973] Trial 70 finished with value: 0.8068075117370892 and parameters: {'n_estimators': 206, 'max_depth': 8, 'learning_rate': 0.05365987054518944, 'subsample': 0.7593704831902475, 'colsample_bytree': 0.9706595599668877, 'gamma': 3.305893478300401, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:43] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:44] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:44,917] Trial 71 finished with value: 0.8039514866979655 and parameters: {'n_estimators': 291, 'max_depth': 8, 'learning_rate': 0.060115310107402446, 'subsample': 0.9050270859829502, 'colsample_bytree': 0.9514077861316922, 'gamma': 1.86295614358649, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:45] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:46] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:48] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:48,861] Trial 72 finished with value: 0.8096635367762129 and parameters: {'n_estimators': 273, 'max_depth': 9, 'learning_rate': 0.11394203594996158, 'subsample': 0.9213037710226064, 'colsample_bytree': 0.9282952646988842, 'gamma': 3.634785713247476, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:49] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:50] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:51,341] Trial 73 finished with value: 0.8068857589984351 and parameters: {'n_estimators': 271, 'max_depth': 8, 'learning_rate': 0.11463714218187818, 'subsample': 0.9218082183941619, 'colsample_bytree': 0.9089549079381547, 'gamma': 3.6363006043031874, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:51] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:52] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:54,418] Trial 74 finished with value: 0.8012519561815337 and parameters: {'n_estimators': 278, 'max_depth': 10, 'learning_rate': 0.08889739723188858, 'subsample': 0.7328080292071131, 'colsample_bytree': 0.9860733703243246, 'gamma': 3.718109488266405, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:54] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:55,534] Trial 75 finished with value: 0.7928794992175273 and parameters: {'n_estimators': 220, 'max_depth': 7, 'learning_rate': 0.13638604694750356, 'subsample': 0.8688519461485485, 'colsample_bytree': 0.9208873405821038, 'gamma': 3.8705756956276725, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:55] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:56] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:57,555] Trial 76 finished with value: 0.7956572769953052 and parameters: {'n_estimators': 248, 'max_depth': 4, 'learning_rate': 0.03658466322457278, 'subsample': 0.9656183067552468, 'colsample_bytree': 0.9699316735875417, 'gamma': 3.1550117853609283, 'min_child_weight': 5}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:57] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:58] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:56:59,396] Trial 77 finished with value: 0.8012128325508607 and parameters: {'n_estimators': 292, 'max_depth': 9, 'learning_rate': 0.07821936043653152, 'subsample': 0.9489876673333906, 'colsample_bytree': 0.9888106702581781, 'gamma': 2.6708134211373995, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:56:59] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:00] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:01,621] Trial 78 finished with value: 0.795657276995305 and parameters: {'n_estimators': 187, 'max_depth': 10, 'learning_rate': 0.12856948798669257, 'subsample': 0.6944874886796967, 'colsample_bytree': 0.954825802014871, 'gamma': 2.442594203441007, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:01] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:02,453] Trial 79 finished with value: 0.8012910798122064 and parameters: {'n_estimators': 214, 'max_depth': 9, 'learning_rate': 0.07037497672322149, 'subsample': 0.8898841402054629, 'colsample_bytree': 0.8714813667975201, 'gamma': 4.016944123727081, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:02] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:03,425] Trial 80 finished with value: 0.7956181533646323 and parameters: {'n_estimators': 243, 'max_depth': 8, 'learning_rate': 0.1197198882494807, 'subsample': 0.8544615989261166, 'colsample_bytree': 0.9701602729930425, 'gamma': 1.6165940154050016, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:03] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:04,558] Trial 81 finished with value: 0.8068466353677621 and parameters: {'n_estimators': 262, 'max_depth': 9, 'learning_rate': 0.1498488046918156, 'subsample': 0.8984859440880799, 'colsample_bytree': 0.9303648696077204, 'gamma': 1.303539802677042, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:04] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:05,704] Trial 82 finished with value: 0.7843505477308295 and parameters: {'n_estimators': 260, 'max_depth': 9, 'learning_rate': 0.09123876066316272, 'subsample': 0.9225439052034632, 'colsample_bytree': 0.943503250107632, 'gamma': 1.0506712523872768, 'min_child_weight': 3}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:05] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:06,816] Trial 83 finished with value: 0.8123630672926447 and parameters: {'n_estimators': 258, 'max_depth': 9, 'learning_rate': 0.10617089743945135, 'subsample': 0.9113759721210968, 'colsample_bytree': 0.8986662243906244, 'gamma': 1.925184479989424, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:06] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:07] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:07,761] Trial 84 finished with value: 0.8068466353677621 and parameters: {'n_estimators': 257, 'max_depth': 10, 'learning_rate': 0.10920400417212764, 'subsample': 0.9413617755729523, 'colsample_bytree': 0.8903134856461311, 'gamma': 1.9417662790084347, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:08] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:09,114] Trial 85 finished with value: 0.8067683881064163 and parameters: {'n_estimators': 267, 'max_depth': 8, 'learning_rate': 0.04536122736710231, 'subsample': 0.91252887584071, 'colsample_bytree': 0.8991147659648089, 'gamma': 2.191569608589506, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:09] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:10] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:10,876] Trial 86 finished with value: 0.7957355242566508 and parameters: {'n_estimators': 177, 'max_depth': 10, 'learning_rate': 0.058567284382197535, 'subsample': 0.8830733735525932, 'colsample_bytree': 0.9193400325239265, 'gamma': 3.452823482635822, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:11] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:11,916] Trial 87 finished with value: 0.8012128325508607 and parameters: {'n_estimators': 251, 'max_depth': 9, 'learning_rate': 0.12457211267725739, 'subsample': 0.9270448552665377, 'colsample_bytree': 0.6053995869937239, 'gamma': 2.9483318789815165, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:12,665] Trial 88 finished with value: 0.8012519561815337 and parameters: {'n_estimators': 232, 'max_depth': 10, 'learning_rate': 0.13463128507661323, 'subsample': 0.8637775337475752, 'colsample_bytree': 0.8836391127418474, 'gamma': 2.425464952525844, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:12] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:13] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:14,227] Trial 89 finished with value: 0.7816118935837245 and parameters: {'n_estimators': 196, 'max_depth': 7, 'learning_rate': 0.010519330022505824, 'subsample': 0.8438127182024887, 'colsample_bytree': 0.8538595230623148, 'gamma': 1.5841767735028056, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:14] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:15,252] Trial 90 finished with value: 0.8068857589984351 and parameters: {'n_estimators': 223, 'max_depth': 9, 'learning_rate': 0.03267663936731816, 'subsample': 0.8182037349335495, 'colsample_bytree': 0.6953147045587041, 'gamma': 3.28992365261817, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:15] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:16,554] Trial 91 finished with value: 0.8124021909233177 and parameters: {'n_estimators': 277, 'max_depth': 9, 'learning_rate': 0.09768827470102256, 'subsample': 0.9069364687002658, 'colsample_bytree': 0.929762321465582, 'gamma': 1.4158666956068413, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:16] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:17,635] Trial 92 finished with value: 0.8012128325508607 and parameters: {'n_estimators': 276, 'max_depth': 9, 'learning_rate': 0.08247894140175424, 'subsample': 0.7915654642161384, 'colsample_bytree': 0.9103431229203195, 'gamma': 1.1108549092459556, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:17] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:18] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:18,992] Trial 93 finished with value: 0.7982394366197182 and parameters: {'n_estimators': 273, 'max_depth': 9, 'learning_rate': 0.10970543903604485, 'subsample': 0.8749035509914052, 'colsample_bytree': 0.990673351138772, 'gamma': 1.351852847264025, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:19] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:19,923] Trial 94 finished with value: 0.8067292644757433 and parameters: {'n_estimators': 286, 'max_depth': 8, 'learning_rate': 0.1010588288863585, 'subsample': 0.9095879195234375, 'colsample_bytree': 0.9583688240371661, 'gamma': 2.034098452943285, 'min_child_weight': 1}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:20] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:20,907] Trial 95 finished with value: 0.7899843505477306 and parameters: {'n_estimators': 202, 'max_depth': 10, 'learning_rate': 0.11541780515679947, 'subsample': 0.9343370579027367, 'colsample_bytree': 0.9459326412209644, 'gamma': 0.8667537187934589, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:21] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:22,101] Trial 96 finished with value: 0.8151408450704226 and parameters: {'n_estimators': 265, 'max_depth': 9, 'learning_rate': 0.17015405337717998, 'subsample': 0.8925731660591734, 'colsample_bytree': 0.9768879740738617, 'gamma': 1.4175605799189084, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:22] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:22,957] Trial 97 finished with value: 0.7982785602503913 and parameters: {'n_estimators': 265, 'max_depth': 9, 'learning_rate': 0.1515988369211253, 'subsample': 0.8907946022604237, 'colsample_bytree': 0.924421423742887, 'gamma': 1.4199495309247498, 'min_child_weight': 3}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:23,766] Trial 98 finished with value: 0.7927621283255085 and parameters: {'n_estimators': 280, 'max_depth': 5, 'learning_rate': 0.16923140084974314, 'subsample': 0.9514478964495888, 'colsample_bytree': 0.8384355247006098, 'gamma': 1.6705765901450196, 'min_child_weight': 3}. Best is trial 67 with value: 0.8208137715179967.
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:23] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
c:\Users\x-hp\AppData\Local\Programs\Python\Python311\Lib\site-packages\xgboost\training.py:183: UserWarning: [23:57:24] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738:
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
[I 2025-05-01 23:57:24,578] Trial 99 finished with value: 0.8180359937402191 and parameters: {'n_estimators': 255, 'max_depth': 8, 'learning_rate': 0.19893561304816362, 'subsample': 0.9152699908652823, 'colsample_bytree': 0.9629367635402358, 'gamma': 1.829928926625862, 'min_child_weight': 2}. Best is trial 67 with value: 0.8208137715179967.
✅ Best XGBoost Parameters: {'n_estimators': 268, 'max_depth': 8, 'learning_rate': 0.05635726740479233, 'subsample': 0.9195908443715842, 'colsample_bytree': 0.9680828085840469, 'gamma': 1.851432684684562, 'min_child_weight': 1}
✅ Best model selected based on F1: XGBoost
[34m[1mwandb[0m: [33mWARNING[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.
